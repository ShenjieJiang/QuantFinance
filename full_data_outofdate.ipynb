{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2934d1a-08bf-467a-a312-ef6404a0675c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from setting.ipynb\n"
     ]
    }
   ],
   "source": [
    "# import import_ipynb\n",
    "# import oss2\n",
    "# from setting import SETTINGS\n",
    "# import pickle\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import io\n",
    "# from dateutil import parser\n",
    "# import datetime\n",
    "# import pandas as pd\n",
    "# from fastparquet import write\n",
    "# from pathlib import  Path\n",
    "\n",
    "# AccessKeyId = SETTINGS[\"oss.accesskey\"]\n",
    "# AccessKeySecret = SETTINGS[\"oss.secret\"]\n",
    "# BucketName = SETTINGS[\"oss.bucketname\"]\n",
    "# Endpoint = SETTINGS[\"oss.endpoint\"]\n",
    "\n",
    "\n",
    "# class newBytes(io.BytesIO):\n",
    "#     def close(self):\n",
    "#         pass\n",
    "\n",
    "\n",
    "# class OssClient(object):\n",
    "#     __instance = None\n",
    "#     __first_init = False\n",
    "\n",
    "#     # 单例模式\n",
    "#     def __new__(cls, *args, **kwargs):\n",
    "#         if not cls.__instance:\n",
    "#             cls.__instance = super().__new__(cls)\n",
    "#         return cls.__instance\n",
    "\n",
    "#     def __init__(self):\n",
    "#         cls = self.__class__\n",
    "#         if not cls.__first_init:\n",
    "#             self.auth = oss2.Auth(AccessKeyId, AccessKeySecret)\n",
    "#             self.bucket = oss2.Bucket(self.auth, Endpoint, BucketName)\n",
    "#             cls.__first_init = True\n",
    "\n",
    "\n",
    "#     def upload_file_from_fileobj(self, object_name, local_file_path):\n",
    "#         \"\"\"\n",
    "#             upload_file_from_fileobj方法：上传文件对象到oss存储空间, 该方法可用于我们从上游服务接收了图片参数，然后以二进制形式读文件，上传到oss存储空间指定位置（abc/efg/00），\n",
    "#         当然也可以将本地文件上传到oss我们的bucket. 其中fileobj不止可以是文件对象，也可以是本地文件路径。 put_object方法底层仍是RESTful API的调用，可以指定headers，规定Content-Type等内容\n",
    "#         \"\"\"\n",
    "#         # 判断bucket中文件是否存在，也可以不判断，会上传更新\n",
    "#         #exist = self.bucket.object_exists(object_name) #<yourObjectName>\n",
    "#         #if exist:\n",
    "#         #    return True\n",
    "#         with open(local_file_path, 'rb') as fileobj:\n",
    "#             result = self.bucket.put_object(object_name, fileobj) #<yourObjectName>\n",
    "#         if result.status == 200:\n",
    "#             return True\n",
    "#         else:\n",
    "#             return False\n",
    "\n",
    "#     def upload_pickle_data(self,df, target_path, *args, report_date=None):\n",
    "#         if isinstance(report_date, datetime.date):\n",
    "#             d = report_date.strftime(\"%Y-%m-%d\")\n",
    "#         if isinstance(report_date, str):\n",
    "#             d = parser.parse(report_date).strftime(\"%Y-%m-%d\")\n",
    "#         if args:\n",
    "#             d = [arg for arg in args][0]\n",
    "#         pickle_buffer = io.BytesIO()\n",
    "#         pickle.dump(df, pickle_buffer)\n",
    "#         target_file_key = os.path.join(target_path, '{}.pkl'.format(d)).replace(\"\\\\\",\"/\")\n",
    "#         result = self.bucket.put_object(target_file_key, pickle_buffer.getvalue())\n",
    "#         if result.status == 200:\n",
    "#             return True\n",
    "#         else:\n",
    "#             return False\n",
    "\n",
    "\n",
    "#     def list_files(self,prefix = None):\n",
    "#         res = []\n",
    "#         for object_info in oss2.ObjectIterator(self.bucket,prefix):\n",
    "#             print(object_info.key)\n",
    "#             res.append(object_info.key)\n",
    "#         return res\n",
    "\n",
    "#     def upload_parquet_data(self,df, target_path, *args, report_date=None):\n",
    "#         if isinstance(report_date, datetime.date):\n",
    "#             d = report_date.strftime(\"%Y-%m-%d\")\n",
    "#         if isinstance(report_date, str):\n",
    "#             d = parser.parse(report_date).strftime(\"%Y-%m-%d\")\n",
    "#         if args:\n",
    "#             d = [arg for arg in args][0]\n",
    "#         target_file_key = os.path.join(target_path, '{}.parquet'.format(d)).replace(\"\\\\\",\"/\")\n",
    "#         mem_buffer = newBytes()\n",
    "#         df.to_parquet('noname', engine='fastparquet', open_with=lambda x, y: mem_buffer)\n",
    "#         result = self.bucket.put_object(target_file_key, mem_buffer.getvalue())\n",
    "#         #f = Path(os.getcwd())/'tmp.parquet'\n",
    "#         #write(f, df)\n",
    "#         #with open(f, 'rb') as fileobj:\n",
    "#         #    result = self.bucket.put_object(target_file_key, fileobj)\n",
    "#         #os.remove(f)\n",
    "#         if result.status == 200:\n",
    "#             return True\n",
    "#         else:\n",
    "#             return False\n",
    "\n",
    "#     def save_data_to_pickle(self,df, file_dir_path, *args,report_date=None):\n",
    "#         if isinstance(report_date, datetime.date):\n",
    "#             d = report_date.strftime(\"%Y-%m-%d\")\n",
    "#         if isinstance(report_date, str):\n",
    "#             d = parser.parse(report_date).strftime(\"%Y-%m-%d\")\n",
    "#         if args:\n",
    "#             d = [arg for arg in args][0]\n",
    "#         target_file_key = os.path.join(file_dir_path, '{}.pkl'.format(d))\n",
    "#         with open(target_file_key, 'wb') as f:\n",
    "#             pickle.dump(df, f)\n",
    "\n",
    "#     def save_data_to_parquet(self,df, file_dir_path, *args,report_date=None):\n",
    "#         if isinstance(report_date, datetime.date):\n",
    "#             d = report_date.strftime(\"%Y-%m-%d\")\n",
    "#         if isinstance(report_date, str):\n",
    "#             d = parser.parse(report_date).strftime(\"%Y-%m-%d\")\n",
    "#         if args:\n",
    "#             d = [arg for arg in args][0]\n",
    "#         target_file_key = os.path.join(file_dir_path, f'{d}.parquet')\n",
    "#         df.to_parquet(target_file_key)\n",
    "\n",
    "\n",
    "#     def read_oss_pickle_file(self,object_name):\n",
    "#         \"\"\"\n",
    "#             download_file_to_fileobj：下载文件到文件流对象。由于get_object接口返回的是一个stream流，需要执行read()后才能计算出返回Object数据的CRC checksum，因此需要在调用该接口后做CRC校验。\n",
    "#         \"\"\"\n",
    "#         object_stream = self.bucket.get_object(object_name) #<yourObjectName>\n",
    "#         result = object_stream.read()\n",
    "#         if object_stream.client_crc != object_stream.server_crc:\n",
    "#             print(\"The CRC checksum between client and server is inconsistent!\")\n",
    "#             result = None\n",
    "#         return pickle.loads(result)\n",
    "\n",
    "#     def read_oss_parquet_file(self,object_name):\n",
    "#         \"\"\"\n",
    "#             download_file_to_fileobj：下载文件到文件流对象。由于get_object接口返回的是一个stream流，需要执行read()后才能计算出返回Object数据的CRC checksum，因此需要在调用该接口后做CRC校验。\n",
    "#         \"\"\"\n",
    "#         object_stream = self.bucket.get_object(object_name) #<yourObjectName>\n",
    "#         result = object_stream.read()\n",
    "#         i = io.BytesIO(result)\n",
    "#         if object_stream.client_crc != object_stream.server_crc:\n",
    "#             print(\"The CRC checksum between client and server is inconsistent!\")\n",
    "#             result = None\n",
    "#         return pd.read_parquet(i)\n",
    "\n",
    "\n",
    "#     def download_file_to_loaclfilepath(self, object_name, local_file_path):\n",
    "#         \"\"\"\n",
    "#             download_file_to_loaclfilepath：下载文件到本地路径。get_object和get_object_to_file的区别是前者是获取文件流实例，可用于代码处理和远程调用参赛。后者是存储到本地路径，返回的是一个http状态的json结果\n",
    "#         \"\"\"\n",
    "#         result = self.bucket.get_object_to_file(object_name, local_file_path) # ('<yourObjectName>', '<yourLocalFile>')\n",
    "#         if result.status == 200:\n",
    "#             return True\n",
    "#         else:\n",
    "#             return False\n",
    "\n",
    "#     def generate_temporary_download_url(self,object_name):\n",
    "#         \"\"\"\n",
    "#             generate_temporary_download_url: 生成加签的临时URL以供授信用户下载。一般在实际业务中，我们是提供给调用方一个临时下载链接，来让其获取文件数据，而不是直接使用以上暴露AccessKeyId和AccessKeySecret的方法。\n",
    "#             因此一般我们会存储某条数据oss的路径（<yourObjectName>）与调用方某个唯一标识的对应关系（如手机号身份证号），在调用方请求时，通过该标识获取其数据的oss文件路径（<yourObjectName>），\n",
    "#             然后制定过期时间，为其生成临时下载链接\n",
    "#             http://bucketname.oss-ap-south-1.aliyuncs.com/abc/efg/0?OSSAccessKeyId=LTA************oN9&Expires=1604638842&Signature=tPgvWz*************Uk%3D\n",
    "#         \"\"\"\n",
    "#         res_temporary_url = self.bucket.sign_url('GET', object_name, 60, slash_safe=True)\n",
    "#         return res_temporary_url\n",
    "\n",
    "\n",
    "# oss_client = OssClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "346b16f1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from setting.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import oss2\n",
    "from setting import SETTINGS\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "from dateutil import parser\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from fastparquet import write\n",
    "from pathlib import  Path\n",
    "\n",
    "AccessKeyId = SETTINGS[\"oss.accesskey\"]\n",
    "AccessKeySecret = SETTINGS[\"oss.secret\"]\n",
    "BucketName = SETTINGS[\"oss.bucketname\"]\n",
    "Endpoint = SETTINGS[\"oss.endpoint\"]\n",
    "\n",
    "\n",
    "class newBytes(io.BytesIO):\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class OssClient(object):\n",
    "    __instance = None\n",
    "    __first_init = False\n",
    "\n",
    "    # 单例模式\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        if not cls.__instance:\n",
    "            cls.__instance = super().__new__(cls)\n",
    "        return cls.__instance\n",
    "\n",
    "    def __init__(self):\n",
    "        cls = self.__class__\n",
    "        if not cls.__first_init:\n",
    "            self.auth = oss2.Auth(AccessKeyId, AccessKeySecret)\n",
    "            self.bucket = oss2.Bucket(self.auth, Endpoint, BucketName)\n",
    "            cls.__first_init = True\n",
    "\n",
    "\n",
    "    def upload_file_from_fileobj(self, object_name, local_file_path):\n",
    "        \"\"\"\n",
    "            upload_file_from_fileobj方法：上传文件对象到oss存储空间, 该方法可用于我们从上游服务接收了图片参数，然后以二进制形式读文件，上传到oss存储空间指定位置（abc/efg/00），\n",
    "        当然也可以将本地文件上传到oss我们的bucket. 其中fileobj不止可以是文件对象，也可以是本地文件路径。 put_object方法底层仍是RESTful API的调用，可以指定headers，规定Content-Type等内容\n",
    "        \"\"\"\n",
    "        # 判断bucket中文件是否存在，也可以不判断，会上传更新\n",
    "        #exist = self.bucket.object_exists(object_name) #<yourObjectName>\n",
    "        #if exist:\n",
    "        #    return True\n",
    "        with open(local_file_path, 'rb') as fileobj:\n",
    "            result = self.bucket.put_object(object_name, fileobj) #<yourObjectName>\n",
    "        if result.status == 200:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def upload_pickle_data(self,df, target_path, *args, report_date=None):\n",
    "        if isinstance(report_date, datetime.date):\n",
    "            d = report_date.strftime(\"%Y-%m-%d\")\n",
    "        if isinstance(report_date, str):\n",
    "            d = parser.parse(report_date).strftime(\"%Y-%m-%d\")\n",
    "        if args:\n",
    "            d = [arg for arg in args][0]\n",
    "        pickle_buffer = io.BytesIO()\n",
    "        pickle.dump(df, pickle_buffer)\n",
    "        target_file_key = os.path.join(target_path, '{}.pkl'.format(d)).replace(\"\\\\\",\"/\")\n",
    "        result = self.bucket.put_object(target_file_key, pickle_buffer.getvalue())\n",
    "        if result.status == 200:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def list_files(self,prefix = None):\n",
    "        res = []\n",
    "        for object_info in oss2.ObjectIterator(self.bucket,prefix):\n",
    "            print(object_info.key)\n",
    "            res.append(object_info.key)\n",
    "        return res\n",
    "\n",
    "    def upload_parquet_data(self,df, target_path, *args, report_date=None):\n",
    "        if isinstance(report_date, datetime.date):\n",
    "            d = report_date.strftime(\"%Y-%m-%d\")\n",
    "        if isinstance(report_date, str):\n",
    "            d = parser.parse(report_date).strftime(\"%Y-%m-%d\")\n",
    "        if args:\n",
    "            d = [arg for arg in args][0]\n",
    "        target_file_key = os.path.join(target_path, '{}.parquet'.format(d)).replace(\"\\\\\",\"/\")\n",
    "        mem_buffer = newBytes()\n",
    "        df.to_parquet('noname', engine='fastparquet', open_with=lambda x, y: mem_buffer)\n",
    "        result = self.bucket.put_object(target_file_key, mem_buffer.getvalue())\n",
    "        #f = Path(os.getcwd())/'tmp.parquet'\n",
    "        #write(f, df)\n",
    "        #with open(f, 'rb') as fileobj:\n",
    "        #    result = self.bucket.put_object(target_file_key, fileobj)\n",
    "        #os.remove(f)\n",
    "        if result.status == 200:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def save_data_to_pickle(self,df, file_dir_path, *args,report_date=None):\n",
    "        if isinstance(report_date, datetime.date):\n",
    "            d = report_date.strftime(\"%Y-%m-%d\")\n",
    "        if isinstance(report_date, str):\n",
    "            d = parser.parse(report_date).strftime(\"%Y-%m-%d\")\n",
    "        if args:\n",
    "            d = [arg for arg in args][0]\n",
    "        target_file_key = os.path.join(file_dir_path, '{}.pkl'.format(d))\n",
    "        with open(target_file_key, 'wb') as f:\n",
    "            pickle.dump(df, f)\n",
    "\n",
    "    def save_data_to_parquet(self,df, file_dir_path, *args,report_date=None):\n",
    "        if isinstance(report_date, datetime.date):\n",
    "            d = report_date.strftime(\"%Y-%m-%d\")\n",
    "        if isinstance(report_date, str):\n",
    "            d = parser.parse(report_date).strftime(\"%Y-%m-%d\")\n",
    "        if args:\n",
    "            d = [arg for arg in args][0]\n",
    "        target_file_key = os.path.join(file_dir_path, f'{d}.parquet')\n",
    "        df.to_parquet(target_file_key)\n",
    "\n",
    "\n",
    "    def read_oss_pickle_file(self,object_name):\n",
    "        \"\"\"\n",
    "            download_file_to_fileobj：下载文件到文件流对象。由于get_object接口返回的是一个stream流，需要执行read()后才能计算出返回Object数据的CRC checksum，因此需要在调用该接口后做CRC校验。\n",
    "        \"\"\"\n",
    "        object_stream = self.bucket.get_object(object_name) #<yourObjectName>\n",
    "        result = object_stream.read()\n",
    "        if object_stream.client_crc != object_stream.server_crc:\n",
    "            print(\"The CRC checksum between client and server is inconsistent!\")\n",
    "            result = None\n",
    "        return pickle.loads(result)\n",
    "\n",
    "    def read_oss_parquet_file(self,object_name):\n",
    "        \"\"\"\n",
    "            download_file_to_fileobj：下载文件到文件流对象。由于get_object接口返回的是一个stream流，需要执行read()后才能计算出返回Object数据的CRC checksum，因此需要在调用该接口后做CRC校验。\n",
    "        \"\"\"\n",
    "        object_stream = self.bucket.get_object(object_name) #<yourObjectName>\n",
    "        result = object_stream.read()\n",
    "        i = io.BytesIO(result)\n",
    "        if object_stream.client_crc != object_stream.server_crc:\n",
    "            print(\"The CRC checksum between client and server is inconsistent!\")\n",
    "            result = None\n",
    "        return pd.read_parquet(i)\n",
    "\n",
    "\n",
    "    def download_file_to_loaclfilepath(self, object_name, local_file_path):\n",
    "        \"\"\"\n",
    "            download_file_to_loaclfilepath：下载文件到本地路径。get_object和get_object_to_file的区别是前者是获取文件流实例，可用于代码处理和远程调用参赛。后者是存储到本地路径，返回的是一个http状态的json结果\n",
    "        \"\"\"\n",
    "        result = self.bucket.get_object_to_file(object_name, local_file_path) # ('<yourObjectName>', '<yourLocalFile>')\n",
    "        if result.status == 200:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def generate_temporary_download_url(self,object_name):\n",
    "        \"\"\"\n",
    "            generate_temporary_download_url: 生成加签的临时URL以供授信用户下载。一般在实际业务中，我们是提供给调用方一个临时下载链接，来让其获取文件数据，而不是直接使用以上暴露AccessKeyId和AccessKeySecret的方法。\n",
    "            因此一般我们会存储某条数据oss的路径（<yourObjectName>）与调用方某个唯一标识的对应关系（如手机号身份证号），在调用方请求时，通过该标识获取其数据的oss文件路径（<yourObjectName>），\n",
    "            然后制定过期时间，为其生成临时下载链接\n",
    "            http://bucketname.oss-ap-south-1.aliyuncs.com/abc/efg/0?OSSAccessKeyId=LTA************oN9&Expires=1604638842&Signature=tPgvWz*************Uk%3D\n",
    "        \"\"\"\n",
    "        res_temporary_url = self.bucket.sign_url('GET', object_name, 60, slash_safe=True)\n",
    "        return res_temporary_url\n",
    "\n",
    "\n",
    "oss_client = OssClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce9978b9-01c7-4463-b253-6c0f5e27428e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from oss_handler.ipynb\n",
      "LTAI5tN7V51xtxUmaKxmRsWo rmqsMszRtvurcckVMD9M14riQYshi6 2nd-data oss-cn-shenzhen.aliyuncs.com\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from oss_handler import OssClient\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from pylab import mpl\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from oss2.exceptions import NoSuchKey\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "oss_client = OssClient()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "313b3ed7-f1a0-4394-a548-c55d5f1e96f8",
   "metadata": {},
   "source": [
    "https://www.ricequant.com/doc/rqdata/python/fundamentals-dictionary.html#%E5%88%A9%E6%B6%A6%E8%A1%A8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc26d6d-0605-4981-9364-4043d1c53938",
   "metadata": {},
   "source": [
    "# 说明"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15085fe6-b2c7-4c0a-b15a-ca6de9b92e0f",
   "metadata": {},
   "source": [
    "## Raw -- 已经完成的代码，但是目前用不到。\n",
    "## Markdown -- 需要改进的代码，不能使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2b436c-f6af-42cd-b697-cf0211f3b100",
   "metadata": {},
   "source": [
    "# 数据调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2fc18ea-b316-4402-bcab-1f200c46aaf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# idx000001 = oss_client.read_oss_pickle_file('ad_hoc_prod/index_hist/000001.XSHG_hist_index.pkl') #上证指数\n",
    "# idx000016 = oss_client.read_oss_pickle_file('ad_hoc_prod/index_hist/000016.XSHG_hist_index.pkl') #上证 50\n",
    "# idx000300 = oss_client.read_oss_pickle_file('ad_hoc_prod/index_hist/000300.XSHG_hist_index.pkl') #沪深 300\n",
    "idx000852 = oss_client.read_oss_pickle_file('ad_hoc_prod/index_hist/000852.XSHG_hist_index.pkl') #中证 1000\n",
    "idx000905 = oss_client.read_oss_pickle_file('ad_hoc_prod/index_hist/000905.XSHG_hist_index.pkl') #中证 500\n",
    "idx000985 = oss_client.read_oss_pickle_file('ad_hoc_prod/index_hist/000985.XSHG_hist_index.pkl') #中证全指\n",
    "# idx399001 = oss_client.read_oss_pickle_file('ad_hoc_prod/index_hist/399001.XSHG_hist_index.pkl') #深证成指\n",
    "# idx399006 = oss_client.read_oss_pickle_file('ad_hoc_prod/index_hist/399006.XSHG_hist_index.pkl') #创业板指\n",
    "# idx399303 = oss_client.read_oss_pickle_file('ad_hoc_prod/index_hist/399303.XSHG_hist_index.pkl') #国证 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16c470c6-4dbb-4b64-b225-cf3ecfc31d63",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "adj_close = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/adj_close.pk')\n",
    "adj_factor = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/adj_factor.pk')\n",
    "adj_high = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/adj_high.pk')\n",
    "adj_low = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/adj_low.pk')\n",
    "adj_open = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/adj_open.pk')\n",
    "\n",
    "circulation_a = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/circulation_a.pk') #流通股本\n",
    "total_a = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/total_a.pk') #总股本\n",
    "circulation_market_value = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/circulation_market_value.pk') #流通市值\n",
    "\n",
    "volume = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/volume.pk') #成交量\n",
    "num_trades = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/num_trades.pk') #交易次数\n",
    "total_turnover = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/total_turnover.pk') #成交额\n",
    "turnover_rate = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/turnover_rate.pk') #换手率\n",
    "\n",
    "close = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/close.pk') \n",
    "open = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/open.pk') \n",
    "high = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/high.pk') \n",
    "low = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/low.pk') \n",
    "\n",
    "limit_down = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/limit_down.pk') #跌停\n",
    "limit_up = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/limit_up.pk') #涨停\n",
    "\n",
    "halt_status = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/halt_status.pk') #停牌 \n",
    "st_status = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/st_status.pk') #st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bb87964-5fd6-4f67-8775-f91166c4f166",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BS_pit = oss_client.read_oss_pickle_file('ad_hoc_prod/pit_fundemental/BS_pit.pkl')\n",
    "CF_pit = oss_client.read_oss_pickle_file('ad_hoc_prod/pit_fundemental/CF_pit.pkl')\n",
    "IS_pit = oss_client.read_oss_pickle_file('ad_hoc_prod/pit_fundemental/IS_pit.pkl')\n",
    "all_pit = oss_client.read_oss_pickle_file('ad_hoc_prod/pit_fundemental/all_pit_fund_new_api.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5585258a-d78b-4e7e-bd0c-c95dfcca4fb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##正常交易非st，halt的票打1.其他为np.nan\n",
    "st_status = st_status.replace(True,np.nan).replace(False,True)\n",
    "halt_status = halt_status.replace(True,np.nan).replace(False,True)\n",
    "\n",
    "adj_return =  adj_close/adj_close.shift(1) - 1\n",
    "adj_return = adj_return.replace(np.inf,np.nan).replace(-np.inf,np.nan)\n",
    "\n",
    "##调仓周期\n",
    "o1o2 = adj_open.shift(-2)/adj_open.shift(-1) - 1 #1日调仓\n",
    "# o1o2 = adj_open.shift(-5) / adj_open.shift(-1) - 1 #5日调仓\n",
    "# o1o2 = adj_open.shift(-10) / adj_open.shift(-1) - 1 #10日调仓\n",
    "# o1o2 = adj_open.shift(-20) / adj_open.shift(-1) - 1 #20日调仓\n",
    "\n",
    "mc = total_a * adj_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51be4e56-9eb8-4b1c-a3df-31382a81f05f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csi500 = idx000905[['open','close','low']].set_index(idx000905['date'])\n",
    "idx_500_ret = (csi500['close']/csi500['close'].shift(1) - 1)\n",
    "\n",
    "mclose = csi500['close']\n",
    "mopen = csi500['open']\n",
    "mlow = csi500['low']\n",
    "\n",
    "open['label'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d60cef6-c141-4a3e-87d8-d063bb98032c",
   "metadata": {},
   "source": [
    "# 中性化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e5ec0df-b551-4cf4-a53f-df2cdf52c0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>logcmv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8272228</th>\n",
       "      <td>2025-02-17</td>\n",
       "      <td>689009</td>\n",
       "      <td>21.842942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              date  ticker     logcmv\n",
       "8272228 2025-02-17  689009  21.842942"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logcmv = np.log(circulation_market_value)\n",
    "logcmv = logcmv.loc['2017':]\n",
    "logcmv = logcmv.stack().reset_index()\n",
    "logcmv.columns = ['date','ticker','logcmv']\n",
    "logcmv.to_pickle('D:/redata/risk_factors/logcmv.pkl')\n",
    "logcmv.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94c54dde-1d75-4ca9-a054-044bc52c819d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>retsum120</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8013866</th>\n",
       "      <td>2025-02-17</td>\n",
       "      <td>689009</td>\n",
       "      <td>0.249438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              date  ticker  retsum120\n",
       "8013866 2025-02-17  689009   0.249438"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retsum120 = adj_return.rolling(window=120).sum()\n",
    "retsum120 = retsum120.loc['2017':]\n",
    "retsum120 = retsum120.stack().reset_index()\n",
    "retsum120.columns = ['date','ticker','retsum120']\n",
    "retsum120.to_pickle('D:/redata/risk_factors/retsum120.pkl')\n",
    "retsum120.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c78c617-47ed-45a4-95b2-8f8c4305728d",
   "metadata": {},
   "source": [
    "# 行业打标多天"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9aa089-7a36-472c-a7f5-935828470fc6",
   "metadata": {},
   "source": [
    "ind = pd.read_pickle('D:/redata/risk_factors/ind_label_multiple_dates.pkl')\n",
    "ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de42fab2-9df2-4e72-92fd-4fd3298e2b5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "starttime = '2025-02-10'\n",
    "endtime = '2025-02-14'\n",
    "\n",
    "ind_label_proc = pd.DataFrame(columns= ['date','ticker','first_industry_code'])\n",
    "\n",
    "# 定义一个函数来生成工作日日期范围\n",
    "def date_range(start, end):\n",
    "    start_date = datetime.strptime(start, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end, '%Y-%m-%d')\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        if current_date.weekday() < 5:  # 0-4代表周一到周五\n",
    "            yield current_date.strftime('%Y-%m-%d')\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "for date in date_range(starttime, endtime):\n",
    "    ind_label_raw = oss_client.read_oss_pickle_file(f'ad_hoc_prod/sector/{date}.pkl')\n",
    "    temp_df = pd.DataFrame(columns=['date', 'ticker', 'first_industry_code'])\n",
    "    temp_df['ticker'] = ind_label_raw.index\n",
    "    temp_df['ticker'] = temp_df['ticker'].str.extract(r'(\\d+)')\n",
    "    temp_df['date'] = ind_label_raw['date'].values\n",
    "    temp_df['first_industry_code'] = ind_label_raw['first_industry_code'].values\n",
    "    \n",
    "    # 将临时 DataFrame 追加到最终的 DataFrame\n",
    "    # ind_label_proc = ind_label_proc.append(temp_df, ignore_index=True)\n",
    "    ind_label_proc = pd.concat([ind_label_proc, temp_df], ignore_index=True)\n",
    "\n",
    "temp_df = ind_label_proc\n",
    "\n",
    "df1 = pd.DataFrame(columns= ['date','ticker','10','11','12','20','21','22','23','24','25','26','27','28','30','31','32','33','34','35','36','37','40','41','42','43','50','60','61','62','63','70','999'])\n",
    "df1['date'] = temp_df['date']\n",
    "df1['ticker'] = temp_df['ticker']\n",
    "\n",
    "for i, code in enumerate(temp_df['first_industry_code']):\n",
    "    if code in df1.columns:\n",
    "        df1.at[i, code] = 1\n",
    "    else:\n",
    "        df1.at[i, code] = 0\n",
    "        \n",
    "df1 = df1.fillna(0)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e2d618-dcf9-408b-8df0-79001d34451d",
   "metadata": {},
   "source": [
    "ind_label = pd.concat([ind, df1], ignore_index=True)\n",
    "ind_label['date'] = pd.to_datetime(ind_label['date'])\n",
    "ind_label = ind_label.reset_index(drop=True)\n",
    "ind_label = pd.DataFrame(ind_label)\n",
    "ind_label.to_pickle('D:/redata/risk_factors/ind_label_multiple_dates.pkl')\n",
    "ind_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a9a839-1d6c-4ce2-862f-d77aca9b3e41",
   "metadata": {},
   "source": [
    "# 单天打标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8019b557-7e88-4fb7-8118-7f9acbc46bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>50</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9041329</th>\n",
       "      <td>2025-02-21</td>\n",
       "      <td>689009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date  ticker   10   11   12   20   21   22   23   24  ...   41  \\\n",
       "9041329 2025-02-21  689009  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "\n",
       "          42   43   50   60   61   62   63   70  999  \n",
       "9041329  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[1 rows x 33 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ind = pd.read_pickle('D:/redata/risk_factors/ind_label_multiple_dates.pkl')\n",
    "ind = pd.read_pickle('D:/redata/risk_factors/ind_label.pkl')\n",
    "ind.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "773a4a5c-1557-448b-b379-ca18f1876c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sensen.t\\AppData\\Local\\Temp\\ipykernel_13820\\470789045.py:26: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df1 = df1.fillna(0)\n"
     ]
    }
   ],
   "source": [
    "date_data = '2025-02-24'\n",
    "\n",
    "ind_label_raw = oss_client.read_oss_pickle_file(f'ad_hoc_prod/sector/{date_data}.pkl')\n",
    "\n",
    "ind_label_proc = pd.DataFrame(columns= ['date','ticker','first_industry_code'])\n",
    "\n",
    "temp_df = pd.DataFrame(columns=['date', 'ticker', 'first_industry_code'])\n",
    "temp_df['ticker'] = ind_label_raw.index\n",
    "temp_df['ticker'] = temp_df['ticker'].str.extract(r'(\\d+)')\n",
    "temp_df['date'] = ind_label_raw['date'].values\n",
    "temp_df['first_industry_code'] = ind_label_raw['first_industry_code'].values\n",
    "ind_label_proc = pd.concat([ind_label_proc, temp_df], ignore_index=True)\n",
    "\n",
    "temp_df = ind_label_proc\n",
    "\n",
    "df1 = pd.DataFrame(columns= ['date','ticker','10','11','12','20','21','22','23','24','25','26','27','28','30','31','32','33','34','35','36','37','40','41','42','43','50','60','61','62','63','70','999'])\n",
    "df1['date'] = temp_df['date']\n",
    "df1['ticker'] = temp_df['ticker']\n",
    "\n",
    "for i, code in enumerate(temp_df['first_industry_code']):\n",
    "    if code in df1.columns:\n",
    "        df1.at[i, code] = 1\n",
    "    else:\n",
    "        df1.at[i, code] = 0\n",
    "        \n",
    "df1 = df1.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ca82573-69fd-4ded-a7b7-ab9dbd0cd614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>50</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>000004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>000005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>000006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9046463</th>\n",
       "      <td>2025-02-24</td>\n",
       "      <td>688799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9046464</th>\n",
       "      <td>2025-02-24</td>\n",
       "      <td>688800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9046465</th>\n",
       "      <td>2025-02-24</td>\n",
       "      <td>688819</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9046466</th>\n",
       "      <td>2025-02-24</td>\n",
       "      <td>688981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9046467</th>\n",
       "      <td>2025-02-24</td>\n",
       "      <td>689009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9046468 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date  ticker   10   11   12   20   21   22   23   24  ...   41  \\\n",
       "0       2016-01-04  000001  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "1       2016-01-04  000002  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "2       2016-01-04  000004  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "3       2016-01-04  000005  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "4       2016-01-04  000006  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "...            ...     ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "9046463 2025-02-24  688799  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "9046464 2025-02-24  688800  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "9046465 2025-02-24  688819  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "9046466 2025-02-24  688981  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "9046467 2025-02-24  689009  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "\n",
       "          42   43   50   60   61   62   63   70  999  \n",
       "0        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1        1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3        1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4        1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "9046463  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9046464  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9046465  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9046466  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9046467  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[9046468 rows x 33 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_label = pd.concat([ind, df1], ignore_index=True)\n",
    "ind_label['date'] = pd.to_datetime(ind_label['date'])\n",
    "ind_label = ind_label.reset_index(drop=True)\n",
    "ind_label = pd.DataFrame(ind_label)\n",
    "ind_label.to_pickle('D:/redata/risk_factors/ind_label.pkl')\n",
    "ind_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4bf83e-2cc2-462e-b83d-3c573b4ac4fe",
   "metadata": {},
   "source": [
    "# 动量市值中性化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7bf4c47f-e734-4f69-99b0-cd8b523b4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "##因子中性化 需要的数据\n",
    "risk1 = pd.read_pickle('D:/redata/risk_factors/logcmv.pkl')\n",
    "risk2 = pd.read_pickle('D:/redata/risk_factors/retsum120.pkl')\n",
    "ind = pd.read_pickle('D:/redata/risk_factors/ind_label.pkl')\n",
    "# ind = pd.read_pickle('D:/redata/risk_factor/ind_label.pk')\n",
    "risk = risk1.merge(risk2,on=['date','ticker'],how='left')\n",
    "risk = risk.merge(ind,on=['date','ticker'],how='left')\n",
    "risk = risk[(risk['date']>'2017-01-01')]\n",
    "risk = risk.drop_duplicates(keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "68f24210-e6b1-48dd-8bcf-c1f06bedd618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##行业中性 2023.8.3 更新\n",
    "def neturalize_ind(factor,risk=risk,risk_cols=['logcmv','retsum120']):\n",
    "    XY = pd.merge(risk, factor, on=['date', 'ticker'], how='right')\n",
    "    #print(XY.head(3))\n",
    "    XY = XY.dropna()\n",
    "    tradeday = XY.date.unique()\n",
    "    daily_resid = []\n",
    "    for date in tqdm(tradeday[:]):\n",
    "        xy = XY[XY['date'] == date]\n",
    "        #只保留有因子值的数据\n",
    "        xy = xy.dropna()\n",
    "\n",
    "        #数据标准化\n",
    "        xy[risk_cols] = (xy[risk_cols] - xy[risk_cols].mean())/xy[risk_cols].std()                \n",
    "        xy['factor'] = (xy['factor']-xy['factor'].mean())/xy['factor'].std() \n",
    "\n",
    "        #选出x y\n",
    "        x = xy.iloc[:,2:-1]\n",
    "        y = xy['factor']\n",
    "        #回归\n",
    "        multi_linear = LinearRegression(fit_intercept=False)  #无截距项\n",
    "        multi_linear.fit(x, y)\n",
    "        beta = multi_linear.coef_\n",
    "        \n",
    "        #取残差\n",
    "        xy['beta'] = np.dot(x, beta)\n",
    "        xy['resid'] = xy['factor'] - xy['beta']\n",
    "        daily_resid.append(xy)\n",
    "    data = pd.concat(daily_resid)\n",
    "    factor = data[['date', 'ticker', 'factor', 'resid']]\n",
    "    return factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fbe64a8d-94c3-4d62-8878-0da429cba847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def factor_neutralize(df,factor_type,if_rank=True,risk=None,risk_cols=None): #factor_type=hist原值；ind行业及其他中性；单个风险因子文件名； if_rank：中性化之前时候对因子做rank处理\n",
    "    #中性之前是否对因子做rank处理\n",
    "    if if_rank == True:\n",
    "        df = df.rank(axis=1, ascending=True)  # 原值越大rank值越大   \n",
    "    #因子原值\n",
    "    if factor_type == 'hist':\n",
    "        factor_rank = df.copy()\n",
    "    #行业市值中性\n",
    "    elif factor_type == 'ind':  ###行业和风格中性\n",
    "        def neu_process(df):\n",
    "            df = df.stack().reset_index()\n",
    "            df.columns = ['date','ticker','factor']\n",
    "            df = neturalize_ind(df,risk,risk_cols)\n",
    "            resid = df.set_index(['date','ticker'])['resid'].unstack()\n",
    "            return resid        \n",
    "        factor_rank = neu_process(df)\n",
    "        factor_rank = factor_rank.rank(axis=1,ascending=True) \n",
    "        \n",
    "    return factor_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c2f8b531-badb-4f37-ad73-e5017fde0182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def neu_process(df):\n",
    "    df = df.stack().reset_index()\n",
    "    df.columns = ['date','ticker','factor']\n",
    "    df = neturalize_ind(df,risk=risk,risk_cols=['logcmv','retsum120'])\n",
    "    resid = df.set_index(['date','ticker'])['resid'].unstack()\n",
    "    return resid    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7b55a523-9d75-45dd-9271-dffa1157dc27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_num_stocks(n):\n",
    "    select_stocks = (factor_rank <= n)\n",
    "    select_stocks_last = select_stocks.iloc[-1:,]\n",
    "    select_stocks_last = select_stocks_last.replace(True,1).replace(False,np.nan)\n",
    "    select_stocks_last = select_stocks_last.dropna(how='all',axis=1)\n",
    "    return select_stocks_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "72bdc204-553b-42d8-af4b-f6d9ecc1fafe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##市值排uni\n",
    "mv_rank = risk1.set_index(['date','ticker'])['logcmv'].unstack()\n",
    "mv_rank = mv_rank.rank(axis=1, ascending=True, pct=True)\n",
    "\n",
    "##小市值\n",
    "small = (mv_rank <= 0.33)\n",
    "small = small.replace(True,1).replace(False,np.nan)\n",
    "small = small.loc['2017':]\n",
    "\n",
    "##中市值\n",
    "median = (mv_rank <= 0.66) & (mv_rank >= 0.33)\n",
    "median = median.replace(True,1).replace(False,np.nan)\n",
    "median = median.loc['2017':]\n",
    "\n",
    "##小中市值\n",
    "sm = (mv_rank <= 0.66)\n",
    "sm = sm.replace(True,1).replace(False,np.nan)\n",
    "sm = sm.loc['2017':]\n",
    "\n",
    "##大市值\n",
    "large = (mv_rank>=0.66)\n",
    "large = large.replace(True,1).replace(False,np.nan)\n",
    "large = large.loc['2017':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "382995ed-034e-49e0-bfa1-f580d56b2247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def position(df, period):\n",
    "    empty = df > 0\n",
    "    empty = empty.replace(True,np.nan).replace(False,np.nan)\n",
    "    \n",
    "    position = df.iloc[::period]\n",
    "    position = pd.concat([empty, position])\n",
    "    position = position.sort_values(['date'])\n",
    "    position = position.reset_index()\n",
    "    position = position.drop_duplicates(subset=['date'], keep='last').ffill(limit = period)\n",
    "    position = position.set_index(['date'])\n",
    "    return position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcac2e1-c185-48f0-9028-4d4519c9e5e7",
   "metadata": {},
   "source": [
    "# 财务数据函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "44586ec4-66b0-40db-a392-3416d570618d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chg_format(df1):\n",
    "    df1['ticker'] = df1['order_book_id'].str[:-5]\n",
    "    df1['date'] = pd.to_datetime(df1['info_date'])\n",
    "    df1['report_period'] = df1['quarter'].map(lambda x:x.replace('q1','0331').replace('q2','0630').replace('q3','0930').replace('q4','1231'))\n",
    "    df1['report_period'] = df1['report_period'].astype('int')\n",
    "    df1 = df1.sort_values(['date','ticker','report_period']).reset_index(drop=True)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ede5923a-21ed-46af-ba48-8c9308b06c10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def func_save(factor,factor_name):\n",
    "    ##只保留当前最新业绩的数据\n",
    "    def func(x):\n",
    "        x = x[x['report_period']>=x['report_period'].expanding(min_periods=1).max()]\n",
    "        return x\n",
    "    factor = factor.groupby('ticker').apply(func).reset_index(drop=True)\n",
    "    #factor同一个date ticker 去重，保留当前最新report_period\n",
    "    factor = factor.drop_duplicates(subset=['date','ticker'],keep='last')\n",
    "    factor = factor.set_index(['date','ticker'])[factor_name].unstack()\n",
    "    factor = pd.concat([factor,Open[['label']]],axis=1).drop(['label'],axis=1)\n",
    "    factor = factor.fillna(method='ffill',limit=150).dropna(how='all',axis=0).dropna(how='all',axis=1)\n",
    "    factor.to_pickle(os.path.join(path2,factor_name+'.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "408bc082-d6d6-4825-a041-735dd3e88b54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##利润表流量单季度数据\n",
    "dfaa = IS_pit\n",
    "dfaa = dfaa.reset_index()\n",
    "dfaa = chg_format(dfaa)\n",
    "dfaa = dfaa.drop_duplicates(subset=['date','ticker','report_period'],keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "050652cc-03de-4e38-8d31-02b9c48d0a02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#时间表\n",
    "mls = pd.DataFrame(dfaa.report_period.unique())\n",
    "mls.columns = ['report_period']\n",
    "mls['pre_report_period'] = mls['report_period'].shift(1)\n",
    "mls['pre2_report_period'] = mls['report_period'].shift(2)\n",
    "mls['pre3_report_period'] = mls['report_period'].shift(3)\n",
    "mls['next1_report_period'] = mls['report_period'].shift(-1)\n",
    "mls['next2_report_period'] = mls['report_period'].shift(-2)\n",
    "mls['next3_report_period'] = mls['report_period'].shift(-3)\n",
    "mls = mls.replace(np.nan,0)\n",
    "mls = mls.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d9ba870a-a1a2-4e18-9004-701783f45f30",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##现金流量单季度数据\n",
    "dfbb = CF_pit\n",
    "dfbb = dfbb.reset_index()\n",
    "dfbb = chg_format(dfbb)\n",
    "dfbb = dfbb.drop_duplicates(subset=['date','ticker','report_period'],keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "03813df5-5861-4590-b172-04768c0fc11a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#时间表\n",
    "mlss = pd.DataFrame(dfbb.report_period.unique())\n",
    "mlss.columns = ['report_period']\n",
    "mlss['pre_report_period'] = mlss['report_period'].shift(1)\n",
    "mlss['pre2_report_period'] = mlss['report_period'].shift(2)\n",
    "mlss['pre3_report_period'] = mlss['report_period'].shift(3)\n",
    "mlss['next1_report_period'] = mlss['report_period'].shift(-1)\n",
    "mlss['next2_report_period'] = mlss['report_period'].shift(-2)\n",
    "mlss['next3_report_period'] = mlss['report_period'].shift(-3)\n",
    "mlss = mlss.replace(np.nan,0)\n",
    "mlss = mlss.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3c83d7db-c504-4570-9caa-67394df54739",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##债务截面单季度数据\n",
    "dfcc = BS_pit\n",
    "dfcc = dfcc.reset_index()\n",
    "dfcc = chg_format(dfcc)\n",
    "dfcc = dfcc.drop_duplicates(subset=['date','ticker','report_period'],keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b07d28b2-a0ed-4ae6-9f6a-fc55bbda70f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#时间表\n",
    "mlsss = pd.DataFrame(dfcc.report_period.unique())\n",
    "mlsss.columns = ['report_period']\n",
    "mlsss['pre_report_period'] = mlsss['report_period'].shift(1)\n",
    "mlsss['pre2_report_period'] = mlsss['report_period'].shift(2)\n",
    "mlsss['pre3_report_period'] = mlsss['report_period'].shift(3)\n",
    "mlsss['next1_report_period'] = mlsss['report_period'].shift(-1)\n",
    "mlsss['next2_report_period'] = mlsss['report_period'].shift(-2)\n",
    "mlsss['next3_report_period'] = mlsss['report_period'].shift(-3)\n",
    "mlsss = mlsss.replace(np.nan,0)\n",
    "mlsss = mlsss.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5cac43ab-8a66-4f95-a3cc-e2e23d10c87e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#IS,CF的单季度计算\n",
    "def single_Q(df1,col):\n",
    "    ##计算单季度数据：原理当季度数据发生更改时，当季的Q和下一季的Q会受到影响。\n",
    "    ##分别计算当季的Q和下一季的Q 然后两个部分合并到一起。 要保证当季的Q和下一季的Q时，用到的数据日期全部不能大于当前日期。\n",
    "    #part1：计算当季的Q\n",
    "    df3 = df1[['date','ticker','quarter','report_period',col]].dropna(subset=[col])\n",
    "    #拼上对应的前3个季度的report_period\n",
    "    df3 = df3.merge(mls[['report_period','pre_report_period']],on=['report_period'],how='left')\n",
    "    df3 = df3.merge(df3,left_on=['ticker','pre_report_period'],right_on=['ticker','report_period'],suffixes=['','_pre'],how='left')\n",
    "    df3 = df3.sort_values(['date','ticker','report_period','date_pre'])\n",
    "    #单季数据=当期-上期 ，【第一季用当期】\n",
    "    df3[col+'_Q'] = df3[col] - df3[col+'_pre']\n",
    "    df3[col+'_Q'] = np.where(df3['quarter'].str[-2:] == 'q1',df3[col],df3[col+'_Q'])\n",
    "    #保留所有Q1和除Q1之外date>=date_pre\n",
    "    df3 = df3[(df3['quarter'].str[-2:]=='q1')|(df3['date']>=df3['date_pre'])] \n",
    "    df3 = df3.drop_duplicates(subset=['date','ticker','report_period'],keep='last') #只保留date_pre最新的一条数据\n",
    "    \n",
    "    #part2：计算下一季Q\n",
    "    tmp = df1[['date','ticker','quarter','report_period',col]].dropna(subset=[col])\n",
    "    #拼上对应的前3个季度的report_period\n",
    "    tmp = tmp.merge(mls[['report_period','next1_report_period']],on=['report_period'],how='left')\n",
    "    tmp = tmp.merge(tmp,left_on=['ticker','next1_report_period'],right_on=['ticker','report_period'],suffixes=['','_next1'],how='left')\n",
    "    tmp[col+'_Q_next1'] = tmp[col+'_next1'] - tmp[col]\n",
    "    tmp[col+'_Q_next1'] = np.where(tmp['quarter'].str[-2:] == 'q4',tmp[col+'_next1'],tmp[col+'_Q_next1']) #如果当前是四季度，\n",
    "    #保留 next_quarter_date<=quarter_date\n",
    "    tmp = tmp[tmp['date_next1']<=tmp['date']]\n",
    "    tmp = tmp.drop_duplicates(subset=['date','ticker','report_period'],keep='last')  #只保留next_quarter_date<=quarter_date，且next_quarter_date最新的\n",
    "    tmp['report_period'] = tmp['report_period_next1']\n",
    "    tmp = tmp[(tmp['quarter'].str[-2:]!='q4')] #去掉所有当前为4季度的情况，如果当前为4季度，即便数据发成更改，也不影响下一季度的单季数据，因为下一季度是一季度\n",
    "    tmp[col+'_Q'] = tmp[col+'_Q_next1']\n",
    "    \n",
    "    #把两个部分合并就是所有发生更新的数据\n",
    "    part1 = df3[['date', 'ticker', 'report_period',col+'_Q']]\n",
    "    part2 = tmp[['date', 'ticker', 'report_period',col+'_Q']]\n",
    "    # part1['label'] = '1' #可删，只是为了识别数据来源\n",
    "    # part2['label'] = '2' #可删，只是为了识别数据来源\n",
    "    part12 = pd.concat([part1,part2])\n",
    "    part12 = part12.sort_values(['date','ticker','report_period'])\n",
    "    part12 = part12.drop_duplicates(subset=['date','ticker','report_period'],keep='last') #删掉重复行\n",
    "    return part12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f39650dc-2541-4604-ac71-7f72f7a9bb34",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#同比yoy\n",
    "def compute_yoy(df4,col):#计算yoy\n",
    "    tool_data = df4[['date','ticker','report_period',col]]\n",
    "    #part1\n",
    "    df1 = df4[['date', 'ticker', 'report_period',col]]\n",
    "    df1['report_period'] = df1['report_period'].astype('int') ###新增\n",
    "    df1['pre_yr_report_period'] = (df1['report_period'].astype('str').str[:4].astype('int')-1).astype('str') + df1['report_period'].astype('str').str[-4:]\n",
    "    df1['pre_yr_report_period'] = df1['pre_yr_report_period'].astype('int')\n",
    "    df1 = df1.merge(tool_data,left_on=['ticker','pre_yr_report_period'],right_on=['ticker','report_period'],how='left',suffixes=['','_pre_yr'])\n",
    "    df1 = df1.sort_values(['date','ticker','report_period','date_pre_yr'])\n",
    "    df1 = df1[df1['date']>=df1['date_pre_yr']]\n",
    "    df1 = df1.drop_duplicates(subset=['date','ticker','report_period'],keep='last')   ##只保留'date','ticker','report_period'对应的'date_pre_yr'最新的一条数据\n",
    "    df1[col+'_yoy'] = (df1[col]-df1[col+'_pre_yr'])/np.abs(df1[col+'_pre_yr'])\n",
    "    #part2\n",
    "    df2 = df4[['date', 'ticker', 'report_period',col]]\n",
    "    df2['report_period'] = df2['report_period'].astype('int') ###新增\n",
    "    df2['next_yr_report_period'] = (df2['report_period'].astype('str').str[:4].astype('int')+1).astype('str') + df2['report_period'].astype('str').str[-4:]\n",
    "    df2['next_yr_report_period'] = df2['next_yr_report_period'].astype('int')\n",
    "    df2 = df2.merge(tool_data,left_on=['ticker','next_yr_report_period'],right_on=['ticker','report_period'],how='left',suffixes=['','_next_yr'])\n",
    "    df2 = df2.sort_values(['date','ticker','report_period','date_next_yr'])\n",
    "    df2 = df2[df2['date']>=df2['date_next_yr']]\n",
    "    df2['report_period'] = df2['report_period_next_yr']\n",
    "    df2 = df2.drop_duplicates(subset=['date','ticker','report_period'],keep='last')   ##只保留'date','ticker','report_period'对应的'date_pre_yr'最新的一条数据\n",
    "    df2[col+'_yoy'] = (df2[col+'_next_yr']-df2[col])/np.abs(df2[col])\n",
    "\n",
    "    #把两个部分合并就是所有发生更新的数据\n",
    "    part1 = df1[['date', 'ticker', 'report_period',col+'_yoy']]\n",
    "    part2 = df2[['date', 'ticker', 'report_period',col+'_yoy']]\n",
    "    part12 = pd.concat([part1,part2])\n",
    "    part12 = part12.sort_values(['date','ticker','report_period'])\n",
    "    part12 = part12.drop_duplicates(subset=['date','ticker','report_period'],keep='last') #删掉重复行\n",
    "    return part12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "562451e5-8e91-4920-ba62-8ca17cd87365",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#T当季-去年同季\n",
    "def compute_T(df4,col):\n",
    "    tool_data = df4[['date','ticker','report_period',col]]\n",
    "    #part1\n",
    "    df1 = df4[['date', 'ticker', 'report_period',col]]\n",
    "    df1['report_period'] = df1['report_period'].astype('int') ###新增\n",
    "    df1['pre_yr_report_period'] = (df1['report_period'].astype('str').str[:4].astype('int')-1).astype('str') + df1['report_period'].astype('str').str[-4:]\n",
    "    df1['pre_yr_report_period'] = df1['pre_yr_report_period'].astype('int')\n",
    "    df1 = df1.merge(tool_data,left_on=['ticker','pre_yr_report_period'],right_on=['ticker','report_period'],how='left',suffixes=['','_pre_yr'])\n",
    "    df1 = df1.sort_values(['date','ticker','report_period','date_pre_yr'])\n",
    "    df1 = df1[df1['date']>=df1['date_pre_yr']]\n",
    "    df1 = df1.drop_duplicates(subset=['date','ticker','report_period'],keep='last')   ##只保留'date','ticker','report_period'对应的'date_pre_yr'最新的一条数据\n",
    "    df1[col+'_T'] = (df1[col]-df1[col+'_pre_yr'])\n",
    "    #part2\n",
    "    df2 = df4[['date', 'ticker', 'report_period',col]]\n",
    "    df2['report_period'] = df2['report_period'].astype('int') ###新增\n",
    "    df2['next_yr_report_period'] = (df2['report_period'].astype('str').str[:4].astype('int')+1).astype('str') + df2['report_period'].astype('str').str[-4:]\n",
    "    df2['next_yr_report_period'] = df2['next_yr_report_period'].astype('int')\n",
    "    df2 = df2.merge(tool_data,left_on=['ticker','next_yr_report_period'],right_on=['ticker','report_period'],how='left',suffixes=['','_next_yr'])\n",
    "    df2 = df2.sort_values(['date','ticker','report_period','date_next_yr'])\n",
    "    df2 = df2[df2['date']>=df2['date_next_yr']]\n",
    "    df2['report_period'] = df2['report_period_next_yr']\n",
    "    df2 = df2.drop_duplicates(subset=['date','ticker','report_period'],keep='last')   ##只保留'date','ticker','report_period'对应的'date_pre_yr'最新的一条数据\n",
    "    df2[col+'_T'] = (df2[col+'_next_yr']-df2[col])\n",
    "\n",
    "    #把两个部分合并就是所有发生更新的数据\n",
    "    part1 = df1[['date', 'ticker', 'report_period',col+'_T']]\n",
    "    part2 = df2[['date', 'ticker', 'report_period',col+'_T']]\n",
    "    part12 = pd.concat([part1,part2])\n",
    "    part12 = part12.sort_values(['date','ticker','report_period'])\n",
    "    part12 = part12.drop_duplicates(subset=['date','ticker','report_period'],keep='last') #删掉重复行\n",
    "    return part12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9780fe99-135a-4859-8d2e-daf717e2021b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Tadd当季+去年同季\n",
    "def compute_Tadd(df4,col):\n",
    "    tool_data = df4[['date','ticker','report_period',col]]\n",
    "    #part1\n",
    "    df1 = df4[['date', 'ticker', 'report_period',col]]\n",
    "    df1['report_period'] = df1['report_period'].astype('int') ###新增\n",
    "    df1['pre_yr_report_period'] = (df1['report_period'].astype('str').str[:4].astype('int')-1).astype('str') + df1['report_period'].astype('str').str[-4:]\n",
    "    df1['pre_yr_report_period'] = df1['pre_yr_report_period'].astype('int')\n",
    "    df1 = df1.merge(tool_data,left_on=['ticker','pre_yr_report_period'],right_on=['ticker','report_period'],how='left',suffixes=['','_pre_yr'])\n",
    "    df1 = df1.sort_values(['date','ticker','report_period','date_pre_yr'])\n",
    "    df1 = df1[df1['date']>=df1['date_pre_yr']]\n",
    "    df1 = df1.drop_duplicates(subset=['date','ticker','report_period'],keep='last')   ##只保留'date','ticker','report_period'对应的'date_pre_yr'最新的一条数据\n",
    "    df1[col+'_T'] = (df1[col]+df1[col+'_pre_yr'])\n",
    "    #part2\n",
    "    df2 = df4[['date', 'ticker', 'report_period',col]]\n",
    "    df2['report_period'] = df2['report_period'].astype('int') ###新增\n",
    "    df2['next_yr_report_period'] = (df2['report_period'].astype('str').str[:4].astype('int')+1).astype('str') + df2['report_period'].astype('str').str[-4:]\n",
    "    df2['next_yr_report_period'] = df2['next_yr_report_period'].astype('int')\n",
    "    df2 = df2.merge(tool_data,left_on=['ticker','next_yr_report_period'],right_on=['ticker','report_period'],how='left',suffixes=['','_next_yr'])\n",
    "    df2 = df2.sort_values(['date','ticker','report_period','date_next_yr'])\n",
    "    df2 = df2[df2['date']>=df2['date_next_yr']]\n",
    "    df2['report_period'] = df2['report_period_next_yr']\n",
    "    df2 = df2.drop_duplicates(subset=['date','ticker','report_period'],keep='last')   ##只保留'date','ticker','report_period'对应的'date_pre_yr'最新的一条数据\n",
    "    df2[col+'_T'] = (df2[col+'_next_yr']+df2[col])\n",
    "\n",
    "    #把两个部分合并就是所有发生更新的数据\n",
    "    part1 = df1[['date', 'ticker', 'report_period',col+'_T']]\n",
    "    part2 = df2[['date', 'ticker', 'report_period',col+'_T']]\n",
    "    part12 = pd.concat([part1,part2])\n",
    "    part12 = part12.sort_values(['date','ticker','report_period'])\n",
    "    part12 = part12.drop_duplicates(subset=['date','ticker','report_period'],keep='last') #删掉重复行\n",
    "    return part12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2b968aab-5b3d-4f36-8a99-f87881c4d741",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#环比QoQ\n",
    "def compute_qoq(df4,col):\n",
    "    tool_data = df4[['date','ticker','report_period',col]]\n",
    "    #part1\n",
    "    df1 = df4[['date','ticker','report_period','pre_report_period',col]]\n",
    "    df1 = df1.merge(tool_data,left_on=['ticker','pre_report_period'],right_on=['ticker','report_period'],how='left',suffixes=['','_pre'])\n",
    "    df1 = df1.sort_values(['date','ticker','report_period','date_pre'])\n",
    "    df1 = df1[df1['date']>=df1['date_pre']]\n",
    "    df1 = df1.drop_duplicates(subset=['date','ticker','report_period'],keep='last')   ##只保留'date','ticker','report_period'对应的'date_pre_yr'最新的一条数据\n",
    "    df1[col+'_QoQ'] = (df1[col]-df1[col+'_pre'])/np.abs(df1[col+'_pre'])\n",
    "    part1 = df1[['date','ticker','report_period',col+'_QoQ']]\n",
    "    #part2\n",
    "    df1 = df4[['date','ticker','report_period','next1_report_period',col]]\n",
    "    df1 = df1.merge(tool_data,left_on=['ticker','next1_report_period'],right_on=['ticker','report_period'],how='left',suffixes=['','_next'])\n",
    "    df1 = df1.sort_values(['date','ticker','report_period','date_next'])\n",
    "    df1 = df1[df1['date']>=df1['date_next']]\n",
    "    df1['report_period'] = df1['report_period_next']\n",
    "    df1 = df1.drop_duplicates(subset=['date','ticker','report_period'],keep='last')   ##只保留'date','ticker','report_period'对应的'date_pre_yr'最新的一条数据\n",
    "    df1[col+'_QoQ'] = (df1[col+'_next']-df1[col])/np.abs(df1[col])\n",
    "    part2 = df1[['date','ticker','report_period',col+'_QoQ']]\n",
    "\n",
    "    #把两个部分合并就是所有发生更新的数据\n",
    "    part12 = pd.concat([part1,part2])\n",
    "    part12 = part12.sort_values(['date','ticker','report_period'])\n",
    "    part12 = part12.drop_duplicates(subset=['date','ticker','report_period'],keep='last') #删掉重复行\n",
    "    return part12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7600df0f-6875-49cb-a5a0-d3441d800786",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#D 当季-上季\n",
    "def compute_D(df4,col):\n",
    "    tool_data = df4[['date','ticker','report_period',col]]\n",
    "    #part1\n",
    "    df1 = df4[['date','ticker','report_period','pre_report_period',col]]\n",
    "    df1 = df1.merge(tool_data,left_on=['ticker','pre_report_period'],right_on=['ticker','report_period'],how='left',suffixes=['','_pre'])\n",
    "    df1 = df1.sort_values(['date','ticker','report_period','date_pre'])\n",
    "    df1 = df1[df1['date']>=df1['date_pre']]\n",
    "    df1 = df1.drop_duplicates(subset=['date','ticker','report_period'],keep='last')   ##只保留'date','ticker','report_period'对应的'date_pre_yr'最新的一条数据\n",
    "    df1[col+'_D'] = df1[col]-df1[col+'_pre']\n",
    "    part1 = df1[['date','ticker','report_period',col+'_D']]\n",
    "    #part2\n",
    "    df1 = df4[['date','ticker','report_period','next1_report_period',col]]\n",
    "    df1 = df1.merge(tool_data,left_on=['ticker','next1_report_period'],right_on=['ticker','report_period'],how='left',suffixes=['','_next'])\n",
    "    df1 = df1.sort_values(['date','ticker','report_period','date_next'])\n",
    "    df1 = df1[df1['date']>=df1['date_next']]\n",
    "    df1['report_period'] = df1['report_period_next']\n",
    "    df1 = df1.drop_duplicates(subset=['date','ticker','report_period'],keep='last')   ##只保留'date','ticker','report_period'对应的'date_pre_yr'最新的一条数据\n",
    "    df1[col+'_D'] = df1[col+'_next']-df1[col]\n",
    "    part2 = df1[['date','ticker','report_period',col+'_D']]\n",
    "\n",
    "    #把两个部分合并就是所有发生更新的数据\n",
    "    part12 = pd.concat([part1,part2])\n",
    "    part12 = part12.sort_values(['date','ticker','report_period'])\n",
    "    part12 = part12.drop_duplicates(subset=['date','ticker','report_period'],keep='last') #删掉重复行\n",
    "    return part12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a9115eb3-c541-46ce-8567-cf23dbc1b09f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 期初 + 期末 的均值\n",
    "def compute_ChuMo(df4,col):\n",
    "    tool_data = df4[['date','ticker','report_period',col]]\n",
    "    #part1\n",
    "    df1 = df4[['date','ticker','report_period','pre_report_period',col]]\n",
    "    df1 = df1.merge(tool_data,left_on=['ticker','pre_report_period'],right_on=['ticker','report_period'],how='left',suffixes=['','_pre'])\n",
    "    df1 = df1.sort_values(['date','ticker','report_period','date_pre'])\n",
    "    df1 = df1[df1['date']>=df1['date_pre']]\n",
    "    df1 = df1.drop_duplicates(subset=['date','ticker','report_period'],keep='last')   ##只保留'date','ticker','report_period'对应的'date_pre_yr'最新的一条数据\n",
    "    df1[col+'_ChuMo'] = (df1[col+'_pre'] + df1[col])\n",
    "    part1 = df1[['date','ticker','report_period',col+'_ChuMo']]\n",
    "    #part2\n",
    "    df1 = df4[['date','ticker','report_period','next1_report_period',col]]\n",
    "    df1 = df1.merge(tool_data,left_on=['ticker','next1_report_period'],right_on=['ticker','report_period'],how='left',suffixes=['','_next'])\n",
    "    df1 = df1.sort_values(['date','ticker','report_period','date_next'])\n",
    "    df1 = df1[df1['date']>=df1['date_next']]\n",
    "    df1['report_period'] = df1['report_period_next']\n",
    "    df1 = df1.drop_duplicates(subset=['date','ticker','report_period'],keep='last')   ##只保留'date','ticker','report_period'对应的'date_pre_yr'最新的一条数据\n",
    "    df1[col+'_ChuMo'] = (df1[col] + df1[col+'_next'])\n",
    "    part2 = df1[['date','ticker','report_period',col+'_ChuMo']]\n",
    "\n",
    "    #把两个部分合并就是所有发生更新的数据\n",
    "    part12 = pd.concat([part1,part2])\n",
    "    part12 = part12.sort_values(['date','ticker','report_period'])\n",
    "    part12 = part12.drop_duplicates(subset=['date','ticker','report_period'],keep='last') #删掉重复行\n",
    "    return part12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "06b2f481-05cc-4672-9f76-8c6f7acb2808",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_ttm(df4,col):\n",
    "    #单季度数据计算的TTM，季度频率\n",
    "    tool_data = df4[['date','ticker','report_period',col+'_Q']]\n",
    "    #part1    \n",
    "    df3 = df4.merge(tool_data,left_on=['ticker','pre_report_period'],right_on=['ticker','report_period'],suffixes=['','_pre'],how='left')\n",
    "    df3 = df3.merge(tool_data,left_on=['ticker','pre2_report_period'],right_on=['ticker','report_period'],suffixes=['','_pre2'],how='left')\n",
    "    df3 = df3.merge(tool_data,left_on=['ticker','pre3_report_period'],right_on=['ticker','report_period'],suffixes=['','_pre3'],how='left')\n",
    "    #去掉 date_pre,date_pre2,date_pre3 大于date的行\n",
    "    df3 = df3[(df3['date']>=df3['date_pre'])&(df3['date']>=df3['date_pre2'])&(df3['date']>=df3['date_pre3'])]\n",
    "    ##只保留'date','ticker','report_period'对应的最新的一条数据\n",
    "    df3 = df3.sort_values(['ticker','date','date_pre','date_pre2','date_pre3']).drop_duplicates(subset=['date','ticker','report_period'],keep='last')\n",
    "    df3[col+'_TTM'] = df3[col+'_Q'] + df3[col+'_Q_pre'] + df3[col+'_Q_pre2']+ df3[col+'_Q_pre3']\n",
    "    part1 = df3[['date', 'ticker', 'report_period',col+'_TTM']]\n",
    "    #part1['label'] = 'part1'\n",
    "\n",
    "    #part2\n",
    "    df3 = df4.merge(tool_data,left_on=['ticker','pre_report_period'],right_on=['ticker','report_period'],suffixes=['','_pre'],how='left')\n",
    "    df3 = df3.merge(tool_data,left_on=['ticker','pre2_report_period'],right_on=['ticker','report_period'],suffixes=['','_pre2'],how='left')\n",
    "    df3 = df3.merge(tool_data,left_on=['ticker','next1_report_period'],right_on=['ticker','report_period'],suffixes=['','_next1'],how='left')\n",
    "    #去掉 date_pre,date_pre2,date_pre3 大于date的行\n",
    "    df3 = df3[(df3['date']>=df3['date_pre'])&(df3['date']>=df3['date_pre2'])&(df3['date']>=df3['date_next1'])]\n",
    "    df3['report_period'] = df3['report_period_next1']\n",
    "    ##只保留'date','ticker','report_period'对应的最新的一条数据\n",
    "    df3 = df3.sort_values(['ticker','date','date_pre','date_pre2','date_next1']).drop_duplicates(subset=['date','ticker','report_period'],keep='last')\n",
    "    df3[col+'_TTM'] = df3[col+'_Q'] + df3[col+'_Q_pre'] + df3[col+'_Q_pre2']+ df3[col+'_Q_next1']\n",
    "    part2 = df3[['date', 'ticker', 'report_period',col+'_TTM']]\n",
    "    #part2['label'] = 'part2'\n",
    "\n",
    "    #part3\n",
    "    df3 = df4.merge(tool_data,left_on=['ticker','pre_report_period'],right_on=['ticker','report_period'],suffixes=['','_pre'],how='left')\n",
    "    df3 = df3.merge(tool_data,left_on=['ticker','next1_report_period'],right_on=['ticker','report_period'],suffixes=['','_next1'],how='left')\n",
    "    df3 = df3.merge(tool_data,left_on=['ticker','next2_report_period'],right_on=['ticker','report_period'],suffixes=['','_next2'],how='left')\n",
    "    #去掉 date_pre,date_pre2,date_pre3 大于date的行\n",
    "    df3 = df3[(df3['date']>=df3['date_pre'])&(df3['date']>=df3['date_next1'])&(df3['date']>=df3['date_next2'])]\n",
    "    df3['report_period'] = df3['report_period_next2']\n",
    "    ##只保留'date','ticker','report_period'对应的最新的一条数据\n",
    "    df3 = df3.sort_values(['ticker','date','date_pre','date_next1','date_next2']).drop_duplicates(subset=['date','ticker','report_period'],keep='last')\n",
    "    df3[col+'_TTM'] = df3[col+'_Q'] + df3[col+'_Q_pre'] + df3[col+'_Q_next1']+ df3[col+'_Q_next2']\n",
    "    part3 = df3[['date', 'ticker', 'report_period',col+'_TTM']]\n",
    "    #part3['label'] = 'part3'\n",
    "\n",
    "    #part4\n",
    "    df3 = df4.merge(tool_data,left_on=['ticker','next1_report_period'],right_on=['ticker','report_period'],suffixes=['','_next1'],how='left')\n",
    "    df3 = df3.merge(tool_data,left_on=['ticker','next2_report_period'],right_on=['ticker','report_period'],suffixes=['','_next2'],how='left')\n",
    "    df3 = df3.merge(tool_data,left_on=['ticker','next3_report_period'],right_on=['ticker','report_period'],suffixes=['','_next3'],how='left')\n",
    "    #去掉 date_pre,date_pre2,date_pre3 大于date的行\n",
    "    df3 = df3[(df3['date']>=df3['date_next1'])&(df3['date']>=df3['date_next2'])&(df3['date']>=df3['date_next3'])]\n",
    "    df3['report_period'] = df3['report_period_next3']\n",
    "    ##只保留'date','ticker','report_period'对应的最新的一条数据\n",
    "    df3 = df3.sort_values(['ticker','date','date_next1','date_next1','date_next1']).drop_duplicates(subset=['date','ticker','report_period'],keep='last')\n",
    "    df3[col+'_TTM'] = df3[col+'_Q'] + df3[col+'_Q_next1'] + df3[col+'_Q_next2']+ df3[col+'_Q_next3']\n",
    "    part4 = df3[['date', 'ticker', 'report_period',col+'_TTM']]\n",
    "    #part4['label'] = 'part4'\n",
    "\n",
    "    part1234 = pd.concat([part1,part2,part3,part4])\n",
    "    part1234 = part1234.sort_values(['date','ticker','report_period'])\n",
    "    part1234 = part1234.drop_duplicates(subset=['date','ticker','report_period'],keep='last') #删掉重复行\n",
    "    return part1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a7a80c9b-f72e-4f5a-a2cc-257a0b515833",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_ttm(df4,col):\n",
    "    #单季度数据计算的TTM，季度频率\n",
    "    tool_data = df4[['date','ticker','report_period',col+'_Q']]\n",
    "    #part1    \n",
    "    df3 = df4.merge(tool_data,left_on=['ticker','pre_report_period'],right_on=['ticker','report_period'],suffixes=['','_pre'],how='left')\n",
    "    df3 = df3.merge(tool_data,left_on=['ticker','pre2_report_period'],right_on=['ticker','report_period'],suffixes=['','_pre2'],how='left')\n",
    "    df3 = df3.merge(tool_data,left_on=['ticker','pre3_report_period'],right_on=['ticker','report_period'],suffixes=['','_pre3'],how='left')\n",
    "    #去掉 date_pre,date_pre2,date_pre3 大于date的行\n",
    "    df3 = df3[(df3['date']>=df3['date_pre'])&(df3['date']>=df3['date_pre2'])&(df3['date']>=df3['date_pre3'])]\n",
    "    ##只保留'date','ticker','report_period'对应的最新的一条数据\n",
    "    df3 = df3.sort_values(['ticker','date','date_pre','date_pre2','date_pre3']).drop_duplicates(subset=['date','ticker','report_period'],keep='last')\n",
    "    df3[col+'_TTM'] = df3[col+'_Q'] + df3[col+'_Q_pre'] + df3[col+'_Q_pre2']+ df3[col+'_Q_pre3']\n",
    "    part1 = df3[['date', 'ticker', 'report_period',col+'_TTM']]\n",
    "    #part1['label'] = 'part1'\n",
    "\n",
    "    #part2\n",
    "    df3 = df4.merge(tool_data,left_on=['ticker','pre_report_period'],right_on=['ticker','report_period'],suffixes=['','_pre'],how='left')\n",
    "    df3 = df3.merge(tool_data,left_on=['ticker','pre2_report_period'],right_on=['ticker','report_period'],suffixes=['','_pre2'],how='left')\n",
    "    df3 = df3.merge(tool_data,left_on=['ticker','next1_report_period'],right_on=['ticker','report_period'],suffixes=['','_next1'],how='left')\n",
    "    #去掉 date_pre,date_pre2,date_pre3 大于date的行\n",
    "    df3 = df3[(df3['date']>=df3['date_pre'])&(df3['date']>=df3['date_pre2'])&(df3['date']>=df3['date_next1'])]\n",
    "    df3['report_period'] = df3['report_period_next1']\n",
    "    ##只保留'date','ticker','report_period'对应的最新的一条数据\n",
    "    df3 = df3.sort_values(['ticker','date','date_pre','date_pre2','date_next1']).drop_duplicates(subset=['date','ticker','report_period'],keep='last')\n",
    "    df3[col+'_TTM'] = df3[col+'_Q'] + df3[col+'_Q_pre'] + df3[col+'_Q_pre2']+ df3[col+'_Q_next1']\n",
    "    part2 = df3[['date', 'ticker', 'report_period',col+'_TTM']]\n",
    "    #part2['label'] = 'part2'\n",
    "\n",
    "    #part3\n",
    "    df3 = df4.merge(tool_data,left_on=['ticker','pre_report_period'],right_on=['ticker','report_period'],suffixes=['','_pre'],how='left')\n",
    "    df3 = df3.merge(tool_data,left_on=['ticker','next1_report_period'],right_on=['ticker','report_period'],suffixes=['','_next1'],how='left')\n",
    "    df3 = df3.merge(tool_data,left_on=['ticker','next2_report_period'],right_on=['ticker','report_period'],suffixes=['','_next2'],how='left')\n",
    "    #去掉 date_pre,date_pre2,date_pre3 大于date的行\n",
    "    df3 = df3[(df3['date']>=df3['date_pre'])&(df3['date']>=df3['date_next1'])&(df3['date']>=df3['date_next2'])]\n",
    "    df3['report_period'] = df3['report_period_next2']\n",
    "    ##只保留'date','ticker','report_period'对应的最新的一条数据\n",
    "    df3 = df3.sort_values(['ticker','date','date_pre','date_next1','date_next2']).drop_duplicates(subset=['date','ticker','report_period'],keep='last')\n",
    "    df3[col+'_TTM'] = df3[col+'_Q'] + df3[col+'_Q_pre'] + df3[col+'_Q_next1']+ df3[col+'_Q_next2']\n",
    "    part3 = df3[['date', 'ticker', 'report_period',col+'_TTM']]\n",
    "    #part3['label'] = 'part3'\n",
    "\n",
    "    #part4\n",
    "    df3 = df4.merge(tool_data,left_on=['ticker','next1_report_period'],right_on=['ticker','report_period'],suffixes=['','_next1'],how='left')\n",
    "    df3 = df3.merge(tool_data,left_on=['ticker','next2_report_period'],right_on=['ticker','report_period'],suffixes=['','_next2'],how='left')\n",
    "    df3 = df3.merge(tool_data,left_on=['ticker','next3_report_period'],right_on=['ticker','report_period'],suffixes=['','_next3'],how='left')\n",
    "    #去掉 date_pre,date_pre2,date_pre3 大于date的行\n",
    "    df3 = df3[(df3['date']>=df3['date_next1'])&(df3['date']>=df3['date_next2'])&(df3['date']>=df3['date_next3'])]\n",
    "    df3['report_period'] = df3['report_period_next3']\n",
    "    ##只保留'date','ticker','report_period'对应的最新的一条数据\n",
    "    df3 = df3.sort_values(['ticker','date','date_next1','date_next1','date_next1']).drop_duplicates(subset=['date','ticker','report_period'],keep='last')\n",
    "    df3[col+'_TTM'] = df3[col+'_Q'] + df3[col+'_Q_next1'] + df3[col+'_Q_next2']+ df3[col+'_Q_next3']\n",
    "    part4 = df3[['date', 'ticker', 'report_period',col+'_TTM']]\n",
    "    #part4['label'] = 'part4'\n",
    "\n",
    "    part1234 = pd.concat([part1,part2,part3,part4])\n",
    "    part1234 = part1234.sort_values(['date','ticker','report_period'])\n",
    "    part1234 = part1234.drop_duplicates(subset=['date','ticker','report_period'],keep='last') #删掉重复行\n",
    "    return part1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2262fcb8-5ef3-416e-b4c0-54ad69b20f50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_finfactor_and_save_data(df, col3, Open):\n",
    "    # 去重\n",
    "    df_processed = df.drop_duplicates(subset=['date', 'ticker'], keep='last')\n",
    "    \n",
    "    # 设置索引并unstack\n",
    "    df_processed = df_processed.set_index(['date', 'ticker'])[col3].unstack()\n",
    "    \n",
    "    # 合并标签数据并删除标签列\n",
    "    df_processed = pd.concat([df_processed, Open[['label']]], axis=1).drop(['label'], axis=1)\n",
    "    \n",
    "    # 填充缺失值并删除全为NaN的行和列\n",
    "    df_processed = df_processed.fillna(method='ffill', limit=150).dropna(how='all', axis=0).dropna(how='all', axis=1)\n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8a8df337-d799-49e6-9d5a-eccc0b7a654c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_extreme_by_MAD(series, n=3):\n",
    "    median = series.median()\n",
    "    median_new = abs(series - median).median()\n",
    "    max_value = median + n * median_new\n",
    "    min_value = median - n * median_new\n",
    "    return np.clip(series, min_value, max_value, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5270c6bb-6a80-4b3a-9855-140fbe08cc8f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    " def func(x):\n",
    "        x = x[x['report_period']>=x['report_period'].expanding(min_periods=1).max()]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee19ac98-e6c4-47f3-94c6-26f0decddd3a",
   "metadata": {},
   "source": [
    "# 分钟量价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5021e962-ecbd-4449-8790-750bda8388d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入数据 \n",
    "def k1min_data_process(date_str):\n",
    "    k1min= oss_client.read_oss_parquet_file(f'ad_hoc_prod/1min_cs/{date_str}.parquet')\n",
    "    k1min = k1min.sort_values(by=[\"RIC\", \"TIME\"])\n",
    "    K1min = k1min.reset_index(drop=True)\n",
    "    k1min['RETURN'] = k1min['CLOSE']/k1min['OPEN'] - 1\n",
    "    k1min = k1min.rename(columns={'RIC': 'ticker', 'DATE': 'date'})\n",
    "    return k1min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f478fb86-baf2-4fd5-953a-9cf2e4cc635b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File for date 2017-01-01 does not exist. Skipping...\n",
      "File for date 2017-01-02 does not exist. Skipping...\n",
      "File for date 2017-01-07 does not exist. Skipping...\n",
      "File for date 2017-01-08 does not exist. Skipping...\n",
      "File for date 2017-01-14 does not exist. Skipping...\n",
      "File for date 2017-01-15 does not exist. Skipping...\n",
      "File for date 2017-01-21 does not exist. Skipping...\n",
      "File for date 2017-01-22 does not exist. Skipping...\n",
      "File for date 2017-01-27 does not exist. Skipping...\n",
      "File for date 2017-01-28 does not exist. Skipping...\n",
      "File for date 2017-01-29 does not exist. Skipping...\n",
      "File for date 2017-01-30 does not exist. Skipping...\n",
      "File for date 2017-01-31 does not exist. Skipping...\n",
      "File for date 2017-02-01 does not exist. Skipping...\n",
      "File for date 2017-02-02 does not exist. Skipping...\n",
      "File for date 2017-02-04 does not exist. Skipping...\n",
      "File for date 2017-02-05 does not exist. Skipping...\n",
      "File for date 2017-02-11 does not exist. Skipping...\n",
      "File for date 2017-02-12 does not exist. Skipping...\n",
      "File for date 2017-02-18 does not exist. Skipping...\n",
      "File for date 2017-02-19 does not exist. Skipping...\n",
      "File for date 2017-02-25 does not exist. Skipping...\n",
      "File for date 2017-02-26 does not exist. Skipping...\n",
      "File for date 2017-03-04 does not exist. Skipping...\n",
      "File for date 2017-03-05 does not exist. Skipping...\n",
      "File for date 2017-03-11 does not exist. Skipping...\n",
      "File for date 2017-03-12 does not exist. Skipping...\n",
      "File for date 2017-03-18 does not exist. Skipping...\n",
      "File for date 2017-03-19 does not exist. Skipping...\n",
      "File for date 2017-03-25 does not exist. Skipping...\n",
      "File for date 2017-03-26 does not exist. Skipping...\n",
      "File for date 2017-04-01 does not exist. Skipping...\n",
      "File for date 2017-04-02 does not exist. Skipping...\n",
      "File for date 2017-04-03 does not exist. Skipping...\n",
      "File for date 2017-04-04 does not exist. Skipping...\n",
      "File for date 2017-04-08 does not exist. Skipping...\n",
      "File for date 2017-04-09 does not exist. Skipping...\n",
      "File for date 2017-04-15 does not exist. Skipping...\n",
      "File for date 2017-04-16 does not exist. Skipping...\n",
      "File for date 2017-04-22 does not exist. Skipping...\n",
      "File for date 2017-04-23 does not exist. Skipping...\n",
      "File for date 2017-04-29 does not exist. Skipping...\n",
      "File for date 2017-04-30 does not exist. Skipping...\n",
      "File for date 2017-05-01 does not exist. Skipping...\n",
      "File for date 2017-05-06 does not exist. Skipping...\n",
      "File for date 2017-05-07 does not exist. Skipping...\n",
      "File for date 2017-05-13 does not exist. Skipping...\n",
      "File for date 2017-05-14 does not exist. Skipping...\n",
      "File for date 2017-05-20 does not exist. Skipping...\n",
      "File for date 2017-05-21 does not exist. Skipping...\n",
      "File for date 2017-05-27 does not exist. Skipping...\n",
      "File for date 2017-05-28 does not exist. Skipping...\n",
      "File for date 2017-05-29 does not exist. Skipping...\n",
      "File for date 2017-05-30 does not exist. Skipping...\n",
      "File for date 2017-06-03 does not exist. Skipping...\n",
      "File for date 2017-06-04 does not exist. Skipping...\n",
      "File for date 2017-06-10 does not exist. Skipping...\n",
      "File for date 2017-06-11 does not exist. Skipping...\n",
      "File for date 2017-06-17 does not exist. Skipping...\n",
      "File for date 2017-06-18 does not exist. Skipping...\n",
      "File for date 2017-06-24 does not exist. Skipping...\n",
      "File for date 2017-06-25 does not exist. Skipping...\n",
      "File for date 2017-07-01 does not exist. Skipping...\n",
      "File for date 2017-07-02 does not exist. Skipping...\n",
      "File for date 2017-07-08 does not exist. Skipping...\n",
      "File for date 2017-07-09 does not exist. Skipping...\n",
      "File for date 2017-07-15 does not exist. Skipping...\n",
      "File for date 2017-07-16 does not exist. Skipping...\n",
      "File for date 2017-07-22 does not exist. Skipping...\n",
      "File for date 2017-07-23 does not exist. Skipping...\n",
      "File for date 2017-07-29 does not exist. Skipping...\n",
      "File for date 2017-07-30 does not exist. Skipping...\n",
      "File for date 2017-08-05 does not exist. Skipping...\n",
      "File for date 2017-08-06 does not exist. Skipping...\n",
      "File for date 2017-08-12 does not exist. Skipping...\n",
      "File for date 2017-08-13 does not exist. Skipping...\n",
      "File for date 2017-08-19 does not exist. Skipping...\n",
      "File for date 2017-08-20 does not exist. Skipping...\n",
      "File for date 2017-08-26 does not exist. Skipping...\n",
      "File for date 2017-08-27 does not exist. Skipping...\n",
      "File for date 2017-09-02 does not exist. Skipping...\n",
      "File for date 2017-09-03 does not exist. Skipping...\n",
      "File for date 2017-09-09 does not exist. Skipping...\n",
      "File for date 2017-09-10 does not exist. Skipping...\n",
      "File for date 2017-09-16 does not exist. Skipping...\n",
      "File for date 2017-09-17 does not exist. Skipping...\n",
      "File for date 2017-09-23 does not exist. Skipping...\n",
      "File for date 2017-09-24 does not exist. Skipping...\n",
      "File for date 2017-09-30 does not exist. Skipping...\n",
      "File for date 2017-10-01 does not exist. Skipping...\n",
      "File for date 2017-10-02 does not exist. Skipping...\n",
      "File for date 2017-10-03 does not exist. Skipping...\n",
      "File for date 2017-10-04 does not exist. Skipping...\n",
      "File for date 2017-10-05 does not exist. Skipping...\n",
      "File for date 2017-10-06 does not exist. Skipping...\n",
      "File for date 2017-10-07 does not exist. Skipping...\n",
      "File for date 2017-10-08 does not exist. Skipping...\n",
      "File for date 2017-10-14 does not exist. Skipping...\n",
      "File for date 2017-10-15 does not exist. Skipping...\n",
      "File for date 2017-10-21 does not exist. Skipping...\n",
      "File for date 2017-10-22 does not exist. Skipping...\n",
      "File for date 2017-10-28 does not exist. Skipping...\n",
      "File for date 2017-10-29 does not exist. Skipping...\n",
      "File for date 2017-11-04 does not exist. Skipping...\n",
      "File for date 2017-11-05 does not exist. Skipping...\n",
      "File for date 2017-11-11 does not exist. Skipping...\n",
      "File for date 2017-11-12 does not exist. Skipping...\n",
      "File for date 2017-11-18 does not exist. Skipping...\n",
      "File for date 2017-11-19 does not exist. Skipping...\n",
      "File for date 2017-11-25 does not exist. Skipping...\n",
      "File for date 2017-11-26 does not exist. Skipping...\n",
      "File for date 2017-12-02 does not exist. Skipping...\n",
      "File for date 2017-12-03 does not exist. Skipping...\n",
      "File for date 2017-12-09 does not exist. Skipping...\n",
      "File for date 2017-12-10 does not exist. Skipping...\n",
      "File for date 2017-12-16 does not exist. Skipping...\n",
      "File for date 2017-12-17 does not exist. Skipping...\n",
      "File for date 2017-12-23 does not exist. Skipping...\n",
      "File for date 2017-12-24 does not exist. Skipping...\n",
      "File for date 2017-12-30 does not exist. Skipping...\n",
      "File for date 2017-12-31 does not exist. Skipping...\n",
      "File for date 2018-01-01 does not exist. Skipping...\n",
      "File for date 2018-01-06 does not exist. Skipping...\n",
      "File for date 2018-01-07 does not exist. Skipping...\n",
      "File for date 2018-01-13 does not exist. Skipping...\n",
      "File for date 2018-01-14 does not exist. Skipping...\n",
      "File for date 2018-01-20 does not exist. Skipping...\n",
      "File for date 2018-01-21 does not exist. Skipping...\n",
      "File for date 2018-01-27 does not exist. Skipping...\n",
      "File for date 2018-01-28 does not exist. Skipping...\n",
      "File for date 2018-02-03 does not exist. Skipping...\n",
      "File for date 2018-02-04 does not exist. Skipping...\n",
      "File for date 2018-02-10 does not exist. Skipping...\n",
      "File for date 2018-02-11 does not exist. Skipping...\n",
      "File for date 2018-02-15 does not exist. Skipping...\n",
      "File for date 2018-02-16 does not exist. Skipping...\n",
      "File for date 2018-02-17 does not exist. Skipping...\n",
      "File for date 2018-02-18 does not exist. Skipping...\n",
      "File for date 2018-02-19 does not exist. Skipping...\n",
      "File for date 2018-02-20 does not exist. Skipping...\n",
      "File for date 2018-02-21 does not exist. Skipping...\n",
      "File for date 2018-02-24 does not exist. Skipping...\n",
      "File for date 2018-02-25 does not exist. Skipping...\n",
      "File for date 2018-03-03 does not exist. Skipping...\n",
      "File for date 2018-03-04 does not exist. Skipping...\n",
      "File for date 2018-03-10 does not exist. Skipping...\n",
      "File for date 2018-03-11 does not exist. Skipping...\n",
      "File for date 2018-03-17 does not exist. Skipping...\n",
      "File for date 2018-03-18 does not exist. Skipping...\n",
      "File for date 2018-03-24 does not exist. Skipping...\n",
      "File for date 2018-03-25 does not exist. Skipping...\n",
      "File for date 2018-03-31 does not exist. Skipping...\n",
      "File for date 2018-04-01 does not exist. Skipping...\n",
      "File for date 2018-04-05 does not exist. Skipping...\n",
      "File for date 2018-04-06 does not exist. Skipping...\n",
      "File for date 2018-04-07 does not exist. Skipping...\n",
      "File for date 2018-04-08 does not exist. Skipping...\n",
      "File for date 2018-04-14 does not exist. Skipping...\n",
      "File for date 2018-04-15 does not exist. Skipping...\n",
      "File for date 2018-04-21 does not exist. Skipping...\n",
      "File for date 2018-04-22 does not exist. Skipping...\n",
      "File for date 2018-04-28 does not exist. Skipping...\n",
      "File for date 2018-04-29 does not exist. Skipping...\n",
      "File for date 2018-04-30 does not exist. Skipping...\n",
      "File for date 2018-05-01 does not exist. Skipping...\n",
      "File for date 2018-05-05 does not exist. Skipping...\n",
      "File for date 2018-05-06 does not exist. Skipping...\n",
      "File for date 2018-05-12 does not exist. Skipping...\n",
      "File for date 2018-05-13 does not exist. Skipping...\n",
      "File for date 2018-05-19 does not exist. Skipping...\n",
      "File for date 2018-05-20 does not exist. Skipping...\n",
      "File for date 2018-05-26 does not exist. Skipping...\n",
      "File for date 2018-05-27 does not exist. Skipping...\n",
      "File for date 2018-06-02 does not exist. Skipping...\n",
      "File for date 2018-06-03 does not exist. Skipping...\n",
      "File for date 2018-06-09 does not exist. Skipping...\n",
      "File for date 2018-06-10 does not exist. Skipping...\n",
      "File for date 2018-06-16 does not exist. Skipping...\n",
      "File for date 2018-06-17 does not exist. Skipping...\n",
      "File for date 2018-06-18 does not exist. Skipping...\n",
      "File for date 2018-06-23 does not exist. Skipping...\n",
      "File for date 2018-06-24 does not exist. Skipping...\n",
      "File for date 2018-06-30 does not exist. Skipping...\n",
      "File for date 2018-07-01 does not exist. Skipping...\n",
      "File for date 2018-07-07 does not exist. Skipping...\n",
      "File for date 2018-07-08 does not exist. Skipping...\n",
      "File for date 2018-07-14 does not exist. Skipping...\n",
      "File for date 2018-07-15 does not exist. Skipping...\n",
      "File for date 2018-07-21 does not exist. Skipping...\n",
      "File for date 2018-07-22 does not exist. Skipping...\n",
      "File for date 2018-07-28 does not exist. Skipping...\n",
      "File for date 2018-07-29 does not exist. Skipping...\n",
      "File for date 2018-08-04 does not exist. Skipping...\n",
      "File for date 2018-08-05 does not exist. Skipping...\n",
      "File for date 2018-08-11 does not exist. Skipping...\n",
      "File for date 2018-08-12 does not exist. Skipping...\n",
      "File for date 2018-08-18 does not exist. Skipping...\n",
      "File for date 2018-08-19 does not exist. Skipping...\n",
      "File for date 2018-08-25 does not exist. Skipping...\n",
      "File for date 2018-08-26 does not exist. Skipping...\n",
      "File for date 2018-09-01 does not exist. Skipping...\n",
      "File for date 2018-09-02 does not exist. Skipping...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mad_hoc_prod/1min_cs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 29\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43moss_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_oss_parquet_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     30\u001b[0m     a \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRIC\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTIME\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     31\u001b[0m     a \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m<string>:142\u001b[0m, in \u001b[0;36mread_oss_parquet_file\u001b[1;34m(self, object_name)\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\oss2\\models.py:313\u001b[0m, in \u001b[0;36mGetObjectResult.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, amt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\oss2\\utils.py:436\u001b[0m, in \u001b[0;36m_FileLikeAdapter.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    434\u001b[0m         real_discard \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscard\n\u001b[1;32m--> 436\u001b[0m \u001b[43m_invoke_crc_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrc_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_discard\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m content \u001b[38;5;241m=\u001b[39m _invoke_cipher_callback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcipher_callback, content, real_discard)\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscard \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m real_discard\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\oss2\\utils.py:329\u001b[0m, in \u001b[0;36m_invoke_crc_callback\u001b[1;34m(crc_callback, content, discard)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_invoke_crc_callback\u001b[39m(crc_callback, content, discard\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m crc_callback:\n\u001b[1;32m--> 329\u001b[0m         \u001b[43mcrc_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdiscard\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\oss2\\utils.py:538\u001b[0m, in \u001b[0;36mCrc64.__call__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m--> 538\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\oss2\\utils.py:541\u001b[0m, in \u001b[0;36mCrc64.update\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m--> 541\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrc64\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\crcmod\\crcmod.py:152\u001b[0m, in \u001b[0;36mCrc.update\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m    149\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Update the current CRC value using the string specified as the data\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m    parameter.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrcValue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_crc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrcValue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\crcmod\\crcmod.py:435\u001b[0m, in \u001b[0;36m_mkCrcFun.<locals>.crcfun\u001b[1;34m(data, crc, table, fun)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcrcfun\u001b[39m(data, crc\u001b[38;5;241m=\u001b[39minitCrc, table\u001b[38;5;241m=\u001b[39m_table, fun\u001b[38;5;241m=\u001b[39m_fun):\n\u001b[1;32m--> 435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xorOut \u001b[38;5;241m^\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxorOut\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m^\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\crcmod\\_crcfunpy.py:105\u001b[0m, in \u001b[0;36m_crc64r\u001b[1;34m(data, crc, table)\u001b[0m\n\u001b[0;32m    103\u001b[0m crc \u001b[38;5;241m=\u001b[39m crc \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFFFFFFFFFFFFFFFF\u001b[39m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m mv\u001b[38;5;241m.\u001b[39mtobytes():\n\u001b[1;32m--> 105\u001b[0m     crc \u001b[38;5;241m=\u001b[39m table[x \u001b[38;5;241m^\u001b[39m (crc \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m)] \u001b[38;5;241m^\u001b[39m (crc \u001b[38;5;241m>>\u001b[39m \u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m crc\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 最大20vulume的mmt\n",
    "def mmt_top20_volume(single_day_data):\n",
    "\n",
    "    ### 公式函数\n",
    "    df = (\n",
    "        single_day_data.assign(vol_rank=lambda x: x.groupby(['ticker', 'date'])['VOL']\n",
    "        .transform(lambda s: s.rank(method='first', ascending=False))).query('vol_rank <= 20')\n",
    "        .groupby(['ticker', 'date'])['RETURN']\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .drop(columns=['vol_rank'], errors='ignore')\n",
    "    )\n",
    "    ### 公式函数\n",
    "    df = pd.DataFrame(df)\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns={df.columns[-1]: 'factor'})\n",
    "    df = df.pivot(index='date', columns='ticker', values='factor')\n",
    "    return df\n",
    "    \n",
    "##多天   \n",
    "start_date = '2017-01-03'\n",
    "end_date = '2025-01-04'\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "for date in date_range:\n",
    "    date_str = date.date().strftime('%Y-%m-%d')\n",
    " \n",
    "    try:\n",
    "        single_day_data = k1min_data_process(date_str)\n",
    "\n",
    "        ### 公式函数\n",
    "        cal_factor = mmt_top20_volume(single_day_data)\n",
    "        ### 公式函数\n",
    "\n",
    "        final_df = pd.concat([final_df, cal_factor])\n",
    "        \n",
    "    except NoSuchKey:\n",
    "        # 捕获 NoSuchKey 异常并跳过该日期\n",
    "        print(f\"File for date {date_str} does not exist. Skipping...\")\n",
    "        continue\n",
    "        \n",
    "final_df = -1*final_df \n",
    "final_df.to_pickle('D:/redata/factor/mmt_last30min@factor.pkl') #如果错过了读这个文件先\n",
    "\n",
    "## 单天\n",
    "date = \"2025-02-20\"\n",
    "mmt_top20_vol_today = mmt_top20_volume(k1min)\n",
    "mmt_top20_vol_full_data = pd.read_pickle('D:/redata/factor/mmt_top20_vol@factor.pkl')\n",
    "mmt_top20_vol_full_data =  pd.concat([mmt_top20_vol_full_data, mmt_top20_vol_today])\n",
    "mmt_top20_vol_full_data.to_pickle(f'D:/redata/factor/mmt_top20_vol{date}@factor.pkl')\n",
    "\n",
    "mmt_top20volumeRet_std_W = -1 * mmt_top20_vol_full_data.rolling(5).std()\n",
    "mmt_top20volumeRet_std_W.to_pickle('D:/redata/factor/mmt_top20volumeRet_std_W@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcf8f46-9c79-428e-9860-adafd19d7492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#尾盘半小时动量\n",
    "def mmt_last30min(single_day_data):\n",
    "    \n",
    "    ### 公式函数\n",
    "    df = single_day_data.groupby('ticker').tail(31)\n",
    "    df = df.groupby(['ticker', 'date'])['CLOSE'].apply(lambda x: x.iloc[-1] / x.iloc[0] - 1)\n",
    "    ### 公式函数\n",
    "    \n",
    "    df = pd.DataFrame(df)\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    df = df.pivot(index='date', columns='ticker', values='CLOSE')\n",
    "    return df\n",
    "\n",
    "##多天   \n",
    "start_date = '2017-01-03'\n",
    "end_date = '2025-01-04'\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "for date in date_range:\n",
    "    date_str = date.date().strftime('%Y-%m-%d')\n",
    " \n",
    "    try:\n",
    "        single_day_data = k1min_data_process(date_str)\n",
    "\n",
    "        ### 公式函数\n",
    "        cal_factor = mmt_last30min(single_day_data)\n",
    "        ### 公式函数\n",
    "\n",
    "        final_df = pd.concat([final_df, cal_factor])\n",
    "        \n",
    "    except NoSuchKey:\n",
    "        # 捕获 NoSuchKey 异常并跳过该日期\n",
    "        print(f\"File for date {date_str} does not exist. Skipping...\")\n",
    "        continue\n",
    "        \n",
    "final_df = -1*final_df \n",
    "final_df.to_pickle('D:/redata/factor/mmt_last30min@factor.pkl') #如果错过了读这个文件先\n",
    "\n",
    "## 单天\n",
    "date = \"2025-02-20\"\n",
    "mmt_last30min_today = mmt_last30min(k1min_data_process(date))\n",
    "mmt_last30min_full_data = pd.read_pickle('D:/redata/factor/mmt_last30min@factor.pkl')\n",
    "mmt_last30min_full_data =  pd.concat([mmt_last30min_full_data, mmt_last30min_today])\n",
    "mmt_last30min_full_data.to_pickle(f'D:/redata/factor/mmt_last30min_{date}@factor.pkl')\n",
    "\n",
    "mmt_last30min_std = -1 * mmt_last30min_full_data.rolling(20).std()\n",
    "mmt_last30min_std.to_pickle('D:/redata/factor/mmt_last30min_std@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d53cad90-6eb8-4a2e-9e92-69971246e8d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 计算每50根K线中50个beta值的均值\n",
    "# window = 50\n",
    "# average_betas = []\n",
    "\n",
    "# for i in range(0, len(k1min) - window + 1, window):  # 步长为50，滑动窗口\n",
    "#     betas = []\n",
    "#     for j in range(i, i + window):  # 在每50根K线内计算beta\n",
    "#         y = k1min[['HIGH']].iloc[j:j + window].values.reshape(-1, 1)\n",
    "#         x = k1min[['LOW']].iloc[j:j + window].values.reshape(-1, 1)\n",
    "        \n",
    "#         # 检查是否足够的数据点\n",
    "#         if len(y) == window and len(x) == window:\n",
    "#             model = LinearRegression().fit(x, y)\n",
    "#             beta = model.coef_[0][0]\n",
    "#             betas.append(beta)\n",
    "    \n",
    "#     # 计算当前50根K线的beta均值\n",
    "#     if betas:\n",
    "#         average_beta = np.mean(betas)\n",
    "#         average_betas.append(average_beta)\n",
    "\n",
    "# # 输出每50根K线的50个beta均值\n",
    "# print(average_betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "190a555e-f5c3-4699-a4d3-fccc2e3f7ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rolling_regression(high, low):\n",
    "#     X = high.values.reshape(-1, 1)\n",
    "#     y = low.values.reshape(-1, 1)\n",
    "#     model = LinearRegression().fit(X, y)\n",
    "#     return model.coef_[0][0]\n",
    "\n",
    "# # Apply the rolling regression function to the grouped DataFrame\n",
    "# result = k1min.groupby(['ticker', 'date'])[['HIGH', 'LOW']].rolling(50).apply(\n",
    "#     lambda x: rolling_regression(x['HIGH'], x['LOW'])\n",
    "# )\n",
    "\n",
    "# # Reset the index to make it more readable\n",
    "# result = result.reset_index(level=[0, 1], drop=True)\n",
    "\n",
    "# # Display the result\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "db4b63e1-79b7-4379-b3f0-1f10f6cc3729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QRS支撑阻力指标\n",
    "# def mmt_ols_beta_mean(data_1min):\n",
    "    \n",
    "#     ### 公式函数\n",
    "#     y = data_1min['HIGH'].values.reshape(-1,1)\n",
    "#     x = data_1min['LOW'].values.reshape(-1,1)\n",
    "#     model = LinearRegression().fit(x, y)\n",
    "#     beta = model.coef_[0][0]\n",
    "    \n",
    "#     factor = factor.groupby(['ticker', 'date'])['CLOSE'].apply(lambda x: x.iloc[-1] / x.iloc[0] - 1)\n",
    "#     ### 公式函数\n",
    "    \n",
    "#     factor = pd.DataFrame(factor)\n",
    "#     factor.columns = [date_data]\n",
    "#     factor = factor.reset_index()\n",
    "    \n",
    "#     pivot_df = factor.pivot(index='date', columns='ticker', values=date_data)\n",
    "#     pivot_df = pivot_df * -1\n",
    "#     return pivot_df\n",
    "\n",
    "# factor_today = mmt_ols_beta_mean(k1min)\n",
    "# factor_full_data = pd.read_pickle('D:/redata/factor/mmt_ols_beta_mean@factor.pkl')\n",
    "# factor_full_data =  pd.concat([factor_full_data, factor_today])\n",
    "# factor_full_data.to_pickle('D:/redata/factor/mmt_ols_beta_mean@factor.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437b4a02-352a-4927-9bbf-f1ebb2713932",
   "metadata": {},
   "source": [
    "# 小市值因子\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "95704097-087d-40b3-82ef-9f1dd80577b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#流动市值对数\n",
    "maketcap = -1 * np.log(adj_close * total_a)\n",
    "maketcap.to_pickle('D:/redata/factor/maketcap@factor.pkl')\n",
    "#收盘价\n",
    "adjclose = adj_close * -1\n",
    "adjclose.to_pickle('D:/redata/factor/adjclose@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0e6be176-b893-4cc2-9cdb-dd305f4f0463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#动量\n",
    "mmt_overnight_M = ((adj_open - adj_close.shift(1))/adj_close.shift(1)).rolling(20).sum()\n",
    "mmt_overnight_M.to_pickle('D:/redata/factor/mmt_overnight_M@factor.pkl')\n",
    "\n",
    "mmt_overnight_A = ((adj_open - adj_close.shift(1))/adj_close.shift(1)).rolling(240).sum() - ((adj_open - adj_close.shift(1))/adj_close.shift(1)).rolling(20).sum()\n",
    "mmt_overnight_A.to_pickle('D:/redata/factor/mmt_overnight_A@factor.pkl')\n",
    "\n",
    "mmt_normal_M = -(adj_open/adj_open.shift(20) - 1)\n",
    "mmt_normal_M.to_pickle('D:/redata/factor/mmt_normal_M@factor.pkl')\n",
    "\n",
    "mmt_normal_A = -(adj_open/adj_open.shift(240) - 1)\n",
    "mmt_normal_A.to_pickle('D:/redata/factor/mmt_normal_A@factor.pkl')\n",
    "\n",
    "# mmt_report_overnight = \n",
    "# mmt_report_jump_open\n",
    "# mmt_report_period\n",
    "\n",
    "mmt_intraday_M = ((adj_open - adj_close)/adj_close).rolling(20).sum()\n",
    "mmt_intraday_M.to_pickle('D:/redata/factor/mmt_intraday_M@factor.pkl')\n",
    "\n",
    "mmt_avg_M = -(adj_close/adj_close.rolling(20).mean())\n",
    "mmt_avg_M.to_pickle('D:/redata/factor/mmt_avg_M@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "740a618b-23d9-44aa-a2e4-b65ee6b2c354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ticker</th>\n",
       "      <th>000001</th>\n",
       "      <th>000002</th>\n",
       "      <th>000004</th>\n",
       "      <th>000005</th>\n",
       "      <th>000006</th>\n",
       "      <th>000007</th>\n",
       "      <th>000008</th>\n",
       "      <th>000009</th>\n",
       "      <th>000010</th>\n",
       "      <th>000011</th>\n",
       "      <th>...</th>\n",
       "      <th>688787</th>\n",
       "      <th>688788</th>\n",
       "      <th>688789</th>\n",
       "      <th>688793</th>\n",
       "      <th>688798</th>\n",
       "      <th>688799</th>\n",
       "      <th>688800</th>\n",
       "      <th>688819</th>\n",
       "      <th>688981</th>\n",
       "      <th>689009</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-12-27</th>\n",
       "      <td>-0.043227</td>\n",
       "      <td>-0.550392</td>\n",
       "      <td>-1.219713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.284113</td>\n",
       "      <td>-0.022479</td>\n",
       "      <td>-0.159682</td>\n",
       "      <td>0.091171</td>\n",
       "      <td>-0.399197</td>\n",
       "      <td>-0.427065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222258</td>\n",
       "      <td>0.404464</td>\n",
       "      <td>0.123911</td>\n",
       "      <td>-0.078095</td>\n",
       "      <td>0.269668</td>\n",
       "      <td>0.114746</td>\n",
       "      <td>0.326303</td>\n",
       "      <td>0.346499</td>\n",
       "      <td>0.074029</td>\n",
       "      <td>0.316789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 5328 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "ticker        000001    000002    000004  000005    000006    000007  \\\n",
       "date                                                                   \n",
       "2024-12-27 -0.043227 -0.550392 -1.219713     NaN -0.284113 -0.022479   \n",
       "\n",
       "ticker        000008    000009    000010    000011  ...    688787    688788  \\\n",
       "date                                                ...                       \n",
       "2024-12-27 -0.159682  0.091171 -0.399197 -0.427065  ...  0.222258  0.404464   \n",
       "\n",
       "ticker        688789    688793    688798    688799    688800    688819  \\\n",
       "date                                                                     \n",
       "2024-12-27  0.123911 -0.078095  0.269668  0.114746  0.326303  0.346499   \n",
       "\n",
       "ticker        688981    689009  \n",
       "date                            \n",
       "2024-12-27  0.074029  0.316789  \n",
       "\n",
       "[1 rows x 5328 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmt_overnight_A.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "17095f48-2987-45ce-a702-3795f3c5d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_1 = (close == limit_down).astype(int)\n",
    "limit_2 = (close == limit_up).astype(int)\n",
    "limit_3 = limit_1+limit_2\n",
    "limit = limit_3.replace({0: 1, 1: 0})\n",
    "adj_close_noZDT = adj_close*limit\n",
    "\n",
    "mmt_off_limit_M = -(adj_close_noZDT - adj_close_noZDT.shift(20))/adj_close_noZDT.shift(20)\n",
    "mmt_off_limit_M = mmt_off_limit_M.replace(1,np.nan)\n",
    "mmt_off_limit_M.to_pickle('D:/redata/factor/mmt_off_limit_M@factor.pkl')\n",
    "\n",
    "mmt_off_limit_A = (adj_close_noZDT - adj_close_noZDT.shift(240))/adj_close_noZDT.shift(240)\n",
    "mmt_off_limit_A = mmt_off_limit_A.replace(np.inf,np.nan).replace(-np.inf,np.nan)\n",
    "mmt_off_limit_A = mmt_off_limit_A.replace(-1,np.nan)\n",
    "mmt_off_limit_A = (mmt_off_limit_A - mmt_off_limit_M)\n",
    "mmt_off_limit_A = -1 * mmt_off_limit_A\n",
    "mmt_off_limit_A.to_pickle('D:/redata/factor/mmt_off_limit_A@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "209f2ab7-c3c2-42a4-847c-74c907eaa160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mmt_range_M\n",
    "amp = adj_high - adj_low\n",
    "rnt = adj_return\n",
    "empty = adj_return -adj_return\n",
    "empty = empty.replace(0,np.nan)\n",
    "rows = len(amp) -20\n",
    "\n",
    "for i in range(0,rows):\n",
    "    amp_20d = amp.iloc[i:i+20,:]\n",
    "    rnt_20d = rnt.iloc[i:i+20,:]\n",
    "\n",
    "    amp_df = amp_20d.rank(ascending=False,method = 'min')\n",
    "\n",
    "    amp_top20 = (amp_df <= 4).astype(int)\n",
    "    amp_low20 = (amp_df >= 16).astype(int)\n",
    "\n",
    "    rnt_top20 = (rnt_20d*amp_top20).rolling(20).sum()\n",
    "    rnt_low20 = (rnt_20d*amp_low20).rolling(20).sum()\n",
    "    result = rnt_top20 - rnt_low20\n",
    "    empty.iloc[i+20:i+21,:] = result.iloc[-1:,:]\n",
    "\n",
    "mmt_range_M = -1 * empty\n",
    "mmt_range_M.to_pickle('D:/redata/factor/mmt_range_M@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "69c14a9e-ec99-4ceb-88ec-2e809ac57904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#波动率\n",
    "vol_up_std_6M = -1 * adj_return.rolling(120).std()\n",
    "vol_up_std_6M.to_pickle('D:/redata/factor/vol_up_std_6M@factor.pkl')\n",
    "\n",
    "vol_highlow_std_6M = -1 * (adj_high/adj_low).rolling(120).std()\n",
    "vol_highlow_std_6M.to_pickle('D:/redata/factor/vol_highlow_std_6M@factor.pkl')\n",
    "\n",
    "vol_upshadow_std_6M = -1* ((adj_high-np.maximum(adj_open,adj_close))/adj_high).rolling(120).std()\n",
    "vol_upshadow_std_6M.to_pickle('D:/redata/factor/vol_upshadow_std_6M@factor.pkl')\n",
    "\n",
    "vol_w_downshadow_std_6M = -1 * ((adj_close-adj_low)/adj_low).rolling(120).std()\n",
    "vol_w_downshadow_std_6M.to_pickle('D:/redata/factor/vol_w_downshadow_std_6M@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1cb3e2af-f7fd-4502-aa6d-f1b369d3d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#流动性\n",
    "liq_turn_std_6M = -1* turnover_rate.rolling(120).std()\n",
    "liq_turn_std_6M.to_pickle('D:/redata/factor/liq_turn_std_6M@factor.pkl')\n",
    "\n",
    "liq_vstd_1M = -1* total_turnover.rolling(20).sum()/adj_return.rolling(20).std()\n",
    "liq_vstd_1M.to_pickle('D:/redata/factor/liq_vstd_1M@factor.pkl')\n",
    "\n",
    "liq_amiihud_avg_1M = (np.abs(adj_return)/total_turnover).rolling(20).mean()\n",
    "liq_amiihud_avg_1M.to_pickle('D:/redata/factor/liq_amiihud_avg_1M@factor.pkl')\n",
    "\n",
    "liq_shortcut_avg_1M = ((2*(adj_high -adj_low) - np.abs(adj_open -adj_close))/total_turnover).rolling(20).mean()\n",
    "liq_shortcut_avg_1M.to_pickle('D:/redata/factor/liq_shortcut_avg_1M@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d27de42c-8b6b-41dd-b54a-66eed9644fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#量价cov\n",
    "corr_ret_turnd_1M = turnover_rate.rolling(20).cov(adj_return)\n",
    "corr_ret_turnd_1M.to_pickle('D:/redata/factor/corr_ret_turnd_1M@factor.pkl')\n",
    "\n",
    "corr_price_turn_1M = -1* turnover_rate.rolling(20).cov(adj_close)\n",
    "corr_price_turn_1M.to_pickle('D:/redata/factor/corr_price_turn_1M@factor.pkl')\n",
    "\n",
    "corr_ret_turn_post_1M = -1 * turnover_rate.shift(1).rolling(20).cov(adj_close)\n",
    "corr_ret_turn_post_1M.to_pickle('D:/redata/factor/corr_ret_turn_post_1M@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1fa508fa-5386-4c7d-9a8b-5b1193db7a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#复合\n",
    "mom_A = 0.7*mmt_off_limit_A + 0.3*mmt_normal_A\n",
    "mom_M = 0.7*mmt_off_limit_M + 0.3*mmt_normal_M\n",
    "mom_overnight_A = mmt_overnight_A\n",
    "reverse_M = 0.5*mmt_intraday_M + 0.5*mmt_range_M\n",
    "vol_6M = 0.25*vol_up_std_6M + 0.25*vol_highlow_std_6M + 0.25*vol_upshadow_std_6M + 0.25*vol_w_downshadow_std_6M\n",
    "turnrate_6M = liq_turn_std_6M\n",
    "ill_price_M = 0.33333*liq_vstd_1M + 0.33333*liq_amiihud_avg_1M + 0.33333*liq_shortcut_avg_1M\n",
    "corrPV_M = 0.33333*corr_ret_turnd_1M + 0.33333*corr_price_turn_1M + 0.33333*corr_ret_turn_post_1M\n",
    "\n",
    "mom_A.to_pickle('D:/redata/factor/mom_A@factor.pkl')\n",
    "mom_M.to_pickle('D:/redata/factor/mom_M@factor.pkl')\n",
    "mom_overnight_A.to_pickle('D:/redata/factor/mom_overnight_A@factor.pkl')\n",
    "reverse_M.to_pickle('D:/redata/factor/reverse_M@factor.pkl')\n",
    "vol_6M.to_pickle('D:/redata/factor/vol_6M@factor.pkl')\n",
    "turnrate_6M.to_pickle('D:/redata/factor/turnrate_6M@factor.pkl')\n",
    "ill_price_M.to_pickle('D:/redata/factor/ill_price_M@factor.pkl')\n",
    "corrPV_M.to_pickle('D:/redata/factor/corrPV_M@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b636acc8-da8c-428b-9207-8f64b9d2e432",
   "metadata": {},
   "outputs": [],
   "source": [
    "#技术\n",
    "volume_20d_fac = -1 * volume.rolling(20).std()/volume.rolling(20).mean()\n",
    "volume_60d_ratio = -1 * volume.rolling(60).mean()/volume.rolling(240).mean()\n",
    "\n",
    "volume_20d_fac.to_pickle('D:/redata/factor/volume_20d_fac@factor.pkl')\n",
    "volume_60d_ratio.to_pickle('D:/redata/factor/volume_60d_ratio@factor.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54698af6-5bb2-41e2-9c98-e14694f4092c",
   "metadata": {},
   "source": [
    "# 量价"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ec9a06-897a-4e44-a5a9-5932c8c5c7aa",
   "metadata": {},
   "source": [
    "## 稳定因子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6eea50-b6fb-4e46-b079-e420ebf85879",
   "metadata": {},
   "source": [
    "换手率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1cf8dbe1-dc4d-4c71-bd86-9a7f0bcb9ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STR 凸显度\n",
    "df_ret = adj_return.copy()\n",
    "\n",
    "def sigma_cal(df_ret, theta=0.1):\n",
    "    returns_median = df_ret.median(axis=1)\n",
    "    frac1 = df_ret.sub(returns_median,axis=0).abs()\n",
    "    frac2 = df_ret.abs().add(returns_median.abs(), axis=0) + theta\n",
    "    sigma = frac1.div(frac2)\n",
    "    return sigma\n",
    "    \n",
    "def weight_cal(sigma, time_range=20, delta=0.7):\n",
    "    df = sigma.groupby(pd.Grouper(freq='20D'))\n",
    "    # df_cleaned = df.dropna(axis=1,how='any')\n",
    "    df_rank = df.apply(lambda x: x.rank(axis=0, ascending=False))\n",
    "    df_rank = df_rank.reset_index(level=0, drop=True)\n",
    "    frac1= df_rank.apply(lambda x: np.power(delta, x))\n",
    "    frac2= frac1.mean(axis=1)\n",
    "    weight= frac1.div(frac2, axis=0)\n",
    "    # assert frac1.iloc[0,1] / frac2.iloc[0] == weight.iloc[0,1]\n",
    "    return weight\n",
    "\n",
    "sigma = sigma_cal(df_ret,theta=1)\n",
    "weight = weight_cal(sigma, time_range=20, delta=0.7)\n",
    "STR = -1*weight.rolling(20).cov(df_ret)\n",
    "\n",
    "STR = STR.rolling(20).mean()\n",
    "STR.to_pickle('D:/redata/factor/STR@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6e010042-ed49-4e39-99e8-36720cb35734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#改进惊恐因子\n",
    "ret_daily = adj_return.copy()\n",
    "\n",
    "idx000985['idx985_ret'] = idx000985['close']/idx000985['prev_close'] -1\n",
    "idx000985 = idx000985[(idx000985['date']>='2014-01-02')]\n",
    "ret_daily['idx985_ret'] = idx000985['idx985_ret'].values\n",
    "\n",
    "def sigma_cal(ret_daily, theta=0.1):\n",
    "    idx985 = ret_daily.iloc[:,-1]\n",
    "    frac1 = ret_daily.sub(idx985,axis=0).abs()\n",
    "    frac2 = ret_daily.abs().add(idx985.abs(), axis=0) + theta\n",
    "    sigma = frac1.div(frac2)\n",
    "    return sigma\n",
    "terra = sigma_cal(ret_daily, theta=0.1)\n",
    "terra = terra.drop(columns=terra.columns[-1])\n",
    "\n",
    "weighted_dec = ret_daily * terra\n",
    "mean_20terra = weighted_dec.rolling(20).mean()\n",
    "std_20terra = weighted_dec.rolling(20).std()\n",
    "terra_20d = -(mean_20terra + std_20terra)\n",
    "\n",
    "terra_20d.to_pickle('D:/redata/factor/terra_20d@factor.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcdaa51-ac2c-4232-a330-0cf110c07c3d",
   "metadata": {},
   "source": [
    "换手率UMR"
   ]
  },
  {
   "cell_type": "raw",
   "id": "074e59a2-aef0-4757-99ce-9feed7ea62b0",
   "metadata": {},
   "source": [
    "adj_return_stocks = adj_return.copy()\n",
    "idx = idx000985.copy()\n",
    "\n",
    "idx['idx000985_return'] = idx['close'].pct_change()\n",
    "idx['date'] = pd.to_datetime(idx['date'])\n",
    "idx = idx[(idx['date']>'2014-01-01')]\n",
    "\n",
    "adj_return_stocks['idx000985_return'] = idx['idx000985_return'].values"
   ]
  },
  {
   "cell_type": "raw",
   "id": "48ee06b4-c63b-4d36-8849-d401a5fe4a32",
   "metadata": {},
   "source": [
    "start_date = adj_close.index.min()\n",
    "end_date = adj_close.index.max()\n",
    "date_range = pd.date_range(start_date, end_date,)\n",
    "\n",
    "newdf = pd.DataFrame(index = adj_close.index,columns = adj_close.columns)\n",
    "# newdf = newdf.reindex(date_range)\n",
    "\n",
    "for i in range(5293):\n",
    "    ex = adj_return_stocks.iloc[:,i] - adj_return_stocks['idx000985_return']\n",
    "    newdf.iloc[:,i] = ex.values\n",
    "\n",
    "exces_ret = pd.DataFrame(newdf)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e5df99c-da53-4fb6-b2bf-ebeb126ce53a",
   "metadata": {},
   "source": [
    "turnovers_risk = turnover_rate.rolling(10).mean() - turnover_rate\n",
    "turnovers_risk_ema = turnovers_risk.ewm(span=5).mean()\n",
    "turnovers_risk_ema_sum = turnovers_risk_ema.cumsum()\n",
    "turnover_umr = turnovers_risk_ema * exces_ret\n",
    "turnover_umr = turnover_umr.rolling(120).mean()\n",
    "\n",
    "turnover_umr.to_pickle('D:/redata/factor/turnover_umr@factor.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a9476e-d07e-4ad4-94c9-1ffa9da8dd13",
   "metadata": {},
   "source": [
    "收益率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4fcb1deb-562d-4dd4-abb6-824e13d14ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = 0.5 ** (1/40)\n",
    "daily_standard_deviation = -1*(np.sqrt(wt * ((adj_return - adj_return.rolling(252).mean())**2).rolling(252).sum()))\n",
    "\n",
    "daily_standard_deviation.to_pickle('D:/redata/factor/daily_standard_devijjwdation@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b5a9ea1a-3b67-42ce-bf5e-e09aa4db05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_daily = adj_return.copy()\n",
    "idx985 = idx000985.copy()\n",
    "idx985['idx985_ret'] = idx985['close']/idx985['prev_close'] -1\n",
    "idx985 = idx985[(idx985['date']>='2014-01-02')].set_index('date')\n",
    "idx985 = idx985[['idx985_ret']]\n",
    "# 计算超额收益\n",
    "excess_ret = ret_daily.sub(idx985['idx985_ret'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4858e59c-67d4-4494-b5c0-bb8e80231332",
   "metadata": {},
   "outputs": [],
   "source": [
    "high1 = adj_high\n",
    "low1 = adj_low\n",
    "close1 = adj_close\n",
    "\n",
    "high_r_std_Nm = (high1 / close1.shift(1)).rolling(120).std()\n",
    "low_r_std_Nm = (low1 / close1.shift(1)).rolling(120).std()\n",
    "\n",
    "hml_r_std_Nm = (high_r_std_Nm - low_r_std_Nm) * -1\n",
    "hpl_r_std_Nm = (high_r_std_Nm + low_r_std_Nm) * -1\n",
    "\n",
    "# hml_r_std_Nm = filter_extreme_by_MAD(hml_r_std_Nm)\n",
    "# hpl_r_std_Nm = filter_extreme_by_MAD(hpl_r_std_Nm)\n",
    "\n",
    "hml_r_std_Nm.to_pickle('D:/redata/factor/hml_r_std_6m@factor.pkl')\n",
    "hpl_r_std_Nm.to_pickle('D:/redata/factor/hpl_r_std_6m@factor.pkl')\n",
    "\n",
    "high_r_std_N = (high1 / close1.shift(1)).rolling(100).std()\n",
    "high_r_std_N = -1 * high_r_std_Nm\n",
    "# high_r_std_N = filter_extreme_by_MAD(high_r_std_N)\n",
    "high_r_std_N.to_pickle('D:/redata/factor/high_r_std_5m@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f08fcbfa-de1e-4e20-bf7a-c69f8af4d17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_5 = adj_close.rolling(5).mean() * -1\n",
    "bias_5 = -1*((adj_close/ma_5) - 1)\n",
    "bias_5.to_pickle('D:/redata/factor/bias_5@factor.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebb765d-196a-4675-8375-7013ff486118",
   "metadata": {},
   "source": [
    "# 300因子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5242119f-ebca-4084-aabb-acd29b12fa82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Mon12m\n",
    "adj_return_daily = adj_return.fillna(0)\n",
    "monthly_returns = adj_return_daily.groupby(adj_return_daily.index.to_period('M')).apply(lambda x: (1 + x).prod() - 1)  # 累积每日收益率\n",
    "monthly_returns = monthly_returns +1\n",
    "monthly_returns = monthly_returns.rolling(11).apply(lambda x: (x).prod())\n",
    "monthly_returns = monthly_returns -1\n",
    "\n",
    "empty = adj_close - adj_close\n",
    "empty = empty.replace(0,np.nan)\n",
    "empty.iloc[-1:,:] = monthly_returns.iloc[-1:,:]\n",
    "mon12m = empty.bfill()\n",
    "mon12m = mon12m.fillna(0)\n",
    "mon12m.to_pickle('D:/redata/factor/mon12m@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "208cdf4e-0c73-4901-a9de-381b5f4c337f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ticker</th>\n",
       "      <th>000001</th>\n",
       "      <th>000002</th>\n",
       "      <th>000004</th>\n",
       "      <th>000005</th>\n",
       "      <th>000006</th>\n",
       "      <th>000007</th>\n",
       "      <th>000008</th>\n",
       "      <th>000009</th>\n",
       "      <th>000010</th>\n",
       "      <th>000011</th>\n",
       "      <th>...</th>\n",
       "      <th>688787</th>\n",
       "      <th>688788</th>\n",
       "      <th>688789</th>\n",
       "      <th>688793</th>\n",
       "      <th>688798</th>\n",
       "      <th>688799</th>\n",
       "      <th>688800</th>\n",
       "      <th>688819</th>\n",
       "      <th>688981</th>\n",
       "      <th>689009</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-02</th>\n",
       "      <td>0.368541</td>\n",
       "      <td>-0.213542</td>\n",
       "      <td>0.17247</td>\n",
       "      <td>-0.117021</td>\n",
       "      <td>0.923261</td>\n",
       "      <td>0.588367</td>\n",
       "      <td>0.455399</td>\n",
       "      <td>-0.122828</td>\n",
       "      <td>0.138211</td>\n",
       "      <td>0.091957</td>\n",
       "      <td>...</td>\n",
       "      <td>1.149057</td>\n",
       "      <td>0.178856</td>\n",
       "      <td>0.261117</td>\n",
       "      <td>0.166018</td>\n",
       "      <td>0.385387</td>\n",
       "      <td>0.155855</td>\n",
       "      <td>1.124046</td>\n",
       "      <td>0.163362</td>\n",
       "      <td>1.252483</td>\n",
       "      <td>1.020991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03</th>\n",
       "      <td>0.368541</td>\n",
       "      <td>-0.213542</td>\n",
       "      <td>0.17247</td>\n",
       "      <td>-0.117021</td>\n",
       "      <td>0.923261</td>\n",
       "      <td>0.588367</td>\n",
       "      <td>0.455399</td>\n",
       "      <td>-0.122828</td>\n",
       "      <td>0.138211</td>\n",
       "      <td>0.091957</td>\n",
       "      <td>...</td>\n",
       "      <td>1.149057</td>\n",
       "      <td>0.178856</td>\n",
       "      <td>0.261117</td>\n",
       "      <td>0.166018</td>\n",
       "      <td>0.385387</td>\n",
       "      <td>0.155855</td>\n",
       "      <td>1.124046</td>\n",
       "      <td>0.163362</td>\n",
       "      <td>1.252483</td>\n",
       "      <td>1.020991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-06</th>\n",
       "      <td>0.368541</td>\n",
       "      <td>-0.213542</td>\n",
       "      <td>0.17247</td>\n",
       "      <td>-0.117021</td>\n",
       "      <td>0.923261</td>\n",
       "      <td>0.588367</td>\n",
       "      <td>0.455399</td>\n",
       "      <td>-0.122828</td>\n",
       "      <td>0.138211</td>\n",
       "      <td>0.091957</td>\n",
       "      <td>...</td>\n",
       "      <td>1.149057</td>\n",
       "      <td>0.178856</td>\n",
       "      <td>0.261117</td>\n",
       "      <td>0.166018</td>\n",
       "      <td>0.385387</td>\n",
       "      <td>0.155855</td>\n",
       "      <td>1.124046</td>\n",
       "      <td>0.163362</td>\n",
       "      <td>1.252483</td>\n",
       "      <td>1.020991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-07</th>\n",
       "      <td>0.368541</td>\n",
       "      <td>-0.213542</td>\n",
       "      <td>0.17247</td>\n",
       "      <td>-0.117021</td>\n",
       "      <td>0.923261</td>\n",
       "      <td>0.588367</td>\n",
       "      <td>0.455399</td>\n",
       "      <td>-0.122828</td>\n",
       "      <td>0.138211</td>\n",
       "      <td>0.091957</td>\n",
       "      <td>...</td>\n",
       "      <td>1.149057</td>\n",
       "      <td>0.178856</td>\n",
       "      <td>0.261117</td>\n",
       "      <td>0.166018</td>\n",
       "      <td>0.385387</td>\n",
       "      <td>0.155855</td>\n",
       "      <td>1.124046</td>\n",
       "      <td>0.163362</td>\n",
       "      <td>1.252483</td>\n",
       "      <td>1.020991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-08</th>\n",
       "      <td>0.368541</td>\n",
       "      <td>-0.213542</td>\n",
       "      <td>0.17247</td>\n",
       "      <td>-0.117021</td>\n",
       "      <td>0.923261</td>\n",
       "      <td>0.588367</td>\n",
       "      <td>0.455399</td>\n",
       "      <td>-0.122828</td>\n",
       "      <td>0.138211</td>\n",
       "      <td>0.091957</td>\n",
       "      <td>...</td>\n",
       "      <td>1.149057</td>\n",
       "      <td>0.178856</td>\n",
       "      <td>0.261117</td>\n",
       "      <td>0.166018</td>\n",
       "      <td>0.385387</td>\n",
       "      <td>0.155855</td>\n",
       "      <td>1.124046</td>\n",
       "      <td>0.163362</td>\n",
       "      <td>1.252483</td>\n",
       "      <td>1.020991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-23</th>\n",
       "      <td>0.368541</td>\n",
       "      <td>-0.213542</td>\n",
       "      <td>0.17247</td>\n",
       "      <td>-0.117021</td>\n",
       "      <td>0.923261</td>\n",
       "      <td>0.588367</td>\n",
       "      <td>0.455399</td>\n",
       "      <td>-0.122828</td>\n",
       "      <td>0.138211</td>\n",
       "      <td>0.091957</td>\n",
       "      <td>...</td>\n",
       "      <td>1.149057</td>\n",
       "      <td>0.178856</td>\n",
       "      <td>0.261117</td>\n",
       "      <td>0.166018</td>\n",
       "      <td>0.385387</td>\n",
       "      <td>0.155855</td>\n",
       "      <td>1.124046</td>\n",
       "      <td>0.163362</td>\n",
       "      <td>1.252483</td>\n",
       "      <td>1.020991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-24</th>\n",
       "      <td>0.368541</td>\n",
       "      <td>-0.213542</td>\n",
       "      <td>0.17247</td>\n",
       "      <td>-0.117021</td>\n",
       "      <td>0.923261</td>\n",
       "      <td>0.588367</td>\n",
       "      <td>0.455399</td>\n",
       "      <td>-0.122828</td>\n",
       "      <td>0.138211</td>\n",
       "      <td>0.091957</td>\n",
       "      <td>...</td>\n",
       "      <td>1.149057</td>\n",
       "      <td>0.178856</td>\n",
       "      <td>0.261117</td>\n",
       "      <td>0.166018</td>\n",
       "      <td>0.385387</td>\n",
       "      <td>0.155855</td>\n",
       "      <td>1.124046</td>\n",
       "      <td>0.163362</td>\n",
       "      <td>1.252483</td>\n",
       "      <td>1.020991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-25</th>\n",
       "      <td>0.368541</td>\n",
       "      <td>-0.213542</td>\n",
       "      <td>0.17247</td>\n",
       "      <td>-0.117021</td>\n",
       "      <td>0.923261</td>\n",
       "      <td>0.588367</td>\n",
       "      <td>0.455399</td>\n",
       "      <td>-0.122828</td>\n",
       "      <td>0.138211</td>\n",
       "      <td>0.091957</td>\n",
       "      <td>...</td>\n",
       "      <td>1.149057</td>\n",
       "      <td>0.178856</td>\n",
       "      <td>0.261117</td>\n",
       "      <td>0.166018</td>\n",
       "      <td>0.385387</td>\n",
       "      <td>0.155855</td>\n",
       "      <td>1.124046</td>\n",
       "      <td>0.163362</td>\n",
       "      <td>1.252483</td>\n",
       "      <td>1.020991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-26</th>\n",
       "      <td>0.368541</td>\n",
       "      <td>-0.213542</td>\n",
       "      <td>0.17247</td>\n",
       "      <td>-0.117021</td>\n",
       "      <td>0.923261</td>\n",
       "      <td>0.588367</td>\n",
       "      <td>0.455399</td>\n",
       "      <td>-0.122828</td>\n",
       "      <td>0.138211</td>\n",
       "      <td>0.091957</td>\n",
       "      <td>...</td>\n",
       "      <td>1.149057</td>\n",
       "      <td>0.178856</td>\n",
       "      <td>0.261117</td>\n",
       "      <td>0.166018</td>\n",
       "      <td>0.385387</td>\n",
       "      <td>0.155855</td>\n",
       "      <td>1.124046</td>\n",
       "      <td>0.163362</td>\n",
       "      <td>1.252483</td>\n",
       "      <td>1.020991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-27</th>\n",
       "      <td>0.368541</td>\n",
       "      <td>-0.213542</td>\n",
       "      <td>0.17247</td>\n",
       "      <td>-0.117021</td>\n",
       "      <td>0.923261</td>\n",
       "      <td>0.588367</td>\n",
       "      <td>0.455399</td>\n",
       "      <td>-0.122828</td>\n",
       "      <td>0.138211</td>\n",
       "      <td>0.091957</td>\n",
       "      <td>...</td>\n",
       "      <td>1.149057</td>\n",
       "      <td>0.178856</td>\n",
       "      <td>0.261117</td>\n",
       "      <td>0.166018</td>\n",
       "      <td>0.385387</td>\n",
       "      <td>0.155855</td>\n",
       "      <td>1.124046</td>\n",
       "      <td>0.163362</td>\n",
       "      <td>1.252483</td>\n",
       "      <td>1.020991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2674 rows × 5328 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "ticker        000001    000002   000004    000005    000006    000007  \\\n",
       "date                                                                    \n",
       "2014-01-02  0.368541 -0.213542  0.17247 -0.117021  0.923261  0.588367   \n",
       "2014-01-03  0.368541 -0.213542  0.17247 -0.117021  0.923261  0.588367   \n",
       "2014-01-06  0.368541 -0.213542  0.17247 -0.117021  0.923261  0.588367   \n",
       "2014-01-07  0.368541 -0.213542  0.17247 -0.117021  0.923261  0.588367   \n",
       "2014-01-08  0.368541 -0.213542  0.17247 -0.117021  0.923261  0.588367   \n",
       "...              ...       ...      ...       ...       ...       ...   \n",
       "2024-12-23  0.368541 -0.213542  0.17247 -0.117021  0.923261  0.588367   \n",
       "2024-12-24  0.368541 -0.213542  0.17247 -0.117021  0.923261  0.588367   \n",
       "2024-12-25  0.368541 -0.213542  0.17247 -0.117021  0.923261  0.588367   \n",
       "2024-12-26  0.368541 -0.213542  0.17247 -0.117021  0.923261  0.588367   \n",
       "2024-12-27  0.368541 -0.213542  0.17247 -0.117021  0.923261  0.588367   \n",
       "\n",
       "ticker        000008    000009    000010    000011  ...    688787    688788  \\\n",
       "date                                                ...                       \n",
       "2014-01-02  0.455399 -0.122828  0.138211  0.091957  ...  1.149057  0.178856   \n",
       "2014-01-03  0.455399 -0.122828  0.138211  0.091957  ...  1.149057  0.178856   \n",
       "2014-01-06  0.455399 -0.122828  0.138211  0.091957  ...  1.149057  0.178856   \n",
       "2014-01-07  0.455399 -0.122828  0.138211  0.091957  ...  1.149057  0.178856   \n",
       "2014-01-08  0.455399 -0.122828  0.138211  0.091957  ...  1.149057  0.178856   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "2024-12-23  0.455399 -0.122828  0.138211  0.091957  ...  1.149057  0.178856   \n",
       "2024-12-24  0.455399 -0.122828  0.138211  0.091957  ...  1.149057  0.178856   \n",
       "2024-12-25  0.455399 -0.122828  0.138211  0.091957  ...  1.149057  0.178856   \n",
       "2024-12-26  0.455399 -0.122828  0.138211  0.091957  ...  1.149057  0.178856   \n",
       "2024-12-27  0.455399 -0.122828  0.138211  0.091957  ...  1.149057  0.178856   \n",
       "\n",
       "ticker        688789    688793    688798    688799    688800    688819  \\\n",
       "date                                                                     \n",
       "2014-01-02  0.261117  0.166018  0.385387  0.155855  1.124046  0.163362   \n",
       "2014-01-03  0.261117  0.166018  0.385387  0.155855  1.124046  0.163362   \n",
       "2014-01-06  0.261117  0.166018  0.385387  0.155855  1.124046  0.163362   \n",
       "2014-01-07  0.261117  0.166018  0.385387  0.155855  1.124046  0.163362   \n",
       "2014-01-08  0.261117  0.166018  0.385387  0.155855  1.124046  0.163362   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2024-12-23  0.261117  0.166018  0.385387  0.155855  1.124046  0.163362   \n",
       "2024-12-24  0.261117  0.166018  0.385387  0.155855  1.124046  0.163362   \n",
       "2024-12-25  0.261117  0.166018  0.385387  0.155855  1.124046  0.163362   \n",
       "2024-12-26  0.261117  0.166018  0.385387  0.155855  1.124046  0.163362   \n",
       "2024-12-27  0.261117  0.166018  0.385387  0.155855  1.124046  0.163362   \n",
       "\n",
       "ticker        688981    689009  \n",
       "date                            \n",
       "2014-01-02  1.252483  1.020991  \n",
       "2014-01-03  1.252483  1.020991  \n",
       "2014-01-06  1.252483  1.020991  \n",
       "2014-01-07  1.252483  1.020991  \n",
       "2014-01-08  1.252483  1.020991  \n",
       "...              ...       ...  \n",
       "2024-12-23  1.252483  1.020991  \n",
       "2024-12-24  1.252483  1.020991  \n",
       "2024-12-25  1.252483  1.020991  \n",
       "2024-12-26  1.252483  1.020991  \n",
       "2024-12-27  1.252483  1.020991  \n",
       "\n",
       "[2674 rows x 5328 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty = adj_close - adj_close\n",
    "empty = empty.replace(0,np.nan)\n",
    "empty.iloc[-1:,:] = monthly_returns.iloc[-1:,:]\n",
    "mon12m = empty.bfill()\n",
    "mon12m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "37912390-b769-43ea-8916-d1e34b538f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AM\n",
    "\n",
    "#单季度\n",
    "col1 = 'total_assets'\n",
    "df_single_Q = single_Q(dfcc,col1)\n",
    "\n",
    "col2 = 'total_assets_Q'\n",
    "df_fin = process_finfactor_and_save_data(df_single_Q, col2, open)\n",
    "\n",
    "AM = df_fin / circulation_market_value\n",
    "AM.to_pickle('D:/redata/factor/AM@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "dfb2ec82-11c4-4af3-addc-08a140fe1311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cashprod\n",
    "\n",
    "#单季度\n",
    "col1 = 'total_assets'\n",
    "df_single_Q = single_Q(dfcc,col1)\n",
    "\n",
    "col2 = 'total_assets_Q'\n",
    "at = process_finfactor_and_save_data(df_single_Q, col2, open)\n",
    "\n",
    "#单季度\n",
    "col11 = 'cash_equivalent'\n",
    "df_single_Qq = single_Q(dfcc,col11)\n",
    "\n",
    "col22 = 'cash_equivalent_Q'\n",
    "che = process_finfactor_and_save_data(df_single_Qq, col22, open)\n",
    "\n",
    "cashprod = (circulation_market_value - at)/che\n",
    "cashprod.to_pickle('D:/redata/factor/cashprod@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "64971d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gross profitability\n",
    "\n",
    "#单季度\n",
    "col1 = 'total_assets'\n",
    "df_single_Q = single_Q(dfcc,col1)\n",
    "\n",
    "col2 = 'total_assets_Q'\n",
    "at = process_finfactor_and_save_data(df_single_Q, col2, open)\n",
    "\n",
    "#单季度\n",
    "col11 = 'cost_of_goods_sold'\n",
    "df_single_Qq = single_Q(dfaa,col11)\n",
    "\n",
    "col22 = 'cost_of_goods_sold_Q'\n",
    "cogs = process_finfactor_and_save_data(df_single_Qq, col22, open)\n",
    "\n",
    "#单季度\n",
    "col111 = 'revenue'\n",
    "df_single_Qqq = single_Q(dfaa,col111)\n",
    "\n",
    "col222 = 'revenue_Q'\n",
    "rvn = process_finfactor_and_save_data(df_single_Qqq, col222, open)\n",
    "\n",
    "gp = (rvn - cogs)/at\n",
    "gp.to_pickle('D:/redata/factor/gp@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5b76b770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dolvol\n",
    "\n",
    "dolvol = np.log(volume.shift(60)*np.abs(adj_close.shift(60)))\n",
    "dolvol = -dolvol\n",
    "dolvol.to_pickle('D:/redata/factor/dolvol@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ee2bd5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eearnig2Price\n",
    "\n",
    "#单季度\n",
    "col1 = 'profit_before_tax'\n",
    "df_singleQ = single_Q(dfaa,col1)\n",
    "\n",
    "col2 = 'profit_before_tax_Q'\n",
    "df_fin = process_finfactor_and_save_data(df_singleQ, col2, open)\n",
    "\n",
    "EP = df_fin / circulation_market_value\n",
    "# EP = np.where(EP < 0, np.nan, EP)\n",
    "a = EP>0\n",
    "Eearnig2Price = (EP*a).replace(0,np.nan)\n",
    "Eearnig2Price.to_pickle('D:/redata/factor/Eearnig2Price@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "78fdd1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#52weekshigh\n",
    "\n",
    "temp = adj_close.rolling(window=240, min_periods=20).max()\n",
    "h52 = adj_close/ temp\n",
    "h52.to_pickle('D:/redata/factor/h52@factor.pkl')                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f0c2a5af-f09c-45e6-aaea-d22b85f0f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ill\n",
    "ill = np.abs(adj_return)/(adj_close*volume)\n",
    "ill = ill.rolling(20).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a1d50-941b-4ef8-9897-33eff3e067bf",
   "metadata": {},
   "source": [
    "# 财务因子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8963bac8-0bf1-4603-947f-c0e6d100ba80",
   "metadata": {},
   "source": [
    "## 稳定因子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6bc73ae0-568e-4dab-832f-55bda543466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#净利润同比增速\n",
    "#单季度\n",
    "col1 = 'net_profit'\n",
    "df_single_Q = single_Q(dfaa,col1)\n",
    "df_merge = df_single_Q.merge(mls,on=['report_period'],how='left')\n",
    "\n",
    "# ttm12月累计——QoQ环比增速——yoy同比增速——Diff当季-上季数值\n",
    "col2 = 'net_profit_Q'\n",
    "df_computed = compute_yoy(df_merge,col2)\n",
    "\n",
    "col3 = 'net_profit_Q_yoy'\n",
    "df_fin = process_finfactor_and_save_data(df_computed, col3, open)\n",
    "df_fin.to_pickle('D:/redata/factor/net_profit_Q_yoy@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "caf85d36-371b-4d45-9bca-3fa6c8a0d6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#营业利润同比增速\n",
    "#单季度\n",
    "col1 = 'operating_revenue'\n",
    "df_single_Q = single_Q(dfaa,col1)\n",
    "df_merge = df_single_Q.merge(mls,on=['report_period'],how='left')\n",
    "\n",
    "# ttm12月累计——QoQ环比增速——yoy同比增速——Diff当季-上季数值\n",
    "col2 = 'operating_revenue_Q'\n",
    "df_computed = compute_yoy(df_merge,col2)\n",
    "\n",
    "col3 = 'operating_revenue_Q_yoy'\n",
    "df_fin = process_finfactor_and_save_data(df_computed, col3, open)\n",
    "df_fin.to_pickle('D:/redata/factor/operating_revenue_Q_yoy@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4a8512f5-e2f8-47c2-9427-5a21300e0d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#归母净利润同比增速\n",
    "\n",
    "#单季度\n",
    "col1 = 'net_profit_parent_company'\n",
    "df_single_Q = single_Q(dfaa,col1)\n",
    "df_merge = df_single_Q.merge(mls,on=['report_period'],how='left')\n",
    "\n",
    "# ttm12月累计——QoQ环比增速——yoy同比增速——Diff当季-上季数值\n",
    "col2 = 'net_profit_parent_company_Q'\n",
    "df_computed = compute_yoy(df_merge,col2)\n",
    "\n",
    "col3 = 'net_profit_parent_company_Q_yoy'\n",
    "df_fin = process_finfactor_and_save_data(df_computed, col3, open)\n",
    "df_fin.to_pickle('D:/redata/factor/net_profit_parent_company_Q_yoy@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "28cdca2b-757b-4d2d-a257-7cff73eb7df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当季营业利润同比增长率\n",
    "\n",
    "#单季度\n",
    "col1 = 'operating_revenue'\n",
    "df_single_Q = single_Q(dfaa,col1)\n",
    "df_merge = df_single_Q.merge(mls,on=['report_period'],how='left')\n",
    "\n",
    "# ttm12月累计——QoQ环比增速——yoy同比增速——Diff当季-上季数值\n",
    "col2 = 'operating_revenue_Q'\n",
    "df_computed = compute_yoy(df_merge,col2)\n",
    "\n",
    "col3 = 'operating_revenue_Q_yoy'\n",
    "df_fin = process_finfactor_and_save_data(df_computed, col3, open)\n",
    "df_fin.to_pickle('D:/redata/factor/operating_revenue_Q_yoy@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1dbe5990-0275-4955-9ba6-704e58b70277",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 营业利润ttm\n",
    "\n",
    "#单季度\n",
    "col1 = 'operating_revenue'\n",
    "df_single_Q = single_Q(dfaa,col1)\n",
    "df_merge = df_single_Q.merge(mls,on=['report_period'],how='left')\n",
    "\n",
    "# ttm12月累计——QoQ环比增速——yoy同比增速——Diff当季-上季数值\n",
    "col2 = 'operating_revenue'\n",
    "df_computed = compute_ttm(df_merge,col2)\n",
    "\n",
    "col3 = 'operating_revenue_TTM'\n",
    "df_fin = process_finfactor_and_save_data(df_computed, col3, open)\n",
    "\n",
    "sp_ttm = df_fin / mc\n",
    "sp_ttm.to_pickle('D:/redata/factor/sp_ttm@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fdd13589-1ebb-4326-86e4-9dd694c2c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BP 股东权益/总市值\n",
    "\n",
    "#单季度\n",
    "col1 = 'equity_parent_company'\n",
    "df_singleQ = single_Q(dfcc,col1)\n",
    "\n",
    "col2 = 'equity_parent_company_Q'\n",
    "df_fin = process_finfactor_and_save_data(df_singleQ, col2, open)\n",
    "\n",
    "BP = df_fin / mc\n",
    "BP.to_pickle('D:/redata/factor/BP@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "be65fd41-337f-4bc8-b0e0-098b13a41519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EP 因子\n",
    "\n",
    "#单季度\n",
    "col = 'net_profit'\n",
    "df_single_Q = single_Q(dfaa,col)\n",
    "\n",
    "col2 = 'net_profit_Q'\n",
    "df_fin = process_finfactor_and_save_data(df_single_Q, col2, open)\n",
    "\n",
    "EP = df_fin / mc\n",
    "EP.to_pickle('D:/redata/factor/EP@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a62a0f70-0792-4628-b3a2-acc95d9c97ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPttm 因子\n",
    "\n",
    "#单季度\n",
    "col = 'net_profit'\n",
    "df_single_Q = single_Q(dfaa,col)\n",
    "df_merge = df_single_Q.merge(mls,on=['report_period'],how='left')\n",
    "\n",
    "col2 = 'net_profit'\n",
    "df_computed = compute_ttm(df_merge,col2)\n",
    "\n",
    "col3 = 'net_profit_TTM'\n",
    "df_fin = process_finfactor_and_save_data(df_computed, col3, open)\n",
    "EP_ttm = df_fin / mc\n",
    "\n",
    "EP_ttm.to_pickle('D:/redata/factor/EP_ttm@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "008112e0-416c-45e9-bde1-5c77586e0d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CFP_ttm\n",
    "\n",
    "#单季度\n",
    "col = 'cash_flow_from_operating_activities'\n",
    "df_single_Q = single_Q(dfbb,col)\n",
    "df_merge = df_single_Q.merge(mlss,on=['report_period'],how='left')\n",
    "\n",
    "# ttm12月累计——QoQ环比增速——yoy同比增速——Diff当季-上季数值\n",
    "df_ttm = compute_ttm(df_merge,col)\n",
    "\n",
    "col1 = 'cash_flow_from_operating_activities_TTM'\n",
    "df_fin = process_finfactor_and_save_data(df_ttm, col1, open)\n",
    "\n",
    "cfp = df_fin/mc\n",
    "cfp.to_pickle('D:/redata/factor/cfp_ttm@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "4a72016d-84b2-40c3-928a-0d6a1fad3952",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 归母ROE\n",
    "\n",
    "#单季度_np\n",
    "col1 = 'net_profit_parent_company'\n",
    "df_single_Q_nppc = single_Q(dfaa,col1)\n",
    "\n",
    "col2 = 'net_profit_parent_company_Q'\n",
    "df_nppc = process_finfactor_and_save_data(df_single_Q_nppc, col2, open)\n",
    "\n",
    "#单季度_epc\n",
    "col3 = 'equity_parent_company'\n",
    "df_single_Q_epc = single_Q(dfcc,col3)\n",
    "\n",
    "col4 = 'equity_parent_company_Q'\n",
    "df_epc = process_finfactor_and_save_data(df_single_Q_epc, col4, open)\n",
    "\n",
    "#单季roe\n",
    "roe = df_nppc/df_epc\n",
    "roe.to_pickle('D:/redata/factor/roe@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9179cbc5-7f16-4481-885a-d1f7f688c860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 归母ROE同比\n",
    "\n",
    "df_roe = df_single_Q_nppc.merge(df_single_Q_epc, on=['ticker', 'date', 'report_period'], how='left')\n",
    "df_roe['roe_Q'] = df_roe['net_profit_parent_company_Q']/ df_roe['equity_parent_company_Q']\n",
    "df_merge = df_roe.merge(mls,on=['report_period'],how='left')\n",
    "\n",
    "# ttm12月累计——QoQ环比增速——yoy同比增速——Diff当季-上季数值\n",
    "col5 = 'roe_Q'\n",
    "df_computed = compute_yoy(df_merge,col5)\n",
    "col6 = 'roe_Q_yoy'\n",
    "delt_roe = process_finfactor_and_save_data(df_computed, col6, open)\n",
    "\n",
    "delt_roe.to_pickle('D:/redata/factor/roe_yoy@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e5ee8902-7852-4887-ab8d-a2a3898beeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 归母ROA\n",
    "\n",
    "#单季度_np\n",
    "col1 = 'net_profit_parent_company'\n",
    "df_single_Q_nppc = single_Q(dfaa,col1)\n",
    "\n",
    "col2 = 'net_profit_parent_company_Q'\n",
    "df_nppc = process_finfactor_and_save_data(df_single_Q_nppc, col2, open)\n",
    "\n",
    "#单季度_epc\n",
    "col3 = 'total_assets'\n",
    "df_single_Q_ta = single_Q(dfcc,col3)\n",
    "\n",
    "col4 = 'total_assets_Q'\n",
    "df_ta = process_finfactor_and_save_data(df_single_Q_ta, col4, open)\n",
    "\n",
    "#单季roe\n",
    "roa = df_nppc/df_ta\n",
    "roa.to_pickle('D:/redata/factor/roa@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e98f2e1f-13ee-42c6-a637-983168b30590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 归母ROA同比\n",
    "\n",
    "df_roa = df_single_Q_nppc.merge(df_single_Q_ta, on=['ticker', 'date', 'report_period'], how='left')\n",
    "df_roa['roa_Q'] = df_roa['net_profit_parent_company_Q']/ df_roa['total_assets_Q']\n",
    "df_merge = df_roa.merge(mls,on=['report_period'],how='left')\n",
    "\n",
    "# ttm12月累计——QoQ环比增速——yoy同比增速——Diff当季-上季数值\n",
    "col5 = 'roa_Q'\n",
    "df_computed = compute_yoy(df_merge,col5)\n",
    "col6 = 'roa_Q_yoy'\n",
    "delt_roa = process_finfactor_and_save_data(df_computed, col6, open)\n",
    "\n",
    "delt_roa.to_pickle('D:/redata/factor/roa_yoy@factor.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf0b037b-3995-4e76-a0ec-d3287a2f7c2e",
   "metadata": {},
   "source": [
    "#杠杆\n",
    "col1 = 'non_current_assets'\n",
    "df_single_Q_nca = single_Q(dfcc,col1)\n",
    "\n",
    "col2 = 'non_current_liabilities'\n",
    "df_single_Q_ncl = single_Q(dfcc,col2)\n",
    "\n",
    "df_merge =  df_single_Q_nca.merge(df_single_Q_ncl, on=['ticker', 'date', 'report_period'], how='left')\n",
    "df_merge['lever_Q'] = df_merge['non_current_liabilities_Q']/ df_merge['non_current_assets_Q']\n",
    "\n",
    "# ttm12月累计——QoQ环比增速——yoy同比增速——Diff当季-上季数值\n",
    "col3 = 'lever_Q'\n",
    "df_computed = compute_yoy(df_merge,col3)\n",
    "\n",
    "col4 = 'lever_Q_yoy'\n",
    "delt_lever = process_finfactor_and_save_data(df_computed, col4, open)\n",
    "\n",
    "delt_lever.to_pickle('D:/redata/factor/lever_yoy@factor.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c067bbd-e8c3-45fb-bc92-cc02808fb210",
   "metadata": {},
   "source": [
    "# qfa_operateincometoebt 单季度 经营活动净收益/利润\n",
    "\n",
    "# 经营活动净收益\n",
    "col1 = 'net_profit_deduct_non_recurring_pnl'\n",
    "df_single_Q_npop = single_Q(dfaa,col1)\n",
    "\n",
    "col2 = 'net_profit_deduct_non_recurring_pnl_Q'\n",
    "df_npop = process_finfactor_and_save_data(df_single_Q_npop, col2, open)\n",
    "\n",
    "# 利润\n",
    "col3 = 'profit_from_operation'\n",
    "df_single_Q_op = single_Q(dfaa,col3)\n",
    "\n",
    "col4 = 'profit_from_operation_Q'\n",
    "df_op = process_finfactor_and_save_data(df_single_Q_op, col4, open)\n",
    "\n",
    "# 单季roe\n",
    "qfa_operateincometoebt = df_npop/df_op\n",
    "qfa_operateincometoebt = -1 * qfa_operateincometoebt\n",
    "qfa_operateincometoebt.to_pickle('D:/redata/factor/qfa_operateincometoebt@factor.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b219b300-e3ce-479c-8774-66d73c2ac759",
   "metadata": {},
   "source": [
    "# qfa_ocftosales 单季度 经营活动净收益/利润\n",
    "\n",
    "# 经营活动净收益\n",
    "col1 = 'cash_flow_from_operating_activities'\n",
    "df_single_Q = single_Q(dfbb,col1)\n",
    "\n",
    "col2 = 'cash_flow_from_operating_activities_Q'\n",
    "df1 = process_finfactor_and_save_data(df_single_Q, col2, open)\n",
    "\n",
    "# 利润\n",
    "col3 = 'operating_revenue'\n",
    "df_single_Q = single_Q(dfaa,col3)\n",
    "\n",
    "col4 = 'operating_revenue_Q'\n",
    "df2 = process_finfactor_and_save_data(df_single_Q, col4, open)\n",
    "\n",
    "# 单季roe\n",
    "qfa_ocftosales = df1/df2\n",
    "qfa_ocftosales.to_pickle('D:/redata/factor/qfa_ocftosales@factor.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f13a4401-1ebd-4c28-a018-4f6522b665c4",
   "metadata": {},
   "source": [
    "# currentdebtodebt 单季度 经营活动净收益/利润\n",
    "\n",
    "# 经营活动净收益\n",
    "col1 = 'current_liabilities'\n",
    "df_single_Q = single_Q(dfcc,col1)\n",
    "\n",
    "col2 = 'current_liabilities_Q'\n",
    "df1 = process_finfactor_and_save_data(df_single_Q, col2, open)\n",
    "\n",
    "# 利润\n",
    "col3 = 'total_liabilities'\n",
    "df_single_Q = single_Q(dfcc,col3)\n",
    "\n",
    "col4 = 'total_liabilities_Q'\n",
    "df2 = process_finfactor_and_save_data(df_single_Q, col4, open)\n",
    "\n",
    "# 单季roe\n",
    "currentdebtodebt = df1/df2\n",
    "currentdebtodebt.to_pickle('D:/redata/factor/currentdebtodebt@factor.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "482954d7-bdc4-4705-9444-30b0da63e1cb",
   "metadata": {},
   "source": [
    "# current 单季度 经营活动净收益/利润\n",
    "\n",
    "# 经营活动净收益\n",
    "col1 = 'current_assets'\n",
    "df_single_Q1 = single_Q(dfcc,col1)\n",
    "\n",
    "col2 = 'current_assets_Q'\n",
    "df1 = process_finfactor_and_save_data(df_single_Q1, col2, open)\n",
    "\n",
    "# 利润\n",
    "col3 = 'current_liabilities'\n",
    "df_single_Q2 = single_Q(dfcc,col3)\n",
    "\n",
    "col4 = 'current_liabilities_Q'\n",
    "df2 = process_finfactor_and_save_data(df_single_Q2, col4, open)\n",
    "\n",
    "# 单季roe\n",
    "current = df1/df2\n",
    "current.to_pickle('D:/redata/factor/current@factor.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66aee0c-b2fb-43e0-9469-0fc33be14e14",
   "metadata": {},
   "source": [
    "# 行业市值中性化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ec5e65c9-45f8-4c2c-9263-50845c03ec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_condition(new_stk=20):\n",
    "    drop_new_stk = adj_open.rolling(window=new_stk, min_periods=new_stk).mean()\n",
    "    drop_new_stk = ~np.isnan(drop_new_stk)\n",
    "    df = st_status * halt_status * drop_new_stk\n",
    "    df = df.replace(True, 1).replace(False, np.nan)\n",
    "    return df.loc['2017':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "302c0d68-88d3-450f-a509-7098b6bef51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>logcmv</th>\n",
       "      <th>retsum120</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>50</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8143977</th>\n",
       "      <td>2024-12-27</td>\n",
       "      <td>689009</td>\n",
       "      <td>21.662996</td>\n",
       "      <td>0.284347</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date  ticker     logcmv  retsum120   10   11   12   20   21  \\\n",
       "8143977 2024-12-27  689009  21.662996   0.284347  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "          22  ...   41   42   43   50   60   61   62   63   70  999  \n",
       "8143977  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[1 rows x 35 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "dc7a50f4-6da3-4488-976a-ffa1f750ae35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 35.48it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 40.91it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 43.13it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 39.21it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 44.70it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 47.04it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 45.15it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 45.45it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 40.25it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 47.38it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 47.76it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 41.28it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 44.72it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 43.68it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 48.33it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 47.07it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 46.97it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 47.22it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 44.45it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_factor(factor_path, output_path, final_universe, risk, risk_cols):\n",
    "    df = pd.read_pickle(factor_path)\n",
    "    df = df.tail(20)\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df_clean = df * final_universe\n",
    "    df_clean = df_clean.dropna(how='all', axis=0)\n",
    "    \n",
    "    factor_type = 'ind'\n",
    "    factor_rank = factor_neutralize(df_clean, factor_type, if_rank=True, risk=risk, risk_cols=risk_cols)\n",
    "    factor_rank.to_pickle(output_path)\n",
    "\n",
    "final_universe = compute_condition(new_stk = 10) # Define your final_universe DataFrame\n",
    "risk = risk  # Define your risk DataFrame\n",
    "risk_cols = ['logcmv', 'retsum120']\n",
    "\n",
    "# List of factors and their corresponding paths\n",
    "factors = [\n",
    "    # ('D:/redata/factor/hpl_r_std_6m@factor.pkl','D:/redata/Neutraliza/hpl_r_std_6m@nu.pkl'),\n",
    "    # ('D:/redata/factor/bias_5@factor.pkl','D:/redata/Neutraliza/bias_5@nu.pkl'),\n",
    "    # ('D:/redata/factor/net_profit_Q_yoy@factor.pkl','D:/redata/Neutraliza/net_profit_Q_yoy@nu.pkl'),\n",
    "    # ('D:/redata/factor/qfa_ocftosales@factor.pkl','D:/redata/Neutraliza/qfa_ocftosales@nu.pkl'),\n",
    "    # ('D:/redata/factor/mmt_avg_M@factor.pkl','D:/redata/Neutraliza/mmt_avg_M@nu.pkl'),\n",
    "\n",
    "    # 组合中 #\n",
    "    # ('D:/redata/factor/operating_revenue_Q_yoy@factor.pkl','D:/redata/Neutraliza/operating_revenue_Q_yoy@nu.pkl'),\n",
    "    ('D:/redata/factor/roe_yoy@factor.pkl','D:/redata/Neutraliza/roe_yoy@nu.pkl'),\n",
    "    ('D:/redata/factor/roa@factor.pkl','D:/redata/Neutraliza/roa@nu.pkl'),\n",
    "    # ('D:/redata/factor/roe@factor.pkl','D:/redata/Neutraliza/roe@nu.pkl'),\n",
    "    ('D:/redata/factor/roa_yoy@factor.pkl','D:/redata/Neutraliza/roa_yoy@nu.pkl'),\n",
    "    \n",
    "    ('D:/redata/factor/net_profit_parent_company_Q_yoy@factor.pkl','D:/redata/Neutraliza/net_profit_parent_company_Q_yoy@nu.pkl'),\n",
    "    ('D:/redata/factor/cfp_ttm@factor.pkl','D:/redata/Neutraliza/cfp_ttm@nu.pkl'),\n",
    "    ('D:/redata/factor/sp_ttm@factor.pkl','D:/redata/Neutraliza/sp_ttm@nu.pkl'),\n",
    "    \n",
    "    ('D:/redata/factor/EP_ttm@factor.pkl','D:/redata/Neutraliza/EP_ttm@nu.pkl'),\n",
    "    # ('D:/redata/factor/EP@factor.pkl','D:/redata/Neutraliza/EP@nu.pkl'),\n",
    "    # ('D:/redata/factor/BP@factor.pkl','D:/redata/Neutraliza/BP@nu.pkl'),\n",
    "    \n",
    "    ('D:/redata/factor/adjclose@factor.pkl','D:/redata/Neutraliza/adjclose@nu.pkl'),\n",
    "    \n",
    "    ('D:/redata/factor/maketcap@factor.pkl','D:/redata/Neutraliza/maketcap@nu.pkl'),\n",
    "    \n",
    "    ('D:/redata/factor/hml_r_std_6m@factor.pkl','D:/redata/Neutraliza/hml_r_std_6m@nu.pkl'),\n",
    "    \n",
    "    # ('D:/redata/factor/terra_20d@factor.pkl','D:/redata/Neutraliza/terra_20d@nu.pkl'),\n",
    "    \n",
    "    # ('D:/redata/factor/mmt_overnight_A@factor.pkl','D:/redata/Neutraliza/mmt_overnight_A@nu.pkl'),\n",
    "    ('D:/redata/factor/mom_overnight_A@factor.pkl','D:/redata/Neutraliza/mom_overnight_A@nu.pkl'),\n",
    "    \n",
    "    # ('D:/redata/factor/mmt_off_limit_A@factor.pkl','D:/redata/Neutraliza/mmt_off_limit_A@nu.pkl'),\n",
    "    # ('D:/redata/factor/mmt_normal_A@factor.pkl','D:/redata/Neutraliza/mmt_normal_A@nu.pkl'),\n",
    "    ('D:/redata/factor/mom_A@factor.pkl','D:/redata/Neutraliza/mom_A@nu.pkl'),\n",
    "    #&&&&&&&\n",
    "    # ('D:/redata/factor/mmt_off_limit_M@factor.pkl','D:/redata/Neutraliza/mmt_off_limit_M@nu.pkl'),\n",
    "    # ('D:/redata/factor/mmt_normal_M@factor.pkl','D:/redata/Neutraliza/mmt_normal_M@nu.pkl'),\n",
    "    ('D:/redata/factor/mom_M@factor.pkl','D:/redata/Neutraliza/mom_M@nu.pkl'),\n",
    "    \n",
    "    # ('D:/redata/factor/mmt_intraday_M@factor.pkl', 'D:/redata/Neutraliza/mmt_intraday_M@nu.pkl'),\n",
    "    # ('D:/redata/factor/mmt_range_M@factor.pkl','D:/redata/Neutraliza/mmt_range_M@nu.pkl'),\n",
    "    ('D:/redata/factor/reverse_M@factor.pkl','D:/redata/Neutraliza/reverse_M@nu.pkl'),\n",
    "    \n",
    "    # ('D:/redata/factor/vol_up_std_6M@factor.pkl','D:/redata/Neutraliza/vol_up_std_6M@nu.pkl'),\n",
    "    # ('D:/redata/factor/vol_highlow_std_6M@factor.pkl','D:/redata/Neutraliza/vol_highlow_std_6M@nu.pkl'),\n",
    "    # ('D:/redata/factor/vol_upshadow_std_6M@factor.pkl','D:/redata/Neutraliza/vol_upshadow_std_6M@nu.pkl'),\n",
    "    # ('D:/redata/factor/vol_w_downshadow_std_6M@factor.pkl','D:/redata/Neutraliza/vol_w_downshadow_std_6M@nu.pkl'),\n",
    "    ('D:/redata/factor/vol_6M@factor.pkl','D:/redata/Neutraliza/vol_6M@nu.pkl'),\n",
    "    \n",
    "    # ('D:/redata/factor/liq_turn_std_6M@factor.pkl','D:/redata/Neutraliza/liq_turn_std_6M@nu.pkl'),\n",
    "    ('D:/redata/factor/turnrate_6M@factor.pkl','D:/redata/Neutraliza/turnrate_6M@nu.pkl'),\n",
    "    \n",
    "    # ('D:/redata/factor/liq_vstd_1M@factor.pkl','D:/redata/Neutraliza/liq_vstd_1M@nu.pkl'),\n",
    "    # ('D:/redata/factor/liq_amiihud_avg_1M@factor.pkl','D:/redata/Neutraliza/liq_amiihud_avg_1M@nu.pkl'),\n",
    "    # ('D:/redata/factor/liq_shortcut_avg_1M@factor.pkl','D:/redata/Neutraliza/liq_shortcut_avg_1M@nu.pkl'),\n",
    "    ('D:/redata/factor/ill_price_M@factor.pkl','D:/redata/Neutraliza/ill_price_M@nu.pkl'),\n",
    "    \n",
    "    # ('D:/redata/factor/corr_ret_turnd_1M@factor.pkl','D:/redata/Neutraliza/corr_ret_turnd_1M@nu.pkl'),\n",
    "    # ('D:/redata/factor/corr_price_turn_1M@factor.pkl','D:/redata/Neutraliza/corr_price_turn_1M@nu.pkl'),\n",
    "    # ('D:/redata/factor/corr_ret_turn_post_1M@factor.pkl','D:/redata/Neutraliza/corr_ret_turn_post_1M@nu.pkl'),\n",
    "    ('D:/redata/factor/corrPV_M@factor.pkl','D:/redata/Neutraliza/corrPV_M@nu.pkl'),\n",
    "\n",
    "    ('D:/redata/factor/volume_20d_fac@factor.pkl','D:/redata/Neutraliza/volume_20d_fac@nu.pkl'),\n",
    "    # ('D:/redata/factor/volume_60d_ratio@factor.pkl','D:/redata/Neutraliza/volume_60d_ratio@nu.pkl'),\n",
    "   \n",
    "    # 组合中 #\n",
    "    ]\n",
    "    \n",
    "# Process each factor\n",
    "for factor_path, output_path in factors:\n",
    "    process_factor(factor_path, output_path, final_universe, risk, risk_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "27350260-390e-4024-bf43-113c9f069e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_IC(factor_rank, ret_rank):\n",
    "    IC = (factor_rank.corrwith(ret_rank, axis=1)).dropna()\n",
    "    IC_mean = IC.mean()\n",
    "    IC_std = IC.std()\n",
    "    ICIR = IC_mean / IC_std\n",
    "    ICres = pd.DataFrame([[IC_mean, ICIR]], columns=['ICmean', 'ICIR'])\n",
    "    ICIR_120_weight = IC.rolling(120).apply(lambda x: x.mean() / x.std()).dropna()\n",
    "    return IC, ICres, ICIR_120_weight\n",
    "\n",
    "# Assuming factor_rank and ret_rank are defined elsewhere in your code\n",
    "# factor_rank = ... # Define your factor_rank DataFrame\n",
    "ret_rank = o1o2.rank(axis=1, ascending=True) #1日调仓\n",
    "ret_rank = ret_rank.ffill()\n",
    "\n",
    "factors_nu = [\n",
    "    ('D:/redata/Neutraliza/bias_5@nu.pkl','D:/redata/ICIR/bias_5@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/net_profit_Q_yoy@nu.pkl','D:/redata/ICIR/net_profit_Q_yoy@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/operating_revenue_Q_yoy@nu.pkl','D:/redata/ICIR/operating_revenue_Q_yoy@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/qfa_ocftosales@nu.pkl','D:/redata/ICIR/qfa_ocftosales@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/BP@nu.pkl','D:/redata/ICIR/BP@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/net_profit_parent_company_Q_yoy@nu.pkl','D:/redata/ICIR/net_profit_parent_company_Q_yoy@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/roe_yoy@nu.pkl','D:/redata/ICIR/roe_yoy@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/roa@nu.pkl','D:/redata/ICIR/roa@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/terra_20d@nu.pkl','D:/redata/ICIR/terra_20d@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/hml_r_std_6m@nu.pkl','D:/redata/ICIR/hml_r_std_6m@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/EP_ttm@nu.pkl','D:/redata/ICIR/EP_ttm@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/EP@nu.pkl','D:/redata/ICIR/EP@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/maketcap@nu.pkl','D:/redata/ICIR/maketcap@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/adjclose@nu.pkl','D:/redata/ICIR/adjclose@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/sp_ttm@nu.pkl','D:/redata/ICIR/sp_ttm@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/roe@nu.pkl','D:/redata/ICIR/roe@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/roa_yoy@nu.pkl','D:/redata/ICIR/roa_yoy@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/cfp_ttm@nu.pkl','D:/redata/ICIR/cfp_ttm@ICIR.pkl'),\n",
    "\n",
    "    \n",
    "    # ('D:/redata/Neutraliza/mmt_overnight_A@nu.pkl','D:/redata/ICIR/mmt_overnight_A@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/mom_overnight_A@nu.pkl','D:/redata/ICIR/mom_overnight_A@ICIR.pkl'),\n",
    "    \n",
    "    # ('D:/redata/Neutraliza/mmt_normal_A@nu.pkl','D:/redata/ICIR/mmt_normal_A@ICIR.pkl'),\n",
    "    # ('D:/redata/Neutraliza/mmt_off_limit_A@nu.pkl','D:/redata/ICIR/mmt_off_limit_A@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/mom_A@nu.pkl','D:/redata/ICIR/mom_A@ICIR.pkl'),\n",
    "\n",
    "    # ('D:/redata/Neutraliza/mmt_normal_M@nu.pkl','D:/redata/ICIR/mmt_normal_M@ICIR.pkl'),\n",
    "    # ('D:/redata/Neutraliza/mmt_off_limit_M@nu.pkl','D:/redata/ICIR/mmt_off_limit_M@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/mom_M@nu.pkl','D:/redata/ICIR/mom_M@ICIR.pkl'),\n",
    "    \n",
    "    # ('D:/redata/Neutraliza/mmt_intraday_M@nu.pkl', 'D:/redata/ICIR/mmt_intraday_M@ICIR.pkl'),\n",
    "    # ('D:/redata/Neutraliza/mmt_range_M@nu.pkl','D:/redata/ICIR/mmt_range_M@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/reverse_M@nu.pkl','D:/redata/ICIR/reverse_M@ICIR.pkl'),\n",
    "    \n",
    "    # ('D:/redata/Neutraliza/vol_up_std_6M@nu.pkl','D:/redata/ICIR/vol_up_std_6M@ICIR.pkl'),\n",
    "    # ('D:/redata/Neutraliza/vol_highlow_std_6M@nu.pkl','D:/redata/ICIR/vol_highlow_std_6M@ICIR.pkl'),\n",
    "    # ('D:/redata/Neutraliza/vol_upshadow_std_6M@nu.pkl','D:/redata/ICIR/vol_upshadow_std_6M@ICIR.pkl'),\n",
    "    # ('D:/redata/Neutraliza/vol_w_downshadow_std_6M@nu.pkl','D:/redata/ICIR/vol_w_downshadow_std_6M@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/vol_6M@nu.pkl','D:/redata/ICIR/vol_6M@ICIR.pkl'),\n",
    "    \n",
    "    # ('D:/redata/Neutraliza/liq_turn_std_6M@nu.pkl','D:/redata/ICIR/liq_turn_std_6M@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/turnrate_6M@nu.pkl','D:/redata/ICIR/turnrate_6M@ICIR.pkl'),\n",
    "    \n",
    "    # ('D:/redata/Neutraliza/liq_vstd_1M@nu.pkl','D:/redata/ICIR/liq_vstd_1M@ICIR.pkl'),\n",
    "    # ('D:/redata/Neutraliza/liq_amiihud_avg_1M@nu.pkl','D:/redata/ICIR/liq_amiihud_avg_1M@ICIR.pkl'),\n",
    "    # ('D:/redata/Neutraliza/liq_shortcut_avg_1M@nu.pkl','D:/redata/ICIR/liq_shortcut_avg_1M@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/ill_price_M@nu.pkl','D:/redata/ICIR/ill_price_M@ICIR.pkl'),\n",
    "    \n",
    "    # ('D:/redata/Neutraliza/corr_ret_turnd_1M@nu.pkl','D:/redata/ICIR/corr_ret_turnd_1M@ICIR.pkl'),\n",
    "    # ('D:/redata/Neutraliza/corr_price_turn_1M@nu.pkl','D:/redata/ICIR/corr_price_turn_1M@ICIR.pkl'),\n",
    "    # ('D:/redata/Neutraliza/corr_ret_turn_post_1M@nu.pkl','D:/redata/ICIR/corr_ret_turn_post_1M@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/corrPV_M@nu.pkl','D:/redata/ICIR/corrPV_M@ICIR.pkl'),\n",
    "\n",
    "    ('D:/redata/Neutraliza/volume_20d_fac@nu.pkl','D:/redata/ICIR/volume_20d_fac@ICIR.pkl'),\n",
    "    ('D:/redata/Neutraliza/volume_60d_ratio@nu.pkl','D:/redata/ICIR/volume_60d_ratio@ICIR.pkl'),\n",
    "]\n",
    "\n",
    "# Process each factor\n",
    "for factor_path, output_path in factors_nu:\n",
    "    factor_rank = pd.read_pickle(factor_path)\n",
    "# Assuming ret_rank is defined and available\n",
    "    IC, ICres, ICIR_120_weight = compute_IC(factor_rank, ret_rank)\n",
    "# Optionally save ICIR_120_weight if needed\n",
    "    ICIR_120_weight.to_pickle(output_path.replace('.pkl', '_120_weight.pkl'))\n",
    "    ICres.to_pickle(output_path)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11647502-a5a6-4bc8-9399-fe6cde93b771",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of file paths\n",
    "f_files = [\n",
    "    'D:/redata/Neutraliza/net_profit_parent_company_Q_yoy@nu.pkl',\n",
    "    'D:/redata/Neutraliza/BP@nu.pkl',\n",
    "    'D:/redata/Neutraliza/roe_yoy@nu.pkl',\n",
    "    'D:/redata/Neutraliza/roa@nu.pkl',\n",
    "    'D:/redata/Neutraliza/terra_20d@nu.pkl',\n",
    "    'D:/redata/Neutraliza/hml_r_std_6m@nu.pkl',\n",
    "    'D:/redata/Neutraliza/EP_ttm@nu.pkl',\n",
    "    'D:/redata/Neutraliza/sp_ttm@nu.pkl',\n",
    "    'D:/redata/Neutraliza/roe@nu.pkl',\n",
    "    'D:/redata/Neutraliza/roa_yoy@nu.pkl',\n",
    "    'D:/redata/Neutraliza/EP@nu.pkl',\n",
    "    'D:/redata/Neutraliza/adjclose@nu.pkl',\n",
    "    'D:/redata/Neutraliza/maketcap@nu.pkl',\n",
    "    'D:/redata/Neutraliza/cfp_ttm@nu.pkl',\n",
    "    'D:/redata/Neutraliza/bias_5@nu.pkl',\n",
    "    'D:/redata/Neutraliza/net_profit_Q_yoy@nu.pkl',\n",
    "    'D:/redata/Neutraliza/operating_revenue_Q_yoy@nu.pkl',\n",
    "    'D:/redata/Neutraliza/qfa_ocftosales@nu.pkl',\n",
    "\n",
    "    \n",
    "    # 'D:/redata/Neutraliza/mmt_overnight_A@nu.pkl',\n",
    "    'D:/redata/Neutraliza/mom_overnight_A@nu.pkl',\n",
    "    \n",
    "    # 'D:/redata/Neutraliza/mmt_intraday_M@nu.pkl',\n",
    "    # 'D:/redata/Neutraliza/mmt_range_M@nu.pkl',\n",
    "    'D:/redata/Neutraliza/reverse_M@nu.pkl',\n",
    "    \n",
    "    # 'D:/redata/Neutraliza/mmt_normal_A@nu.pkl',\n",
    "    # 'D:/redata/Neutraliza/mmt_off_limit_A@nu.pkl',\n",
    "    'D:/redata/Neutraliza/mom_A@nu.pkl',\n",
    "\n",
    "    # 'D:/redata/Neutraliza/mmt_normal_M@nu.pkl',\n",
    "    # 'D:/redata/Neutraliza/mmt_off_limit_M@nu.pkl',\n",
    "    'D:/redata/Neutraliza/mom_M@nu.pkl',\n",
    "    \n",
    "    # 'D:/redata/Neutraliza/vol_up_std_6M@nu.pkl',\n",
    "    # 'D:/redata/Neutraliza/vol_highlow_std_6M@nu.pkl',\n",
    "    # 'D:/redata/Neutraliza/vol_upshadow_std_6M@nu.pkl',\n",
    "    # 'D:/redata/Neutraliza/vol_w_downshadow_std_6M@nu.pkl',\n",
    "    'D:/redata/Neutraliza/vol_6M@nu.pkl',\n",
    "    \n",
    "    # 'D:/redata/Neutraliza/liq_turn_std_6M@nu.pkl',\n",
    "    'D:/redata/Neutraliza/turnrate_6M@nu.pkl',\n",
    "    \n",
    "    # 'D:/redata/Neutraliza/liq_vstd_1M@nu.pkl',\n",
    "    # 'D:/redata/Neutraliza/liq_amiihud_avg_1M@nu.pkl',\n",
    "    # 'D:/redata/Neutraliza/liq_shortcut_avg_1M@nu.pkl',\n",
    "    'D:/redata/Neutraliza/ill_price_M@nu.pkl',\n",
    "    \n",
    "    # 'D:/redata/Neutraliza/corr_ret_turnd_1M@nu.pkl',\n",
    "    # 'D:/redata/Neutraliza/corr_price_turn_1M@nu.pkl',\n",
    "    # 'D:/redata/Neutraliza/corr_ret_turn_post_1M@nu.pkl',\n",
    "    'D:/redata/Neutraliza/corrPV_M@nu.pkl',\n",
    "\n",
    "    'D:/redata/Neutraliza/volume_20d_fac@nu.pkl',\n",
    "    'D:/redata/Neutraliza/volume_60d_ratio@nu.pkl',\n",
    "]\n",
    "\n",
    "i_files = [\n",
    "    'D:/redata/ICIR/net_profit_parent_company_Q_yoy@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/BP@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/roe_yoy@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/roa@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/terra_20d@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/hml_r_std_6m@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/EP_ttm@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/sp_ttm@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/roe@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/roa_yoy@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/EP@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/adjclose@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/maketcap@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/cfp_ttm@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/bias_5@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/net_profit_Q_yoy@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/operating_revenue_Q_yoy@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/qfa_ocftosales@ICIR_120_weight.pkl',\n",
    "\n",
    "    # 'D:/redata/ICIR/mmt_overnight_A@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/mom_overnight_A@ICIR_120_weight.pkl',\n",
    "    \n",
    "    # 'D:/redata/ICIR/mmt_intraday_M@ICIR_120_weight.pkl',\n",
    "    # 'D:/redata/ICIR/mmt_range_M@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/reverse_M@ICIR_120_weight.pkl',\n",
    "    \n",
    "    # 'D:/redata/ICIR/mmt_normal_A@ICIR_120_weight.pkl',\n",
    "    # 'D:/redata/ICIR/mmt_off_limit_A@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/mom_A@ICIR_120_weight.pkl',\n",
    "\n",
    "    # 'D:/redata/ICIR/mmt_normal_M@ICIR_120_weight.pkl',\n",
    "    # 'D:/redata/ICIR/mmt_off_limit_M@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/mom_M@ICIR_120_weight.pkl',\n",
    "    \n",
    "    # 'D:/redata/ICIR/vol_up_std_6M@ICIR_120_weight.pkl',\n",
    "    # 'D:/redata/ICIR/vol_highlow_std_6M@ICIR_120_weight.pkl',\n",
    "    # 'D:/redata/ICIR/vol_upshadow_std_6M@ICIR_120_weight.pkl',\n",
    "    # 'D:/redata/ICIR/vol_w_downshadow_std_6M@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/vol_6M@ICIR_120_weight.pkl',\n",
    "    \n",
    "    # 'D:/redata/ICIR/liq_turn_std_6M@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/turnrate_6M@ICIR_120_weight.pkl',\n",
    "    \n",
    "    # 'D:/redata/ICIR/liq_vstd_1M@ICIR_120_weight.pkl',\n",
    "    # 'D:/redata/ICIR/liq_amiihud_avg_1M@ICIR_120_weight.pkl',\n",
    "    # 'D:/redata/ICIR/liq_shortcut_avg_1M@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/ill_price_M@ICIR_120_weight.pkl',\n",
    "    \n",
    "    # 'D:/redata/ICIR/corr_ret_turnd_1M@ICIR_120_weight.pkl',\n",
    "    # 'D:/redata/ICIR/corr_price_turn_1M@ICIR_120_weight.pkl',\n",
    "    # 'D:/redata/ICIR/corr_ret_turn_post_1M@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/corrPV_M@ICIR_120_weight.pkl',\n",
    "\n",
    "    'D:/redata/ICIR/volume_20d_fac@ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/volume_60d_ratio@ICIR_120_weight.pkl',\n",
    "]\n",
    "\n",
    "o_files = [\n",
    "    'D:/redata/CombineFactors/net_profit_parent_company_Q_yoy@combine.pkl',\n",
    "    'D:/redata/CombineFactors/BP@combine.pkl',\n",
    "    'D:/redata/CombineFactors/roe_yoy@combine.pkl',\n",
    "    'D:/redata/CombineFactors/roa@combine.pkl',  \n",
    "    'D:/redata/CombineFactors/terra_20d@combine.pkl',\n",
    "    'D:/redata/CombineFactors/hml_r_std_6m@combine.pkl',\n",
    "    'D:/redata/CombineFactors/EP_ttm@combine.pkl',\n",
    "    'D:/redata/CombineFactors/sp_ttm@combine.pkl',\n",
    "    'D:/redata/CombineFactors/roe@combine.pkl',\n",
    "    'D:/redata/CombineFactors/roa_yoy@combine.pkl',\n",
    "    'D:/redata/CombineFactors/EP@combine.pkl',\n",
    "    'D:/redata/CombineFactors/adjclose@combine.pkl',\n",
    "    'D:/redata/CombineFactors/maketcap@combine.pkl',\n",
    "    'D:/redata/CombineFactors/cfp_ttm@combine.pkl',\n",
    "    'D:/redata/CombineFactors/bias_5@combine.pkl',\n",
    "    'D:/redata/CombineFactors/net_profit_Q_yoy@combine.pkl',\n",
    "    'D:/redata/CombineFactors/operating_revenue_Q_yoy@combine.pkl',\n",
    "    'D:/redata/CombineFactors/qfa_ocftosales@combine.pkl',\n",
    "    \n",
    "    # 'D:/redata/CombineFactors/mmt_overnight_A@combine.pkl',\n",
    "    'D:/redata/CombineFactors/mom_overnight_A@combine.pkl',\n",
    "    \n",
    "    # 'D:/redata/CombineFactors/mmt_intraday_M@combine.pkl',\n",
    "    # 'D:/redata/CombineFactors/mmt_range_M@combine.pkl',\n",
    "    'D:/redata/CombineFactors/reverse_M@combine.pkl',\n",
    "    \n",
    "    # 'D:/redata/CombineFactors/mmt_off_limit_A@combine.pkl',\n",
    "    # 'D:/redata/CombineFactors/mmt_normal_A@combine.pkl',\n",
    "    'D:/redata/CombineFactors/mom_A@combine.pkl',\n",
    "\n",
    "    # 'D:/redata/CombineFactors/mmt_off_limit_M@combine.pkl',\n",
    "    # 'D:/redata/CombineFactors/mmt_normal_M@combine.pkl',\n",
    "    'D:/redata/CombineFactors/mom_M@combine.pkl',\n",
    "    \n",
    "    \n",
    "    # 'D:/redata/CombineFactors/vol_up_std_6M@combine.pkl',\n",
    "    # 'D:/redata/CombineFactors/vol_highlow_std_6M@combine.pkl',\n",
    "    # 'D:/redata/CombineFactors/vol_upshadow_std_6M@combine.pkl',\n",
    "    # 'D:/redata/CombineFactors/vol_w_downshadow_std_6M@combine.pkl',\n",
    "    'D:/redata/CombineFactors/vol_6M@combine.pkl',\n",
    "    \n",
    "    # 'D:/redata/CombineFactors/liq_turn_std_6M@combine.pkl',\n",
    "    'D:/redata/CombineFactors/turnrate_6M@combine.pkl',\n",
    "    \n",
    "    # 'D:/redata/CombineFactors/liq_vstd_1M@combine.pkl',\n",
    "    # 'D:/redata/CombineFactors/liq_amiihud_avg_1M@combine.pkl',\n",
    "    # 'D:/redata/CombineFactors/liq_shortcut_avg_1M@combine.pkl',\n",
    "    'D:/redata/CombineFactors/ill_price_M@combine.pkl',\n",
    "    \n",
    "    # 'D:/redata/CombineFactors/corr_ret_turnd_1M@combine.pkl',\n",
    "    # 'D:/redata/CombineFactors/corr_price_turn_1M@combine.pkl',\n",
    "    # 'D:/redata/CombineFactors/corr_ret_turn_post_1M@combine.pkl',\n",
    "    'D:/redata/CombineFactors/corrPV_M@combine.pkl',\n",
    "\n",
    "    'D:/redata/CombineFactors/volume_20d_fac@combine.pkl',\n",
    "    'D:/redata/CombineFactors/volume_60d_ratio@combine.pkl',\n",
    "]\n",
    "\n",
    "# Read f files\n",
    "f_dataframes = [pd.read_pickle(file) for file in f_files]\n",
    "\n",
    "# Read i files and convert to DataFrame\n",
    "i_dataframes = []\n",
    "for file in i_files:\n",
    "    df = pd.read_pickle(file)\n",
    "    df = pd.DataFrame(df)\n",
    "    df.columns = ['ICIR']\n",
    "    i_dataframes.append(df)\n",
    "\n",
    "# Perform multiplication and save results\n",
    "results = []\n",
    "for f_df, i_df, o_file in zip(f_dataframes, i_dataframes, o_files):\n",
    "    result = f_df.mul(i_df['ICIR'], axis=0)\n",
    "    result.to_pickle(o_file)\n",
    "    results.append(result)\n",
    "\n",
    "# Now results list contains all multiplication operation后的 DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927892c6-6bbc-4da7-b040-ec771d469ac4",
   "metadata": {},
   "source": [
    "# 相关性检验"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11e6dede-a923-4553-8099-d5d6b63b6d67",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "i2 = pd.read_pickle('D:/redata/ICIR/roe@ICIR_120_weight.pkl')\n",
    "i3 = pd.read_pickle('D:/redata/ICIR/roe_yoy@ICIR_120_weight.pkl')\n",
    "i4 = pd.read_pickle('D:/redata/ICIR/roa@ICIR_120_weight.pkl')\n",
    "i5 = pd.read_pickle('D:/redata/ICIR/hml_r_std_6m@ICIR_120_weight.pkl')\n",
    "i6 = pd.read_pickle('D:/redata/ICIR/net_profit_Q_yoy@ICIR_120_weight.pkl')\n",
    "i7 = pd.read_pickle('D:/redata/ICIR/net_profit_parent_company_Q_yoy@ICIR_120_weight.pkl')\n",
    "i8 = pd.read_pickle('D:/redata/ICIR/operating_revenue_Q_yoy@ICIR_120_weight.pkl')\n",
    "i9 = pd.read_pickle('D:/redata/ICIR/BP@ICIR_120_weight.pkl')\n",
    "i10 = pd.read_pickle('D:/redata/ICIR/bias_5@ICIR_120_weight.pkl')\n",
    "i11 = pd.read_pickle('D:/redata/ICIR/qfa_ocftosales@ICIR_120_weight.pkl')\n",
    "i12 = pd.read_pickle('D:/redata/ICIR/terra_20d@ICIR_120_weight.pkl')\n",
    "i14 = pd.read_pickle('D:/redata/ICIR/EP_ttm@ICIR_120_weight.pkl')\n",
    "i15 = pd.read_pickle('D:/redata/ICIR/sp_ttm@ICIR_120_weight.pkl')\n",
    "i16 = pd.read_pickle('D:/redata/ICIR/roa_yoy@ICIR_120_weight.pkl')\n",
    "i17 = pd.read_pickle('D:/redata/ICIR/EP@ICIR_120_weight.pkl')\n",
    "i18 = pd.read_pickle('D:/redata/ICIR/adjclose@ICIR_120_weight.pkl')\n",
    "i19 = pd.read_pickle('D:/redata/ICIR/maketcap@ICIR_120_weight.pkl')\n",
    "i20 = pd.read_pickle('D:/redata/ICIR/cfp_ttm@ICIR_120_weight.pkl')\n",
    "\n",
    "# i21 = pd.read_pickle('D:/redata/ICIR/mmt_overnight_A@ICIR_120_weight.pkl')\n",
    "i21 = pd.read_pickle('D:/redata/ICIR/mom_overnight_A@ICIR_120_weight.pkl')\n",
    "\n",
    "# i22 = pd.read_pickle('D:/redata/ICIR/mmt_normal_A@ICIR_120_weight.pkl')\n",
    "# i23 = pd.read_pickle('D:/redata/ICIR/mmt_off_limit_A@ICIR_120_weight.pkl')\n",
    "i22 = pd.read_pickle('D:/redata/ICIR/mom_A@ICIR_120_weight.pkl')\n",
    "\n",
    "# i24 = pd.read_pickle('D:/redata/ICIR/mmt_intraday_M@ICIR_120_weight.pkl')\n",
    "# i25 = pd.read_pickle('D:/redata/ICIR/mmt_range_M@ICIR_120_weight.pkl')\n",
    "i23 = pd.read_pickle('D:/redata/ICIR/reverse_M@ICIR_120_weight.pkl')\n",
    "\n",
    "# i26 = pd.read_pickle('D:/redata/ICIR/vol_up_std_6M@ICIR_120_weight.pkl')\n",
    "# i27 = pd.read_pickle('D:/redata/ICIR/vol_highlow_std_6M@ICIR_120_weight.pkl')\n",
    "# i28 = pd.read_pickle('D:/redata/ICIR/vol_upshadow_std_6M@ICIR_120_weight.pkl')\n",
    "# i29 = pd.read_pickle('D:/redata/ICIR/vol_w_downshadow_std_6M@ICIR_120_weight.pkl')\n",
    "i24 = pd.read_pickle('D:/redata/ICIR/vol_6M@ICIR_120_weight.pkl')\n",
    "\n",
    "# i30 = pd.read_pickle('D:/redata/ICIR/liq_turn_std_6M@ICIR_120_weight.pkl')\n",
    "i25 = pd.read_pickle('D:/redata/ICIR/turnrate_6M@ICIR_120_weight.pkl')\n",
    "\n",
    "# i31 = pd.read_pickle('D:/redata/ICIR/liq_vstd_1M@ICIR_120_weight.pkl')\n",
    "# i32 = pd.read_pickle('D:/redata/ICIR/liq_amiihud_avg_1M@ICIR_120_weight.pkl')\n",
    "# i33 = pd.read_pickle('D:/redata/ICIR/liq_shortcut_avg_1M@ICIR_120_weight.pkl')\n",
    "i26 = pd.read_pickle('D:/redata/ICIR/ill_price_M@ICIR_120_weight.pkl')\n",
    "\n",
    "# i34 = pd.read_pickle('D:/redata/ICIR/corr_ret_turnd_1M@ICIR_120_weight.pkl')\n",
    "# i35 = pd.read_pickle('D:/redata/ICIR/corr_price_turn_1M@ICIR_120_weight.pkl')\n",
    "# i36 = pd.read_pickle('D:/redata/ICIR/corr_ret_turn_post_1M@ICIR_120_weight.pkl')\n",
    "i27 = pd.read_pickle('D:/redata/ICIR/corrPV_M@ICIR_120_weight.pkl')\n",
    "\n",
    "# i37 = pd.read_pickle('D:/redata/ICIR/mmt_normal_M@ICIR_120_weight.pkl')\n",
    "# i38 = pd.read_pickle('D:/redata/ICIR/mmt_off_limit_M@ICIR_120_weight.pkl')\n",
    "i28 = pd.read_pickle('D:/redata/ICIR/mom_M@ICIR_120_weight.pkl')\n",
    "\n",
    "i29 = pd.read_pickle('D:/redata/ICIR/volume_20d_fac@ICIR_120_weight.pkl')\n",
    "i30 = pd.read_pickle('D:/redata/ICIR/volume_60d_ratio@ICIR_120_weight.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030d242a-c40e-419a-90d2-5696492e38e3",
   "metadata": {},
   "source": [
    "# 选股"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90747d96-93fc-4789-8f12-873888f7fb6f",
   "metadata": {},
   "source": [
    "cor_df = pd.concat([i3,i5,i7,i14,i15,i18,i19,i21,i22,i23,i24,i25,i26,i28,i29], axis=1) # ,i21,i22,i23,i24,i25,i26,i28,i29\n",
    "cor_df = cor_df.dropna(axis=0)\n",
    "\n",
    "correlation_matrix = (cor_df.corr()).dropna()\n",
    "sns.heatmap(correlation_matrix, annot=True,cmap='coolwarm',vmin=-1,vmax=1)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e897cfb0-5201-4c6c-8440-78207d138350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 有ICIR加权时，权重有延迟。会导致选股失败。\n",
    "# f2 = pd.read_pickle('D:/redata/CombineFactors/roe@combine.pkl')\n",
    "# f3 = pd.read_pickle('D:/redata/CombineFactors/roe_yoy@combine.pkl')\n",
    "# f4 = pd.read_pickle('D:/redata/CombineFactors/roa@combine.pkl')\n",
    "# f5 = pd.read_pickle('D:/redata/CombineFactors/hml_r_std_6m@combine.pkl')\n",
    "# f6 = pd.read_pickle('D:/redata/CombineFactors/net_profit_Q_yoy@combine.pkl')\n",
    "# f7 = pd.read_pickle('D:/redata/CombineFactors/net_profit_parent_company_Q_yoy@combine.pkl')\n",
    "# f8 = pd.read_pickle('D:/redata/CombineFactors/operating_revenue_Q_yoy@combine.pkl')\n",
    "# f9 = pd.read_pickle('D:/redata/CombineFactors/BP@combine.pkl')\n",
    "# f10 = pd.read_pickle('D:/redata/CombineFactors/bias_5@combine.pkl')\n",
    "# f11 = pd.read_pickle('D:/redata/CombineFactors/qfa_ocftosales@combine.pkl')\n",
    "# f12 = pd.read_pickle('D:/redata/CombineFactors/terra_20d@combine.pkl')\n",
    "# f14 = pd.read_pickle('D:/redata/CombineFactors/EP_ttm@combine.pkl')\n",
    "# f15 = pd.read_pickle('D:/redata/CombineFactors/sp_ttm@combine.pkl')\n",
    "# f16 = pd.read_pickle('D:/redata/CombineFactors/roa_yoy@combine.pkl')\n",
    "# f17 = pd.read_pickle('D:/redata/CombineFactors/EP@combine.pkl')\n",
    "# f18 = pd.read_pickle('D:/redata/CombineFactors/adjclose@combine.pkl')\n",
    "# f19 = pd.read_pickle('D:/redata/CombineFactors/maketcap@combine.pkl')\n",
    "# f20 = pd.read_pickle('D:/redata/CombineFactors/cfp_ttm@combine.pkl')\n",
    "\n",
    "# # f21 = pd.read_pickle('D:/redata/CombineFactors/mmt_overnight_A@combine.pkl')\n",
    "# f21 = pd.read_pickle('D:/redata/CombineFactors/mom_overnight_A@combine.pkl')\n",
    "\n",
    "# # f22 = pd.read_pickle('D:/redata/CombineFactors/mmt_intraday_M@combine.pkl')\n",
    "# # f23 = pd.read_pickle('D:/redata/CombineFactors/mmt_range_M@combine.pkl')  \n",
    "# f22 = pd.read_pickle('D:/redata/CombineFactors/reverse_M@combine.pkl')\n",
    "\n",
    "# # f24 = pd.read_pickle('D:/redata/CombineFactors/mmt_normal_A@combine.pkl')\n",
    "# # f25 = pd.read_pickle('D:/redata/CombineFactors/mmt_off_limit_A@combine.pkl')\n",
    "# f23 = pd.read_pickle('D:/redata/CombineFactors/mom_A@combine.pkl')\n",
    "\n",
    "# # f26 = pd.read_pickle('D:/redata/CombineFactors/vol_up_std_6M@combine.pkl')\n",
    "# # f27 = pd.read_pickle('D:/redata/CombineFactors/vol_highlow_std_6M@combine.pkl')\n",
    "# # f28 = pd.read_pickle('D:/redata/CombineFactors/vol_upshadow_std_6M@combine.pkl')\n",
    "# # f29 = pd.read_pickle('D:/redata/CombineFactors/vol_w_downshadow_std_6M@combine.pkl')\n",
    "# f24 = pd.read_pickle('D:/redata/CombineFactors/vol_6M@combine.pkl')\n",
    "\n",
    "# # f30 = pd.read_pickle('D:/redata/CombineFactors/liq_turn_std_6M@combine.pkl')\n",
    "# f25 = pd.read_pickle('D:/redata/CombineFactors/turnrate_6M@combine.pkl')\n",
    "\n",
    "# # f31 = pd.read_pickle('D:/redata/CombineFactors/liq_vstd_1M@combine.pkl')\n",
    "# # f32 = pd.read_pickle('D:/redata/CombineFactors/liq_amiihud_avg_1M@combine.pkl')\n",
    "# # f33 = pd.read_pickle('D:/redata/CombineFactors/liq_shortcut_avg_1M@combine.pkl')\n",
    "# f26 = pd.read_pickle('D:/redata/CombineFactors/ill_price_M@combine.pkl')\n",
    "\n",
    "# # f34 = pd.read_pickle('D:/redata/CombineFactors/corr_ret_turnd_1M@combine.pkl')\n",
    "# # f35 = pd.read_pickle('D:/redata/CombineFactors/corr_price_turn_1M@combine.pkl')\n",
    "# # f36 = pd.read_pickle('D:/redata/CombineFactors/corr_ret_turn_post_1M@combine.pkl')\n",
    "# f27 = pd.read_pickle('D:/redata/CombineFactors/corrPV_M@combine.pkl')\n",
    "\n",
    "# # f37 = pd.read_pickle('D:/redata/CombineFactors/mmt_normal_M@combine.pkl')\n",
    "# # f38 = pd.read_pickle('D:/redata/CombineFactors/mmt_off_limit_M@combine.pkl')\n",
    "# f28 = pd.read_pickle('D:/redata/CombineFactors/mom_M@combine.pkl')\n",
    "\n",
    "# f39 = pd.read_pickle('D:/redata/Neutraliza/volume_20d_fac@nu.pkl')\n",
    "# f40 = pd.read_pickle('D:/redata/Neutraliza/volume_60d_ratio@nu.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "5c5c0e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = pd.read_pickle('D:/redata/Neutraliza/roe@nu.pkl')\n",
    "f3 = pd.read_pickle('D:/redata/Neutraliza/roe_yoy@nu.pkl')\n",
    "f4 = pd.read_pickle('D:/redata/Neutraliza/roa@nu.pkl')\n",
    "f5 = pd.read_pickle('D:/redata/Neutraliza/hml_r_std_6m@nu.pkl')\n",
    "f6 = pd.read_pickle('D:/redata/Neutraliza/net_profit_Q_yoy@nu.pkl')\n",
    "f7 = pd.read_pickle('D:/redata/Neutraliza/net_profit_parent_company_Q_yoy@nu.pkl')\n",
    "f8 = pd.read_pickle('D:/redata/Neutraliza/operating_revenue_Q_yoy@nu.pkl')\n",
    "f9 = pd.read_pickle('D:/redata/Neutraliza/BP@nu.pkl')\n",
    "f10 = pd.read_pickle('D:/redata/Neutraliza/bias_5@nu.pkl')\n",
    "f11 = pd.read_pickle('D:/redata/Neutraliza/qfa_ocftosales@nu.pkl')\n",
    "f12 = pd.read_pickle('D:/redata/Neutraliza/terra_20d@nu.pkl')\n",
    "f14 = pd.read_pickle('D:/redata/Neutraliza/EP_ttm@nu.pkl')\n",
    "f15 = pd.read_pickle('D:/redata/Neutraliza/sp_ttm@nu.pkl')\n",
    "f16 = pd.read_pickle('D:/redata/Neutraliza/roa_yoy@nu.pkl')\n",
    "f17 = pd.read_pickle('D:/redata/Neutraliza/EP@nu.pkl')\n",
    "f18 = pd.read_pickle('D:/redata/Neutraliza/adjclose@nu.pkl')\n",
    "f19 = pd.read_pickle('D:/redata/Neutraliza/maketcap@nu.pkl')\n",
    "f20 = pd.read_pickle('D:/redata/Neutraliza/cfp_ttm@nu.pkl')\n",
    "\n",
    "# f21 = pd.read_pickle('D:/redata/Neutraliza/mmt_overnight_A@nu.pkl')\n",
    "fh21 = pd.read_pickle('D:/redata/Neutraliza/mom_overnight_A@nu.pkl')\n",
    "\n",
    "# f22 = pd.read_pickle('D:/redata/Neutraliza/mmt_intraday_M@nu.pkl')\n",
    "# f23 = pd.read_pickle('D:/redata/Neutraliza/mmt_range_M@nu.pkl')  \n",
    "fh22 = pd.read_pickle('D:/redata/Neutraliza/reverse_M@nu.pkl')\n",
    "\n",
    "# f24 = pd.read_pickle('D:/redata/Neutraliza/mmt_normal_A@nu.pkl')\n",
    "# f25 = pd.read_pickle('D:/redata/Neutraliza/mmt_off_limit_A@nu.pkl')\n",
    "fh23 = pd.read_pickle('D:/redata/Neutraliza/mom_A@nu.pkl')\n",
    "\n",
    "# f26 = pd.read_pickle('D:/redata/Neutraliza/vol_up_std_6M@nu.pkl')\n",
    "# f27 = pd.read_pickle('D:/redata/Neutraliza/vol_highlow_std_6M@nu.pkl')\n",
    "# f28 = pd.read_pickle('D:/redata/Neutraliza/vol_upshadow_std_6M@nu.pkl')\n",
    "# f29 = pd.read_pickle('D:/redata/Neutraliza/vol_w_downshadow_std_6M@nu.pkl')\n",
    "fh24 = pd.read_pickle('D:/redata/Neutraliza/vol_6M@nu.pkl')\n",
    "\n",
    "# f30 = pd.read_pickle('D:/redata/Neutraliza/liq_turn_std_6M@nu.pkl')\n",
    "fh25 = pd.read_pickle('D:/redata/Neutraliza/turnrate_6M@nu.pkl')\n",
    "\n",
    "# f31 = pd.read_pickle('D:/redata/Neutraliza/liq_vstd_1M@nu.pkl')\n",
    "# f32 = pd.read_pickle('D:/redata/Neutraliza/liq_amiihud_avg_1M@nu.pkl')\n",
    "# f33 = pd.read_pickle('D:/redata/Neutraliza/liq_shortcut_avg_1M@nu.pkl')\n",
    "fh26 = pd.read_pickle('D:/redata/Neutraliza/ill_price_M@nu.pkl')\n",
    "\n",
    "# f34 = pd.read_pickle('D:/redata/Neutraliza/corr_ret_turnd_1M@nu.pkl')\n",
    "# f35 = pd.read_pickle('D:/redata/Neutraliza/corr_price_turn_1M@nu.pkl')\n",
    "# f36 = pd.read_pickle('D:/redata/Neutraliza/corr_ret_turn_post_1M@nu.pkl')\n",
    "fh27 = pd.read_pickle('D:/redata/Neutraliza/corrPV_M@nu.pkl')\n",
    "\n",
    "# f37 = pd.read_pickle('D:/redata/Neutraliza/mmt_normal_M@nu.pkl')\n",
    "# f38 = pd.read_pickle('D:/redata/Neutraliza/mmt_off_limit_M@nu.pkl')\n",
    "fh28 = pd.read_pickle('D:/redata/Neutraliza/mom_M@nu.pkl')\n",
    "\n",
    "f39 = pd.read_pickle('D:/redata/Neutraliza/volume_20d_fac@nu.pkl')\n",
    "f40 = pd.read_pickle('D:/redata/Neutraliza/volume_60d_ratio@nu.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3ef68df4-0dfb-44c7-8b6d-3e076b6973c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_mean = f3+f5+f7+f14+f15+f18+f19+fh21+fh22+fh23/2+fh24/2+fh25/2+fh26+fh28/2+f39\n",
    "# comb_mean = f3+f5+f7+f14+f15+f18+f19+fh21+fh22+fh23+fh24+fh25+fh26+fh28+f39\n",
    "# comb_mean = f5+f14+f18+f19+f20+fh21+fh22+fh23/2+fh24/2+fh25/2+fh26+fh28/2+f39\n",
    "\n",
    "comb_mean.to_pickle('D:/redata/factor/comb_mean.pkl')\n",
    "\n",
    "#市值分层read_pickle\n",
    "comb_mean_s = comb_mean * small\n",
    "comb_mean_s.to_pickle('D:/redata/factor/comb_mean_s.pkl')\n",
    "\n",
    "comb_mean_m = comb_mean * median\n",
    "comb_mean_m.to_pickle('D:/redata/factor/comb_mean_m.pkl')\n",
    "\n",
    "comb_mean_sm = comb_mean * sm\n",
    "comb_mean_sm.to_pickle('D:/redata/factor/comb_mean_sm.pkl')\n",
    "\n",
    "comb_mean_l = comb_mean * large\n",
    "comb_mean_l.to_pickle('D:/redata/factor/comb_mean_l.pkl')\n",
    "\n",
    "# 5、10、20日\n",
    "# a = position(comb_mean_sm, 5)\n",
    "# a.to_pickle('D:/redata/factor/comb_mean_sm5d.pkl')\n",
    "# b = position(comb_mean_sm, 10)\n",
    "# b.to_pickle('D:/redata/factor/comb_mean_sm10d.pkl')\n",
    "# d = position(comb_mean_sm, 20)\n",
    "# d.to_pickle('D:/redata/factor/comb_mean_sm20d.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ec6faed7-3d2f-455f-8fe0-03e832c22050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ticker</th>\n",
       "      <th>000001</th>\n",
       "      <th>000002</th>\n",
       "      <th>000004</th>\n",
       "      <th>000006</th>\n",
       "      <th>000007</th>\n",
       "      <th>000008</th>\n",
       "      <th>000009</th>\n",
       "      <th>000010</th>\n",
       "      <th>000011</th>\n",
       "      <th>000012</th>\n",
       "      <th>...</th>\n",
       "      <th>688787</th>\n",
       "      <th>688788</th>\n",
       "      <th>688789</th>\n",
       "      <th>688793</th>\n",
       "      <th>688798</th>\n",
       "      <th>688799</th>\n",
       "      <th>688800</th>\n",
       "      <th>688819</th>\n",
       "      <th>688981</th>\n",
       "      <th>689009</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-12-26</th>\n",
       "      <td>35046.5</td>\n",
       "      <td>28990.0</td>\n",
       "      <td>26073.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35058.0</td>\n",
       "      <td>27422.5</td>\n",
       "      <td>33863.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28180.5</td>\n",
       "      <td>32004.5</td>\n",
       "      <td>...</td>\n",
       "      <td>27892.5</td>\n",
       "      <td>36908.0</td>\n",
       "      <td>30163.0</td>\n",
       "      <td>22634.0</td>\n",
       "      <td>34218.5</td>\n",
       "      <td>33591.5</td>\n",
       "      <td>36506.0</td>\n",
       "      <td>40619.0</td>\n",
       "      <td>36869.0</td>\n",
       "      <td>34698.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-27</th>\n",
       "      <td>35216.5</td>\n",
       "      <td>29156.0</td>\n",
       "      <td>26113.0</td>\n",
       "      <td>29256.0</td>\n",
       "      <td>33967.5</td>\n",
       "      <td>27124.5</td>\n",
       "      <td>34443.0</td>\n",
       "      <td>28308.0</td>\n",
       "      <td>29017.0</td>\n",
       "      <td>31728.0</td>\n",
       "      <td>...</td>\n",
       "      <td>27511.5</td>\n",
       "      <td>37529.0</td>\n",
       "      <td>30345.5</td>\n",
       "      <td>22879.5</td>\n",
       "      <td>34767.5</td>\n",
       "      <td>33095.5</td>\n",
       "      <td>36174.0</td>\n",
       "      <td>40987.0</td>\n",
       "      <td>36576.5</td>\n",
       "      <td>34130.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 4953 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "ticker       000001   000002   000004   000006   000007   000008   000009  \\\n",
       "date                                                                        \n",
       "2024-12-26  35046.5  28990.0  26073.0      NaN  35058.0  27422.5  33863.5   \n",
       "2024-12-27  35216.5  29156.0  26113.0  29256.0  33967.5  27124.5  34443.0   \n",
       "\n",
       "ticker       000010   000011   000012  ...   688787   688788   688789  \\\n",
       "date                                   ...                              \n",
       "2024-12-26      NaN  28180.5  32004.5  ...  27892.5  36908.0  30163.0   \n",
       "2024-12-27  28308.0  29017.0  31728.0  ...  27511.5  37529.0  30345.5   \n",
       "\n",
       "ticker       688793   688798   688799   688800   688819   688981   689009  \n",
       "date                                                                       \n",
       "2024-12-26  22634.0  34218.5  33591.5  36506.0  40619.0  36869.0  34698.5  \n",
       "2024-12-27  22879.5  34767.5  33095.5  36174.0  40987.0  36576.5  34130.0  \n",
       "\n",
       "[2 rows x 4953 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_mean.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "31b92a46-ac96-4f4f-bbee-a2cc249f8866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ticker</th>\n",
       "      <th>300552</th>\n",
       "      <th>301071</th>\n",
       "      <th>301141</th>\n",
       "      <th>301230</th>\n",
       "      <th>301302</th>\n",
       "      <th>301328</th>\n",
       "      <th>301368</th>\n",
       "      <th>301388</th>\n",
       "      <th>301508</th>\n",
       "      <th>301516</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-12-27</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ticker      300552  301071  301141  301230  301302  301328  301368  301388  \\\n",
       "date                                                                         \n",
       "2024-12-27       1       1       1       1       1       1       1       1   \n",
       "\n",
       "ticker      301508  301516  \n",
       "date                        \n",
       "2024-12-27       1       1  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('D:/redata/factor/comb_mean_sm.pkl')###读入因子值\n",
    "df = df.replace(np.inf,np.nan).replace(-np.inf,np.nan)\n",
    "#剔除 新股 st halt\n",
    "df_clean = (df * final_universe)\n",
    "df_clean = df_clean.dropna(how='all',axis=0)\n",
    "factor_type = 'hist'  # 'hist' 因子原值， 'ind' 对因子做行业中性\n",
    "factor_rank = factor_neutralize(df_clean,factor_type,if_rank=True,risk=risk,risk_cols=['logcmv','retsum120']) \n",
    "\n",
    "select_num_stocks(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0943387b-76b1-4569-a14a-8849d427a6f4",
   "metadata": {},
   "source": [
    "### 通达信选股"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae0b77d-233c-487f-9fa4-e1eabc6ab827",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13282a0f-eb8b-4cdc-85ba-878d55191eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa1445-c5a6-4564-b2df-8433bb83e683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b12018f-81f0-45ac-be76-b03f6e11960f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f509485-1638-40bd-a399-5704cdd03a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4166be3b-00e3-4dec-8bf1-7129903d38a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb7f995-3515-4375-9d5e-9d7ae0e83058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01da8242-b784-46c7-9882-c6b5c3371bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fcc933-f235-4982-8cb1-e3f692ed70e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5932a350-f87b-4f40-b0cf-57f9d1129f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5969c840-22cd-4b52-8318-ccbf95afc0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b093afab-24ba-4db3-822c-a3fd59e64a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
