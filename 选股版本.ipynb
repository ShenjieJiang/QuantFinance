{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6973bd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import oss_handler\n",
    "from oss_handler import OssClient\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from pylab import mpl\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9eb07d",
   "metadata": {},
   "source": [
    "# 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "873bc6d1-f5ee-4878-bdbb-5d1b9e54a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "oss_client = OssClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86955143-e744-4e12-b797-4bd7338414b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx000001 = oss_client.read_oss_pickle_file('ad_hoc_prod/index_hist/000001.XSHG_hist_index.pkl') #上证指数\n",
    "idx000016 = oss_client.read_oss_pickle_file('ad_hoc_prod/index_hist/000016.XSHG_hist_index.pkl') #上证 50\n",
    "idx000300 = oss_client.read_oss_pickle_file('ad_hoc_prod/index_hist/000300.XSHG_hist_index.pkl') #沪深 300\n",
    "idx000852 = oss_client.read_oss_pickle_file('ad_hoc_prod/index_hist/000852.XSHG_hist_index.pkl') #中证 1000\n",
    "idx000905 = oss_client.read_oss_pickle_file('ad_hoc_prod/index_hist/000905.XSHG_hist_index.pkl') #中证 500\n",
    "idx000985 = oss_client.read_oss_pickle_file('ad_hoc_prod/index_hist/000985.XSHG_hist_index.pkl') #中证全指\n",
    "# idx399001 = oss_client.read_oss_pickle_file('ad_hoc_prod/index_hist/399001.XSHG_hist_index.pkl') #深证成指\n",
    "# idx399006 = oss_client.read_oss_pickle_file('ad_hoc_prod/index_hist/399006.XSHG_hist_index.pkl') #创业板指\n",
    "# idx399303 = oss_client.read_oss_pickle_file('ad_hoc_prod/index_hist/399303.XSHG_hist_index.pkl') #国证 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f0aeda3-7b80-4961-b32f-ed52494c12c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_close = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/adj_close.pk')\n",
    "adj_factor = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/adj_factor.pk')\n",
    "adj_high = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/adj_high.pk')\n",
    "adj_low = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/adj_low.pk')\n",
    "adj_open = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/adj_open.pk')\n",
    "\n",
    "circulation_a = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/circulation_a.pk') #流通股本\n",
    "total_a = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/total_a.pk') #总股本\n",
    "circulation_market_value = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/circulation_market_value.pk') #流通市值\n",
    "\n",
    "volume = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/volume.pk') #成交量\n",
    "num_trades = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/num_trades.pk') #交易次数\n",
    "total_turnover = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/total_turnover.pk') #换手额\n",
    "turnover_rate = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/turnover_rate.pk') #换手率\n",
    "\n",
    "close = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/close.pk') \n",
    "open = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/open.pk') \n",
    "high = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/high.pk') \n",
    "low = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/low.pk') \n",
    "\n",
    "limit_down = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/limit_down.pk') #跌停\n",
    "limit_up = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/limit_up.pk') #涨停\n",
    "\n",
    "halt_status = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/halt_status.pk') #停牌 \n",
    "st_status = oss_client.read_oss_pickle_file('ad_hoc_prod/fields_full/st_status.pk') #st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec837824-034d-4d4a-9550-278e67360640",
   "metadata": {},
   "outputs": [],
   "source": [
    "BS_pit = oss_client.read_oss_pickle_file('ad_hoc_prod/pit_fundemental/BS_pit.pkl')\n",
    "CF_pit = oss_client.read_oss_pickle_file('ad_hoc_prod/pit_fundemental/CF_pit.pkl')\n",
    "IS_pit = oss_client.read_oss_pickle_file('ad_hoc_prod/pit_fundemental/IS_pit.pkl')\n",
    "all_pit = oss_client.read_oss_pickle_file('ad_hoc_prod/pit_fundemental/all_pit_fund_new_api.pkl')\n",
    "\n",
    "# current_perf = oss_client.read_oss_pickle_file('ad_hoc_prod/pit_fundemental/current_perf.pkl') #当下表现\n",
    "# perf_forecast = oss_client.read_oss_pickle_file('ad_hoc_prod/pit_fundemental/perf_forecast.pkl') #预期\n",
    "# devidend = oss_client.read_oss_parquet_file('ad_hoc_prod/pit_fundemental/rq_dividend.parquet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e2959dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##正常交易非st，halt的票打1.其他为np.nan\n",
    "st_status = st_status.replace(True,np.nan).replace(False,True)\n",
    "halt_status = halt_status.replace(True,np.nan).replace(False,True)\n",
    "\n",
    "adj_return =  adj_close/adj_close.shift(1) - 1\n",
    "adj_return = adj_return.replace(np.inf,np.nan).replace(-np.inf,np.nan)\n",
    "\n",
    "##调仓周期\n",
    "o1o2 = adj_open.shift(-2)/adj_open.shift(-1) - 1 #1日调仓\n",
    "# o1o2 = adj_open.shift(-5) / adj_open.shift(-1) - 1 #5日调仓\n",
    "# o1o2 = adj_open.shift(-10) / adj_open.shift(-1) - 1 #10日调仓\n",
    "# o1o2 = adj_open.shift(-20) / adj_open.shift(-1) - 1 #20日调仓\n",
    "\n",
    "mc = total_a * adj_close\n",
    "\n",
    "csi500 = idx000905[['open','close','low']].set_index(idx000905['date'])\n",
    "idx_500_ret = (csi500['close']/csi500['close'].shift(1) - 1)\n",
    "\n",
    "mclose = csi500['close']\n",
    "mopen = csi500['open']\n",
    "mlow = csi500['low']\n",
    "\n",
    "open['label'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d974d917-e64b-44c3-a359-e732875ec787",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 中性化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a40fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##因子中性化 需要的数据\n",
    "risk1 = pd.read_pickle('D:/redata/risk_factor/logcmv.pk')\n",
    "risk2 = pd.read_pickle('D:/redata/risk_factor/retsum120.pk')\n",
    "ind = pd.read_pickle('D:/redata/risk_factor/ind_label.pk')\n",
    "risk = risk1.merge(risk2,on=['date','ticker'],how='left')\n",
    "risk = risk.merge(ind,on=['date','ticker'],how='left')\n",
    "risk = risk[(risk['date']>'2017-01-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d98c2817",
   "metadata": {},
   "outputs": [],
   "source": [
    "##行业中性 2023.8.3 更新\n",
    "def neturalize_ind(factor,risk=risk,risk_cols=['logcmv','retsum120']):\n",
    "    XY = pd.merge(risk, factor, on=['date', 'ticker'], how='right')\n",
    "    #print(XY.head(3))\n",
    "    XY = XY.dropna()\n",
    "    tradeday = XY.date.unique()\n",
    "    daily_resid = []\n",
    "    for date in tqdm(tradeday[:]):\n",
    "        xy = XY[XY['date'] == date]\n",
    "        #只保留有因子值的数据\n",
    "        xy = xy.dropna()\n",
    "\n",
    "        #数据标准化\n",
    "        xy[risk_cols] = (xy[risk_cols] - xy[risk_cols].mean())/xy[risk_cols].std()                \n",
    "        xy['factor'] = (xy['factor']-xy['factor'].mean())/xy['factor'].std() \n",
    "\n",
    "        #选出x y\n",
    "        x = xy.iloc[:,2:-1]\n",
    "        y = xy['factor']\n",
    "        #回归\n",
    "        multi_linear = LinearRegression(fit_intercept=False)  #无截距项\n",
    "        multi_linear.fit(x, y)\n",
    "        beta = multi_linear.coef_\n",
    "        \n",
    "        #取残差\n",
    "        xy['beta'] = np.dot(x, beta)\n",
    "        xy['resid'] = xy['factor'] - xy['beta']\n",
    "        daily_resid.append(xy)\n",
    "    data = pd.concat(daily_resid)\n",
    "    factor = data[['date', 'ticker', 'factor', 'resid']]\n",
    "    return factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc1b4fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor_neutralize(df,factor_type,if_rank=True,risk=None,risk_cols=None): #factor_type=hist原值；ind行业及其他中性；单个风险因子文件名； if_rank：中性化之前时候对因子做rank处理\n",
    "    #中性之前是否对因子做rank处理\n",
    "    if if_rank == True:\n",
    "        df = df.rank(axis=1, ascending=True)  # 原值越大rank值越大   \n",
    "    #因子原值\n",
    "    if factor_type == 'hist':\n",
    "        factor_rank = df.copy()\n",
    "    #行业市值中性\n",
    "    elif factor_type == 'ind':  ###行业和风格中性\n",
    "        def neu_process(df):\n",
    "            df = df.stack().reset_index()\n",
    "            df.columns = ['date','ticker','factor']\n",
    "            df = neturalize_ind(df,risk,risk_cols)\n",
    "            resid = df.set_index(['date','ticker'])['resid'].unstack()\n",
    "            return resid        \n",
    "        factor_rank = neu_process(df)\n",
    "        factor_rank = factor_rank.rank(axis=1,ascending=True) \n",
    "        \n",
    "    return factor_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ff1b13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neu_process(df):\n",
    "    df = df.stack().reset_index()\n",
    "    df.columns = ['date','ticker','factor']\n",
    "    df = neturalize_ind(df,risk=risk,risk_cols=['logcmv','retsum120'])\n",
    "    resid = df.set_index(['date','ticker'])['resid'].unstack()\n",
    "    return resid        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c21f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_num_stocks(n):\n",
    "    select_stocks = (factor_rank <= n)\n",
    "    select_stocks_last = select_stocks.iloc[-1:,]\n",
    "    select_stocks_last = select_stocks_last.replace(True,1).replace(False,np.nan)\n",
    "    select_stocks_last = select_stocks_last.dropna(how='all',axis=1)\n",
    "    return select_stocks_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "312cdeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "##市值排uni\n",
    "mv_rank = risk1.set_index(['date','ticker'])['logcmv'].unstack()\n",
    "mv_rank = mv_rank.rank(axis=1, ascending=True, pct=True)\n",
    "\n",
    "##小市值\n",
    "small = (mv_rank <= 0.33)\n",
    "small = small.replace(True,1).replace(False,np.nan)\n",
    "small = small.loc['2017':]\n",
    "\n",
    "##中市值\n",
    "median = (mv_rank <= 0.66) & (mv_rank >= 0.33)\n",
    "median = median.replace(True,1).replace(False,np.nan)\n",
    "median = median.loc['2017':]\n",
    "\n",
    "##小中市值\n",
    "sm = (mv_rank <= 0.66)\n",
    "sm = sm.replace(True,1).replace(False,np.nan)\n",
    "sm = sm.loc['2017':]\n",
    "\n",
    "##大市值\n",
    "large = (mv_rank>=0.66)\n",
    "large = large.replace(True,1).replace(False,np.nan)\n",
    "large = large.loc['2017':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fc6920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def position(df, period):\n",
    "    empty = df > 0\n",
    "    empty = empty.replace(True,np.nan).replace(False,np.nan)\n",
    "    \n",
    "    position = df.iloc[::period]\n",
    "    position = pd.concat([empty, position])\n",
    "    position = position.sort_values(['date'])\n",
    "    position = position.reset_index()\n",
    "    position = position.drop_duplicates(subset=['date'], keep='last').ffill(limit = period)\n",
    "    position = position.set_index(['date'])\n",
    "    return position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81344286",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = 'Z:/redata/Sen/fundmental_nonfillna' #财报原始数据，单个指标，用于计算因子值的中间数据\n",
    "path2 = 'Z:/redata/Sen/' #日频财务数据\n",
    "path_Q = 'Z:/redata/Sen/fundmental_Q'#季度频率，单季度财务数据，用于计算因子值的中间数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fe0f17",
   "metadata": {},
   "source": [
    "# 财务数据函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c45c93f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chg_format(df1):\n",
    "    df1['ticker'] = df1['order_book_id'].str[:-5]\n",
    "    df1['date'] = pd.to_datetime(df1['info_date'])\n",
    "    df1['report_period'] = df1['quarter'].map(lambda x:x.replace('q1','0331').replace('q2','0630').replace('q3','0930').replace('q4','1231'))\n",
    "    df1['report_period'] = df1['report_period'].astype('int')\n",
    "    df1 = df1.sort_values(['date','ticker','report_period']).reset_index(drop=True)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37b57d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_save(factor,factor_name):\n",
    "    ##只保留当前最新业绩的数据\n",
    "    def func(x):\n",
    "        x = x[x['report_period']>=x['report_period'].expanding(min_periods=1).max()]\n",
    "        return x\n",
    "    factor = factor.groupby('ticker').apply(func).reset_index(drop=True)\n",
    "    #factor同一个date ticker 去重，保留当前最新report_period\n",
    "    factor = factor.drop_duplicates(subset=['date','ticker'],keep='last')\n",
    "    factor = factor.set_index(['date','ticker'])[factor_name].unstack()\n",
    "    factor = pd.concat([factor,Open[['label']]],axis=1).drop(['label'],axis=1)\n",
    "    factor = factor.fillna(method='ffill',limit=150).dropna(how='all',axis=0).dropna(how='all',axis=1)\n",
    "    factor.to_pickle(os.path.join(path2,factor_name+'.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e939317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##利润表流量单季度数据\n",
    "# dfaa = pd.read_pickle('Z:/fundamental/IS_pit.pkl')\n",
    "dfaa = IS_pit\n",
    "dfaa = dfaa.reset_index()\n",
    "dfaa = chg_format(dfaa)\n",
    "dfaa = dfaa.drop_duplicates(subset=['date','ticker','report_period'],keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a6c46ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#时间表\n",
    "mls = pd.DataFrame(dfaa.report_period.unique())\n",
    "mls.columns = ['report_period']\n",
    "mls['pre_report_period'] = mls['report_period'].shift(1)\n",
    "mls['pre2_report_period'] = mls['report_period'].shift(2)\n",
    "mls['pre3_report_period'] = mls['report_period'].shift(3)\n",
    "mls['next1_report_period'] = mls['report_period'].shift(-1)\n",
    "mls['next2_report_period'] = mls['report_period'].shift(-2)\n",
    "mls['next3_report_period'] = mls['report_period'].shift(-3)\n",
    "mls = mls.replace(np.nan,0)\n",
    "mls = mls.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23a7e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##现金流量单季度数据\n",
    "# dfbb = pd.read_pickle('Z:/fundamental/CF_pit.pkl')\n",
    "dfbb = CF_pit\n",
    "dfbb = dfbb.reset_index()\n",
    "dfbb = chg_format(dfbb)\n",
    "dfbb = dfbb.drop_duplicates(subset=['date','ticker','report_period'],keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d97e8ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#时间表\n",
    "mlss = pd.DataFrame(dfbb.report_period.unique())\n",
    "mlss.columns = ['report_period']\n",
    "mlss['pre_report_period'] = mlss['report_period'].shift(1)\n",
    "mlss['pre2_report_period'] = mlss['report_period'].shift(2)\n",
    "mlss['pre3_report_period'] = mlss['report_period'].shift(3)\n",
    "mlss['next1_report_period'] = mlss['report_period'].shift(-1)\n",
    "mlss['next2_report_period'] = mlss['report_period'].shift(-2)\n",
    "mlss['next3_report_period'] = mlss['report_period'].shift(-3)\n",
    "mlss = mlss.replace(np.nan,0)\n",
    "mlss = mlss.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea4dcad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##债务截面单季度数据\n",
    "# dfcc = pd.read_pickle('Z:/fundamental/BS_pit.pkl')\n",
    "dfcc = BS_pit\n",
    "dfcc = dfcc.reset_index()\n",
    "dfcc = chg_format(dfcc)\n",
    "dfcc = dfcc.drop_duplicates(subset=['date','ticker','report_period'],keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5744141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#时间表\n",
    "mlsss = pd.DataFrame(dfcc.report_period.unique())\n",
    "mlsss.columns = ['report_period']\n",
    "mlsss['pre_report_period'] = mlsss['report_period'].shift(1)\n",
    "mlsss['pre2_report_period'] = mlsss['report_period'].shift(2)\n",
    "mlsss['pre3_report_period'] = mlsss['report_period'].shift(3)\n",
    "mlsss['next1_report_period'] = mlsss['report_period'].shift(-1)\n",
    "mlsss['next2_report_period'] = mlsss['report_period'].shift(-2)\n",
    "mlsss['next3_report_period'] = mlsss['report_period'].shift(-3)\n",
    "mlsss = mlsss.replace(np.nan,0)\n",
    "mlsss = mlsss.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6d0d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IS,CF的单季度计算\n",
    "def single_Q(df1,col):\n",
    "    ##计算单季度数据：原理当季度数据发生更改时，当季的Q和下一季的Q会受到影响。\n",
    "    ##分别计算当季的Q和下一季的Q 然后两个部分合并到一起。 要保证当季的Q和下一季的Q时，用到的数据日期全部不能大于当前日期。\n",
    "    #part1：计算当季的Q\n",
    "    df3 = df1[['date','ticker','quarter','report_period',col]].dropna(subset=[col])\n",
    "    #拼上对应的前3个季度的report_period\n",
    "    df3 = df3.merge(mls[['report_period','pre_report_period']],on=['report_period'],how='left')\n",
    "    df3 = df3.merge(df3,left_on=['ticker','pre_report_period'],right_on=['ticker','report_period'],suffixes=['','_pre'],how='left')\n",
    "    df3 = df3.sort_values(['date','ticker','report_period','date_pre'])\n",
    "    #单季数据=当期-上期 ，【第一季用当期】\n",
    "    df3[col+'_Q'] = df3[col] - df3[col+'_pre']\n",
    "    df3[col+'_Q'] = np.where(df3['quarter'].str[-2:] == 'q1',df3[col],df3[col+'_Q'])\n",
    "    #保留所有Q1和除Q1之外date>=date_pre\n",
    "    df3 = df3[(df3['quarter'].str[-2:]=='q1')|(df3['date']>=df3['date_pre'])] \n",
    "    df3 = df3.drop_duplicates(subset=['date','ticker','report_period'],keep='last') #只保留date_pre最新的一条数据\n",
    "    \n",
    "    #part2：计算下一季Q\n",
    "    tmp = df1[['date','ticker','quarter','report_period',col]].dropna(subset=[col])\n",
    "    #拼上对应的前3个季度的report_period\n",
    "    tmp = tmp.merge(mls[['report_period','next1_report_period']],on=['report_period'],how='left')\n",
    "    tmp = tmp.merge(tmp,left_on=['ticker','next1_report_period'],right_on=['ticker','report_period'],suffixes=['','_next1'],how='left')\n",
    "    tmp[col+'_Q_next1'] = tmp[col+'_next1'] - tmp[col]\n",
    "    tmp[col+'_Q_next1'] = np.where(tmp['quarter'].str[-2:] == 'q4',tmp[col+'_next1'],tmp[col+'_Q_next1']) #如果当前是四季度，\n",
    "    #保留 next_quarter_date<=quarter_date\n",
    "    tmp = tmp[tmp['date_next1']<=tmp['date']]\n",
    "    tmp = tmp.drop_duplicates(subset=['date','ticker','report_period'],keep='last')  #只保留next_quarter_date<=quarter_date，且next_quarter_date最新的\n",
    "    tmp['report_period'] = tmp['report_period_next1']\n",
    "    tmp = tmp[(tmp['quarter'].str[-2:]!='q4')] #去掉所有当前为4季度的情况，如果当前为4季度，即便数据发成更改，也不影响下一季度的单季数据，因为下一季度是一季度\n",
    "    tmp[col+'_Q'] = tmp[col+'_Q_next1']\n",
    "    \n",
    "    #把两个部分合并就是所有发生更新的数据\n",
    "    part1 = df3[['date', 'ticker', 'report_period',col+'_Q']]\n",
    "    part2 = tmp[['date', 'ticker', 'report_period',col+'_Q']]\n",
    "    # part1['label'] = '1' #可删，只是为了识别数据来源\n",
    "    # part2['label'] = '2' #可删，只是为了识别数据来源\n",
    "    part12 = pd.concat([part1,part2])\n",
    "    part12 = part12.sort_values(['date','ticker','report_period'])\n",
    "    part12 = part12.drop_duplicates(subset=['date','ticker','report_period'],keep='last') #删掉重复行\n",
    "    return part12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e2b4daed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#同比yoy\n",
    "def compute_yoy(df4,col):#计算yoy\n",
    "    tool_data = df4[['date','ticker','report_period',col]]\n",
    "    #part1\n",
    "    df1 = df4[['date', 'ticker', 'report_period',col]]\n",
    "    df1['report_period'] = df1['report_period'].astype('int') ###新增\n",
    "    df1['pre_yr_report_period'] = (df1['report_period'].astype('str').str[:4].astype('int')-1).astype('str') + df1['report_period'].astype('str').str[-4:]\n",
    "    df1['pre_yr_report_period'] = df1['pre_yr_report_period'].astype('int')\n",
    "    df1 = df1.merge(tool_data,left_on=['ticker','pre_yr_report_period'],right_on=['ticker','report_period'],how='left',suffixes=['','_pre_yr'])\n",
    "    df1 = df1.sort_values(['date','ticker','report_period','date_pre_yr'])\n",
    "    df1 = df1[df1['date']>=df1['date_pre_yr']]\n",
    "    df1 = df1.drop_duplicates(subset=['date','ticker','report_period'],keep='last')   ##只保留'date','ticker','report_period'对应的'date_pre_yr'最新的一条数据\n",
    "    df1[col+'_yoy'] = (df1[col]-df1[col+'_pre_yr'])/np.abs(df1[col+'_pre_yr'])\n",
    "    #part2\n",
    "    df2 = df4[['date', 'ticker', 'report_period',col]]\n",
    "    df2['report_period'] = df2['report_period'].astype('int') ###新增\n",
    "    df2['next_yr_report_period'] = (df2['report_period'].astype('str').str[:4].astype('int')+1).astype('str') + df2['report_period'].astype('str').str[-4:]\n",
    "    df2['next_yr_report_period'] = df2['next_yr_report_period'].astype('int')\n",
    "    df2 = df2.merge(tool_data,left_on=['ticker','next_yr_report_period'],right_on=['ticker','report_period'],how='left',suffixes=['','_next_yr'])\n",
    "    df2 = df2.sort_values(['date','ticker','report_period','date_next_yr'])\n",
    "    df2 = df2[df2['date']>=df2['date_next_yr']]\n",
    "    df2['report_period'] = df2['report_period_next_yr']\n",
    "    df2 = df2.drop_duplicates(subset=['date','ticker','report_period'],keep='last')   ##只保留'date','ticker','report_period'对应的'date_pre_yr'最新的一条数据\n",
    "    df2[col+'_yoy'] = (df2[col+'_next_yr']-df2[col])/np.abs(df2[col])\n",
    "\n",
    "    #把两个部分合并就是所有发生更新的数据\n",
    "    part1 = df1[['date', 'ticker', 'report_period',col+'_yoy']]\n",
    "    part2 = df2[['date', 'ticker', 'report_period',col+'_yoy']]\n",
    "    part12 = pd.concat([part1,part2])\n",
    "    part12 = part12.sort_values(['date','ticker','report_period'])\n",
    "    part12 = part12.drop_duplicates(subset=['date','ticker','report_period'],keep='last') #删掉重复行\n",
    "    return part12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c39e96ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#T当季-去年同季\n",
    "def compute_T(df4,col):\n",
    "    tool_data = df4[['date','ticker','report_period',col]]\n",
    "    #part1\n",
    "    df1 = df4[['date', 'ticker', 'report_period',col]]\n",
    "    df1['report_period'] = df1['report_period'].astype('int') ###新增\n",
    "    df1['pre_yr_report_period'] = (df1['report_period'].astype('str').str[:4].astype('int')-1).astype('str') + df1['report_period'].astype('str').str[-4:]\n",
    "    df1['pre_yr_report_period'] = df1['pre_yr_report_period'].astype('int')\n",
    "    df1 = df1.merge(tool_data,left_on=['ticker','pre_yr_report_period'],right_on=['ticker','report_period'],how='left',suffixes=['','_pre_yr'])\n",
    "    df1 = df1.sort_values(['date','ticker','report_period','date_pre_yr'])\n",
    "    df1 = df1[df1['date']>=df1['date_pre_yr']]\n",
    "    df1 = df1.drop_duplicates(subset=['date','ticker','report_period'],keep='last')   ##只保留'date','ticker','report_period'对应的'date_pre_yr'最新的一条数据\n",
    "    df1[col+'_T'] = (df1[col]-df1[col+'_pre_yr'])\n",
    "    #part2\n",
    "    df2 = df4[['date', 'ticker', 'report_period',col]]\n",
    "    df2['report_period'] = df2['report_period'].astype('int') ###新增\n",
    "    df2['next_yr_report_period'] = (df2['report_period'].astype('str').str[:4].astype('int')+1).astype('str') + df2['report_period'].astype('str').str[-4:]\n",
    "    df2['next_yr_report_period'] = df2['next_yr_report_period'].astype('int')\n",
    "    df2 = df2.merge(tool_data,left_on=['ticker','next_yr_report_period'],right_on=['ticker','report_period'],how='left',suffixes=['','_next_yr'])\n",
    "    df2 = df2.sort_values(['date','ticker','report_period','date_next_yr'])\n",
    "    df2 = df2[df2['date']>=df2['date_next_yr']]\n",
    "    df2['report_period'] = df2['report_period_next_yr']\n",
    "    df2 = df2.drop_duplicates(subset=['date','ticker','report_period'],keep='last')   ##只保留'date','ticker','report_period'对应的'date_pre_yr'最新的一条数据\n",
    "    df2[col+'_T'] = (df2[col+'_next_yr']-df2[col])\n",
    "\n",
    "    #把两个部分合并就是所有发生更新的数据\n",
    "    part1 = df1[['date', 'ticker', 'report_period',col+'_T']]\n",
    "    part2 = df2[['date', 'ticker', 'report_period',col+'_T']]\n",
    "    part12 = pd.concat([part1,part2])\n",
    "    part12 = part12.sort_values(['date','ticker','report_period'])\n",
    "    part12 = part12.drop_duplicates(subset=['date','ticker','report_period'],keep='last') #删掉重复行\n",
    "    return part12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82bb16be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tadd当季+去年同季\n",
    "def compute_Tadd(df4,col):\n",
    "    tool_data = df4[['date','ticker','report_period',col]]\n",
    "    #part1\n",
    "    df1 = df4[['date', 'ticker', 'report_period',col]]\n",
    "    df1['report_period'] = df1['report_period'].astype('int') ###新增\n",
    "    df1['pre_yr_report_period'] = (df1['report_period'].astype('str').str[:4].astype('int')-1).astype('str') + df1['report_period'].astype('str').str[-4:]\n",
    "    df1['pre_yr_report_period'] = df1['pre_yr_report_period'].astype('int')\n",
    "    df1 = df1.merge(tool_data,left_on=['ticker','pre_yr_report_period'],right_on=['ticker','report_period'],how='left',suffixes=['','_pre_yr'])\n",
    "    df1 = df1.sort_values(['date','ticker','report_period','date_pre_yr'])\n",
    "    df1 = df1[df1['date']>=df1['date_pre_yr']]\n",
    "    df1 = df1.drop_duplicates(subset=['date','ticker','report_period'],keep='last')   ##只保留'date','ticker','report_period'对应的'date_pre_yr'最新的一条数据\n",
    "    df1[col+'_T'] = (df1[col]+df1[col+'_pre_yr'])\n",
    "    #part2\n",
    "    df2 = df4[['date', 'ticker', 'report_period',col]]\n",
    "    df2['report_period'] = df2['report_period'].astype('int') ###新增\n",
    "    df2['next_yr_report_period'] = (df2['report_period'].astype('str').str[:4].astype('int')+1).astype('str') + df2['report_period'].astype('str').str[-4:]\n",
    "    df2['next_yr_report_period'] = df2['next_yr_report_period'].astype('int')\n",
    "    df2 = df2.merge(tool_data,left_on=['ticker','next_yr_report_period'],right_on=['ticker','report_period'],how='left',suffixes=['','_next_yr'])\n",
    "    df2 = df2.sort_values(['date','ticker','report_period','date_next_yr'])\n",
    "    df2 = df2[df2['date']>=df2['date_next_yr']]\n",
    "    df2['report_period'] = df2['report_period_next_yr']\n",
    "    df2 = df2.drop_duplicates(subset=['date','ticker','report_period'],keep='last')   ##只保留'date','ticker','report_period'对应的'date_pre_yr'最新的一条数据\n",
    "    df2[col+'_T'] = (df2[col+'_next_yr']+df2[col])\n",
    "\n",
    "    #把两个部分合并就是所有发生更新的数据\n",
    "    part1 = df1[['date', 'ticker', 'report_period',col+'_T']]\n",
    "    part2 = df2[['date', 'ticker', 'report_period',col+'_T']]\n",
    "    part12 = pd.concat([part1,part2])\n",
    "    part12 = part12.sort_values(['date','ticker','report_period'])\n",
    "    part12 = part12.drop_duplicates(subset=['date','ticker','report_period'],keep='last') #删掉重复行\n",
    "    return part12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd33692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#环比QoQ\n",
    "def compute_qoq(df4,col):\n",
    "    tool_data = df4[['date','ticker','report_period',col]]\n",
    "    #part1\n",
    "    df1 = df4[['date','ticker','report_period','pre_report_period',col]]\n",
    "    df1 = df1.merge(tool_data,left_on=['ticker','pre_report_period'],right_on=['ticker','report_period'],how='left',suffixes=['','_pre'])\n",
    "    df1 = df1.sort_values(['date','ticker','report_period','date_pre'])\n",
    "    df1 = df1[df1['date']>=df1['date_pre']]\n",
    "    df1 = df1.drop_duplicates(subset=['date','ticker','report_period'],keep='last')   ##只保留'date','ticker','report_period'对应的'date_pre_yr'最新的一条数据\n",
    "    df1[col+'_QoQ'] = (df1[col]-df1[col+'_pre'])/np.abs(df1[col+'_pre'])\n",
    "    part1 = df1[['date','ticker','report_period',col+'_QoQ']]\n",
    "    #part2\n",
    "    df1 = df4[['date','ticker','report_period','next1_report_period',col]]\n",
    "    df1 = df1.merge(tool_data,left_on=['ticker','next1_report_period'],right_on=['ticker','report_period'],how='left',suffixes=['','_next'])\n",
    "    df1 = df1.sort_values(['date','ticker','report_period','date_next'])\n",
    "    df1 = df1[df1['date']>=df1['date_next']]\n",
    "    df1['report_period'] = df1['report_period_next']\n",
    "    df1 = df1.drop_duplicates(subset=['date','ticker','report_period'],keep='last')   ##只保留'date','ticker','report_period'对应的'date_pre_yr'最新的一条数据\n",
    "    df1[col+'_QoQ'] = (df1[col+'_next']-df1[col])/np.abs(df1[col])\n",
    "    part2 = df1[['date','ticker','report_period',col+'_QoQ']]\n",
    "\n",
    "    #把两个部分合并就是所有发生更新的数据\n",
    "    part12 = pd.concat([part1,part2])\n",
    "    part12 = part12.sort_values(['date','ticker','report_period'])\n",
    "    part12 = part12.drop_duplicates(subset=['date','ticker','report_period'],keep='last') #删掉重复行\n",
    "    return part12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55ecaf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#D 当季-上季\n",
    "def compute_D(df4,col):\n",
    "    tool_data = df4[['date','ticker','report_period',col]]\n",
    "    #part1\n",
    "    df1 = df4[['date','ticker','report_period','pre_report_period',col]]\n",
    "    df1 = df1.merge(tool_data,left_on=['ticker','pre_report_period'],right_on=['ticker','report_period'],how='left',suffixes=['','_pre'])\n",
    "    df1 = df1.sort_values(['date','ticker','report_period','date_pre'])\n",
    "    df1 = df1[df1['date']>=df1['date_pre']]\n",
    "    df1 = df1.drop_duplicates(subset=['date','ticker','report_period'],keep='last')   ##只保留'date','ticker','report_period'对应的'date_pre_yr'最新的一条数据\n",
    "    df1[col+'_D'] = df1[col]-df1[col+'_pre']\n",
    "    part1 = df1[['date','ticker','report_period',col+'_D']]\n",
    "    #part2\n",
    "    df1 = df4[['date','ticker','report_period','next1_report_period',col]]\n",
    "    df1 = df1.merge(tool_data,left_on=['ticker','next1_report_period'],right_on=['ticker','report_period'],how='left',suffixes=['','_next'])\n",
    "    df1 = df1.sort_values(['date','ticker','report_period','date_next'])\n",
    "    df1 = df1[df1['date']>=df1['date_next']]\n",
    "    df1['report_period'] = df1['report_period_next']\n",
    "    df1 = df1.drop_duplicates(subset=['date','ticker','report_period'],keep='last')   ##只保留'date','ticker','report_period'对应的'date_pre_yr'最新的一条数据\n",
    "    df1[col+'_D'] = df1[col+'_next']-df1[col]\n",
    "    part2 = df1[['date','ticker','report_period',col+'_D']]\n",
    "\n",
    "    #把两个部分合并就是所有发生更新的数据\n",
    "    part12 = pd.concat([part1,part2])\n",
    "    part12 = part12.sort_values(['date','ticker','report_period'])\n",
    "    part12 = part12.drop_duplicates(subset=['date','ticker','report_period'],keep='last') #删掉重复行\n",
    "    return part12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "30978642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ttm(df4,col):\n",
    "    #单季度数据计算的TTM，季度频率\n",
    "    tool_data = df4[['date','ticker','report_period',col+'_Q']]\n",
    "    #part1    \n",
    "    df3 = df4.merge(tool_data,left_on=['ticker','pre_report_period'],right_on=['ticker','report_period'],suffixes=['','_pre'],how='left')\n",
    "    df3 = df3.merge(tool_data,left_on=['ticker','pre2_report_period'],right_on=['ticker','report_period'],suffixes=['','_pre2'],how='left')\n",
    "    df3 = df3.merge(tool_data,left_on=['ticker','pre3_report_period'],right_on=['ticker','report_period'],suffixes=['','_pre3'],how='left')\n",
    "    #去掉 date_pre,date_pre2,date_pre3 大于date的行\n",
    "    df3 = df3[(df3['date']>=df3['date_pre'])&(df3['date']>=df3['date_pre2'])&(df3['date']>=df3['date_pre3'])]\n",
    "    ##只保留'date','ticker','report_period'对应的最新的一条数据\n",
    "    df3 = df3.sort_values(['ticker','date','date_pre','date_pre2','date_pre3']).drop_duplicates(subset=['date','ticker','report_period'],keep='last')\n",
    "    df3[col+'_TTM'] = df3[col+'_Q'] + df3[col+'_Q_pre'] + df3[col+'_Q_pre2']+ df3[col+'_Q_pre3']\n",
    "    part1 = df3[['date', 'ticker', 'report_period',col+'_TTM']]\n",
    "    #part1['label'] = 'part1'\n",
    "\n",
    "    #part2\n",
    "    df3 = df4.merge(tool_data,left_on=['ticker','pre_report_period'],right_on=['ticker','report_period'],suffixes=['','_pre'],how='left')\n",
    "    df3 = df3.merge(tool_data,left_on=['ticker','pre2_report_period'],right_on=['ticker','report_period'],suffixes=['','_pre2'],how='left')\n",
    "    df3 = df3.merge(tool_data,left_on=['ticker','next1_report_period'],right_on=['ticker','report_period'],suffixes=['','_next1'],how='left')\n",
    "    #去掉 date_pre,date_pre2,date_pre3 大于date的行\n",
    "    df3 = df3[(df3['date']>=df3['date_pre'])&(df3['date']>=df3['date_pre2'])&(df3['date']>=df3['date_next1'])]\n",
    "    df3['report_period'] = df3['report_period_next1']\n",
    "    ##只保留'date','ticker','report_period'对应的最新的一条数据\n",
    "    df3 = df3.sort_values(['ticker','date','date_pre','date_pre2','date_next1']).drop_duplicates(subset=['date','ticker','report_period'],keep='last')\n",
    "    df3[col+'_TTM'] = df3[col+'_Q'] + df3[col+'_Q_pre'] + df3[col+'_Q_pre2']+ df3[col+'_Q_next1']\n",
    "    part2 = df3[['date', 'ticker', 'report_period',col+'_TTM']]\n",
    "    #part2['label'] = 'part2'\n",
    "\n",
    "    #part3\n",
    "    df3 = df4.merge(tool_data,left_on=['ticker','pre_report_period'],right_on=['ticker','report_period'],suffixes=['','_pre'],how='left')\n",
    "    df3 = df3.merge(tool_data,left_on=['ticker','next1_report_period'],right_on=['ticker','report_period'],suffixes=['','_next1'],how='left')\n",
    "    df3 = df3.merge(tool_data,left_on=['ticker','next2_report_period'],right_on=['ticker','report_period'],suffixes=['','_next2'],how='left')\n",
    "    #去掉 date_pre,date_pre2,date_pre3 大于date的行\n",
    "    df3 = df3[(df3['date']>=df3['date_pre'])&(df3['date']>=df3['date_next1'])&(df3['date']>=df3['date_next2'])]\n",
    "    df3['report_period'] = df3['report_period_next2']\n",
    "    ##只保留'date','ticker','report_period'对应的最新的一条数据\n",
    "    df3 = df3.sort_values(['ticker','date','date_pre','date_next1','date_next2']).drop_duplicates(subset=['date','ticker','report_period'],keep='last')\n",
    "    df3[col+'_TTM'] = df3[col+'_Q'] + df3[col+'_Q_pre'] + df3[col+'_Q_next1']+ df3[col+'_Q_next2']\n",
    "    part3 = df3[['date', 'ticker', 'report_period',col+'_TTM']]\n",
    "    #part3['label'] = 'part3'\n",
    "\n",
    "    #part4\n",
    "    df3 = df4.merge(tool_data,left_on=['ticker','next1_report_period'],right_on=['ticker','report_period'],suffixes=['','_next1'],how='left')\n",
    "    df3 = df3.merge(tool_data,left_on=['ticker','next2_report_period'],right_on=['ticker','report_period'],suffixes=['','_next2'],how='left')\n",
    "    df3 = df3.merge(tool_data,left_on=['ticker','next3_report_period'],right_on=['ticker','report_period'],suffixes=['','_next3'],how='left')\n",
    "    #去掉 date_pre,date_pre2,date_pre3 大于date的行\n",
    "    df3 = df3[(df3['date']>=df3['date_next1'])&(df3['date']>=df3['date_next2'])&(df3['date']>=df3['date_next3'])]\n",
    "    df3['report_period'] = df3['report_period_next3']\n",
    "    ##只保留'date','ticker','report_period'对应的最新的一条数据\n",
    "    df3 = df3.sort_values(['ticker','date','date_next1','date_next1','date_next1']).drop_duplicates(subset=['date','ticker','report_period'],keep='last')\n",
    "    df3[col+'_TTM'] = df3[col+'_Q'] + df3[col+'_Q_next1'] + df3[col+'_Q_next2']+ df3[col+'_Q_next3']\n",
    "    part4 = df3[['date', 'ticker', 'report_period',col+'_TTM']]\n",
    "    #part4['label'] = 'part4'\n",
    "\n",
    "    part1234 = pd.concat([part1,part2,part3,part4])\n",
    "    part1234 = part1234.sort_values(['date','ticker','report_period'])\n",
    "    part1234 = part1234.drop_duplicates(subset=['date','ticker','report_period'],keep='last') #删掉重复行\n",
    "    return part1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b079b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date ticker report_period  revenue_TTM  revenue_TTM_growth\n",
      "3 2022-12-31      A    2022-12-31        460.0                 NaN\n",
      "4 2023-03-31      A    2023-03-31        500.0            8.695652\n",
      "5 2023-06-30      A    2023-06-30        540.0            8.000000\n",
      "6 2023-09-30      A    2023-09-30        580.0            7.407407\n",
      "7 2023-12-31      A    2023-12-31        620.0            6.896552\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compute_ttm_rate(df4, col):\n",
    "    # 单季度数据计算的TTM，季度频率\n",
    "    tool_data = df4[['date', 'ticker', 'report_period', col + '_Q']]\n",
    "    \n",
    "    # Part1\n",
    "    df3 = df4.merge(tool_data, left_on=['ticker', 'pre_report_period'], right_on=['ticker', 'report_period'], suffixes=['', '_pre'], how='left')\n",
    "    df3 = df3.merge(tool_data, left_on=['ticker', 'pre2_report_period'], right_on=['ticker', 'report_period'], suffixes=['', '_pre2'], how='left')\n",
    "    df3 = df3.merge(tool_data, left_on=['ticker', 'pre3_report_period'], right_on=['ticker', 'report_period'], suffixes=['', '_pre3'], how='left')\n",
    "    df3 = df3[(df3['date'] >= df3['date_pre']) & (df3['date'] >= df3['date_pre2']) & (df3['date'] >= df3['date_pre3'])]\n",
    "    df3 = df3.sort_values(['ticker', 'date', 'date_pre', 'date_pre2', 'date_pre3']).drop_duplicates(subset=['date', 'ticker', 'report_period'], keep='last')\n",
    "    df3[col + '_TTM'] = df3[col + '_Q'] + df3[col + '_Q_pre'] + df3[col + '_Q_pre2'] + df3[col + '_Q_pre3']\n",
    "    part1 = df3[['date', 'ticker', 'report_period', col + '_TTM']]\n",
    "    \n",
    "    # Part2\n",
    "    df3 = df4.merge(tool_data, left_on=['ticker', 'pre_report_period'], right_on=['ticker', 'report_period'], suffixes=['', '_pre'], how='left')\n",
    "    df3 = df3.merge(tool_data, left_on=['ticker', 'pre2_report_period'], right_on=['ticker', 'report_period'], suffixes=['', '_pre2'], how='left')\n",
    "    df3 = df3.merge(tool_data, left_on=['ticker', 'next1_report_period'], right_on=['ticker', 'report_period'], suffixes=['', '_next1'], how='left')\n",
    "    df3 = df3[(df3['date'] >= df3['date_pre']) & (df3['date'] >= df3['date_pre2']) & (df3['date'] >= df3['date_next1'])]\n",
    "    df3['report_period'] = df3['report_period_next1']\n",
    "    df3 = df3.sort_values(['ticker', 'date', 'date_pre', 'date_pre2', 'date_next1']).drop_duplicates(subset=['date', 'ticker', 'report_period'], keep='last')\n",
    "    df3[col + '_TTM'] = df3[col + '_Q'] + df3[col + '_Q_pre'] + df3[col + '_Q_pre2'] + df3[col + '_Q_next1']\n",
    "    part2 = df3[['date', 'ticker', 'report_period', col + '_TTM']]\n",
    "    \n",
    "    # Part3\n",
    "    df3 = df4.merge(tool_data, left_on=['ticker', 'pre_report_period'], right_on=['ticker', 'report_period'], suffixes=['', '_pre'], how='left')\n",
    "    df3 = df3.merge(tool_data, left_on=['ticker', 'next1_report_period'], right_on=['ticker', 'report_period'], suffixes=['', '_next1'], how='left')\n",
    "    df3 = df3.merge(tool_data, left_on=['ticker', 'next2_report_period'], right_on=['ticker', 'report_period'], suffixes=['', '_next2'], how='left')\n",
    "    df3 = df3[(df3['date'] >= df3['date_pre']) & (df3['date'] >= df3['date_next1']) & (df3['date'] >= df3['date_next2'])]\n",
    "    df3['report_period'] = df3['report_period_next2']\n",
    "    df3 = df3.sort_values(['ticker', 'date', 'date_pre', 'date_next1', 'date_next2']).drop_duplicates(subset=['date', 'ticker', 'report_period'], keep='last')\n",
    "    df3[col + '_TTM'] = df3[col + '_Q'] + df3[col + '_Q_pre'] + df3[col + '_Q_next1'] + df3[col + '_Q_next2']\n",
    "    part3 = df3[['date', 'ticker', 'report_period', col + '_TTM']]\n",
    "    \n",
    "    # Part4\n",
    "    df3 = df4.merge(tool_data, left_on=['ticker', 'next1_report_period'], right_on=['ticker', 'report_period'], suffixes=['', '_next1'], how='left')\n",
    "    df3 = df3.merge(tool_data, left_on=['ticker', 'next2_report_period'], right_on=['ticker', 'report_period'], suffixes=['', '_next2'], how='left')\n",
    "    df3 = df3.merge(tool_data, left_on=['ticker', 'next3_report_period'], right_on=['ticker', 'report_period'], suffixes=['', '_next3'], how='left')\n",
    "    df3 = df3[(df3['date'] >= df3['date_next1']) & (df3['date'] >= df3['date_next2']) & (df3['date'] >= df3['date_next3'])]\n",
    "    df3['report_period'] = df3['report_period_next3']\n",
    "    df3 = df3.sort_values(['ticker', 'date', 'date_next1', 'date_next1', 'date_next1']).drop_duplicates(subset=['date', 'ticker', 'report_period'], keep='last')\n",
    "    df3[col + '_TTM'] = df3[col + '_Q'] + df3[col + '_Q_next1'] + df3[col + '_Q_next2'] + df3[col + '_Q_next3']\n",
    "    part4 = df3[['date', 'ticker', 'report_period', col + '_TTM']]\n",
    "    \n",
    "    part1234 = pd.concat([part1, part2, part3, part4])\n",
    "    part1234 = part1234.sort_values(['date', 'ticker', 'report_period'])\n",
    "    part1234 = part1234.drop_duplicates(subset=['date', 'ticker', 'report_period'], keep='last')\n",
    "    \n",
    "    # 计算TTM增长率\n",
    "    part1234['prev_' + col + '_TTM'] = part1234.groupby('ticker')[col + '_TTM'].shift(1)\n",
    "    part1234[col + '_TTM_growth'] = (part1234[col + '_TTM'] - part1234['prev_' + col + '_TTM']) / part1234['prev_' + col + '_TTM'] * 100\n",
    "    \n",
    "    return part1234[['date', 'ticker', 'report_period', col + '_TTM', col + '_TTM_growth']]\n",
    "\n",
    "# 示例数据\n",
    "data = {\n",
    "    'date': pd.date_range(start='2022-03-31', periods=8, freq='Q'),\n",
    "    'ticker': ['A'] * 8,\n",
    "    'report_period': pd.date_range(start='2022-03-31', periods=8, freq='Q'),\n",
    "    'revenue_Q': [100, 110, 120, 130, 140, 150, 160, 170],\n",
    "    'pre_report_period': pd.date_range(start='2021-12-31', periods=8, freq='Q'),\n",
    "    'pre2_report_period': pd.date_range(start='2021-09-30', periods=8, freq='Q'),\n",
    "    'pre3_report_period': pd.date_range(start='2021-06-30', periods=8, freq='Q'),\n",
    "    'next1_report_period': pd.date_range(start='2022-06-30', periods=8, freq='Q'),\n",
    "    'next2_report_period': pd.date_range(start='2022-09-30', periods=8, freq='Q'),\n",
    "    'next3_report_period': pd.date_range(start='2022-12-31', periods=8, freq='Q')\n",
    "}\n",
    "df4 = pd.DataFrame(data)\n",
    "\n",
    "result = compute_ttm_rate(df4, 'revenue')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "386e5ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_finfactor_and_save_data(df, col3, Open):\n",
    "    # 去重\n",
    "    df_processed = df.drop_duplicates(subset=['date', 'ticker'], keep='last')\n",
    "    \n",
    "    # 设置索引并unstack\n",
    "    df_processed = df_processed.set_index(['date', 'ticker'])[col3].unstack()\n",
    "    \n",
    "    # 合并标签数据并删除标签列\n",
    "    df_processed = pd.concat([df_processed, Open[['label']]], axis=1).drop(['label'], axis=1)\n",
    "    \n",
    "    # 填充缺失值并删除全为NaN的行和列\n",
    "    df_processed = df_processed.fillna(method='ffill', limit=150).dropna(how='all', axis=0).dropna(how='all', axis=1)\n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a972c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_extreme_by_MAD(series, n=3):\n",
    "    median = series.median()\n",
    "    median_new = abs(series - median).median()\n",
    "    max_value = median + n * median_new\n",
    "    min_value = median - n * median_new\n",
    "    return np.clip(series, min_value, max_value, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33ef1137",
   "metadata": {},
   "outputs": [],
   "source": [
    " def func(x):\n",
    "        x = x[x['report_period']>=x['report_period'].expanding(min_periods=1).max()]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc54ee77",
   "metadata": {},
   "source": [
    "# Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c5ab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "lncap = np.log(circle_mv)\n",
    "lncap.to_pickle('Z:/redata/Sen/lncap.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9591e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lncap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dbcfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lncap3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b363cc47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da94dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY = pd.merge(lncap, lncap3, on=['date', 'ticker'], how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d85ee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lncap3 = (lncap)**3\n",
    "lncap3.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "XY = pd.merge(lncap, lncap3, on=['date', 'ticker'], how='right')\n",
    "    #print(XY.head(3))\n",
    "XY = XY.dropna()\n",
    "tradeday = XY.date.unique()\n",
    "daily_resid = []\n",
    "for date in tqdm(tradeday[:]):\n",
    "    xy = XY[XY['date'] == date]\n",
    "        #只保留有因子值的数据\n",
    "    xy = xy.dropna() \n",
    "\n",
    "        #选出x y\n",
    "    x = xy.iloc[:,2:-1]\n",
    "    y = xy['factor']\n",
    "        #回归\n",
    "    multi_linear = LinearRegression(fit_intercept=False)  #无截距项\n",
    "    multi_linear.fit(x, y)\n",
    "    beta = multi_linear.coef_\n",
    "        \n",
    "        #取残差\n",
    "    xy['beta'] = np.dot(x, beta)\n",
    "    xy['resid'] = xy['factor'] - xy['beta']\n",
    "    daily_resid.append(xy)\n",
    "data = pd.concat(daily_resid)\n",
    "factor = data[['date', 'ticker', 'factor', 'resid']]\n",
    "return factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1545233",
   "metadata": {},
   "source": [
    "# BETA因子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a8e0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_500_ret = pd.DataFrame(idx_500_ret)\n",
    "idx_500_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e643c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "excess500 = adj_return - idx_500_ret\n",
    "excess500.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b9e59",
   "metadata": {},
   "source": [
    "# 财务因子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7b3abe2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#净利润同比增速\n",
    "#单季度\n",
    "col1 = 'net_profit'\n",
    "df_single_Q = single_Q(dfaa,col1)\n",
    "df_merge = df_single_Q.merge(mls,on=['report_period'],how='left')\n",
    "\n",
    "# ttm12月累计——QoQ环比增速——yoy同比增速——Diff当季-上季数值\n",
    "col2 = 'net_profit_Q'\n",
    "df_computed = compute_yoy(df_merge,col2)\n",
    "\n",
    "col3 = 'net_profit_Q_yoy'\n",
    "df_fin = process_finfactor_and_save_data(df_computed, col3, open)\n",
    "df_fin.to_pickle('D:/redata/factor/net_profit_Q_yoy@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "28f866c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#归母净利润同比增速\n",
    "\n",
    "#单季度\n",
    "col1 = 'net_profit_parent_company'\n",
    "df_single_Q = single_Q(dfaa,col1)\n",
    "df_merge = df_single_Q.merge(mls,on=['report_period'],how='left')\n",
    "\n",
    "# ttm12月累计——QoQ环比增速——yoy同比增速——Diff当季-上季数值\n",
    "col2 = 'net_profit_parent_company_Q'\n",
    "df_computed = compute_yoy(df_merge,col2)\n",
    "\n",
    "col3 = 'net_profit_parent_company_Q_yoy'\n",
    "df_fin = process_finfactor_and_save_data(df_computed, col3, open)\n",
    "df_fin.to_pickle('D:/redata/factor/net_profit_parent_company_Q_yoy@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a60845a6-b7f0-4eba-92a9-909b87540114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当季营业同比增长率\n",
    "\n",
    "#单季度\n",
    "col1 = 'operating_revenue'\n",
    "df_single_Q = single_Q(dfaa,col1)\n",
    "df_merge = df_single_Q.merge(mls,on=['report_period'],how='left')\n",
    "\n",
    "# ttm12月累计——QoQ环比增速——yoy同比增速——Diff当季-上季数值\n",
    "col2 = 'operating_revenue_Q'\n",
    "df_computed = compute_yoy(df_merge,col2)\n",
    "\n",
    "col3 = 'operating_revenue_Q_yoy'\n",
    "df_fin = process_finfactor_and_save_data(df_computed, col3, open)\n",
    "df_fin.to_pickle('D:/redata/factor/operating_revenue_Q_yoy@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "68b7bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 营业收入ttm增长率\n",
    "\n",
    "#单季度\n",
    "col1 = 'operating_revenue'\n",
    "df_single_Q = single_Q(dfaa,col1)\n",
    "df_merge = df_single_Q.merge(mls,on=['report_period'],how='left')\n",
    "\n",
    "# ttm12月累计——QoQ环比增速——yoy同比增速——Diff当季-上季数值\n",
    "col2 = 'operating_revenue'\n",
    "df_computed = compute_ttm(df_merge,col2)\n",
    "\n",
    "col3 = 'operating_revenue_TTM'\n",
    "df_fin = process_finfactor_and_save_data(df_computed, col3, open)\n",
    "df_fin.to_pickle('D:/redata/factor/operating_revenue_TTM@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aca446a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BP 股东权益/总市值\n",
    "\n",
    "#单季度\n",
    "col1 = 'equity_parent_company'\n",
    "df_singleQ = single_Q(dfcc,col1)\n",
    "\n",
    "col2 = 'equity_parent_company_Q'\n",
    "df_fin = process_finfactor_and_save_data(df_singleQ, col2, open)\n",
    "\n",
    "BP = df_fin / mc\n",
    "BP.to_pickle('D:/redata/factor/BP@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "34d2dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EP 因子\n",
    "\n",
    "#单季度\n",
    "col = 'net_profit'\n",
    "df_single_Q = single_Q(dfaa,col)\n",
    "\n",
    "col2 = 'net_profit_Q'\n",
    "df_fin = process_finfactor_and_save_data(df_single_Q, col2, open)\n",
    "\n",
    "EP = df_fin / mc\n",
    "EP.to_pickle('D:/redata/factor/EP@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "314d8693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CFP_ttm\n",
    "\n",
    "#单季度\n",
    "col = 'cash_flow_from_operating_activities'\n",
    "df_single_Q = single_Q(dfbb,col)\n",
    "df_merge = df_single_Q.merge(mlss,on=['report_period'],how='left')\n",
    "\n",
    "# ttm12月累计——QoQ环比增速——yoy同比增速——Diff当季-上季数值\n",
    "df_ttm = compute_ttm(df_merge,col)\n",
    "\n",
    "col1 = 'cash_flow_from_operating_activities_TTM'\n",
    "df_fin = process_finfactor_and_save_data(df_ttm, col1, open)\n",
    "\n",
    "cfp = df_fin/mc\n",
    "cfp.to_pickle('Z:/redata/Sen/cfp_ttm@factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e2dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增长率(本期-上期）_TTM 先做single_Q\n",
    "df6= df5.groupby('ticker').apply(func).reset_index(drop=True)\n",
    "    #factor同一个date ticker 去重，保留当前最新report_period\n",
    "df6 = df6.drop_duplicates(subset=['date','ticker'],keep='last')\n",
    "df6 = df6.sort_values(['ticker','report_period'])\n",
    "df6.set_index(['date','report_period'],inplace = True)\n",
    "# 计算增长率ttm\n",
    "df6grouped = df6.groupby('ticker').apply(lambda x:x.pct_change().rolling(4).sum())\n",
    "df6grouped['ticker'] = df6['ticker']\n",
    "df6grouped = df6grouped.dropna(how = 'all', axis = 0)\n",
    "df7 = df6grouped.reset_index()\n",
    "\n",
    "col = 'operating_revenue_TTM'\n",
    "\n",
    "df7 = df7.drop_duplicates(subset=['date','ticker'],keep='last')\n",
    "df7 = df7.set_index(['date','ticker'])[col].unstack()\n",
    "df7 = pd.concat([df7,Open[['label']]],axis=1).drop(['label'],axis=1)\n",
    "df7 = df7.fillna(method='ffill',limit=150).dropna(how='all',axis=0).dropna(how='all',axis=1)\n",
    "\n",
    "df7.to_pickle('Z:/redata/Sen/operating_revenue_rate_TTM_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a9d58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a222b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df5.loc[:'2022-10-22']\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1249174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 营业外收支利润净额/利润总额\n",
    "col = 'non_operating_revenue'\n",
    "\n",
    "#单季度\n",
    "df1 = single_Q(dfaa,col)\n",
    "\n",
    "col = 'non_operating_revenue_Q'\n",
    "df1 = df1.drop_duplicates(subset=['date','ticker'],keep='last')\n",
    "df1 = df1.set_index(['date','ticker'])[col].unstack()\n",
    "df1 = pd.concat([df1,Open[['label']]],axis=1).drop(['label'],axis=1)\n",
    "df1 = df1.fillna(method='ffill',limit=150).dropna(how='all',axis=0).dropna(how='all',axis=1)\n",
    "\n",
    "col = 'non_operating_expense'\n",
    "\n",
    "#单季度\n",
    "df2 = single_Q(dfaa,col)\n",
    "\n",
    "col = 'non_operating_expense_Q'\n",
    "df2 = df2.drop_duplicates(subset=['date','ticker'],keep='last')\n",
    "df2 = df2.set_index(['date','ticker'])[col].unstack()\n",
    "df2 = pd.concat([df2,Open[['label']]],axis=1).drop(['label'],axis=1)\n",
    "df2 = df2.fillna(method='ffill',limit=150).dropna(how='all',axis=0).dropna(how='all',axis=1)\n",
    "\n",
    "col = 'profit_before_tax'\n",
    "\n",
    "#单季度\n",
    "df3 = single_Q(dfaa,col)\n",
    "\n",
    "col = 'profit_before_tax_Q'\n",
    "df3 = df3.drop_duplicates(subset=['date','ticker'],keep='last')\n",
    "df3 = df3.set_index(['date','ticker'])[col].unstack()\n",
    "df3 = pd.concat([df3,Open[['label']]],axis=1).drop(['label'],axis=1)\n",
    "df3 = df3.fillna(method='ffill',limit=150).dropna(how='all',axis=0).dropna(how='all',axis=1)\n",
    "\n",
    "df4 = (df1 - df2)/df3\n",
    "df4 = df4.replace(0, np.nan)\n",
    "df4 = filter_extreme_by_MAD(df4)\n",
    "\n",
    "NNOP = -df4\n",
    "NNOP.to_pickle('Z:/redata/Sen/NNOP_factor.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b0a9f2",
   "metadata": {},
   "source": [
    "# 量价因子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eaa7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "AOG = (adj_open.shift(-1)/adj_close).sub(mopen.shift(-1) / mclose,axis=0)\n",
    "AOG.to_pickle('Z:/redata/Sen/AOG_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b61a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "JOR = (adj_low.shift(-1)/adj_close).sub(mlow.shift(-1)/mclose,axis=0)\n",
    "JOR.to_pickle('Z:/redata/Sen/JOR_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1074fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "OverNight = (adj_open/adj_close.shift(1)).astype('float')\n",
    "IntraDay = (adj_close/adj_open).astype('float')\n",
    "\n",
    "ON20 = np.log(OverNight).rolling(20).sum()\n",
    "IN20 = np.log(IntraDay).rolling(20).sum()\n",
    "\n",
    "ON20.to_pickle('Z:/redata/Sen/ON20_factor.pkl')\n",
    "IN20.to_pickle('Z:/redata/Sen/IN20_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c65ae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mom6m = (1+ adj_return.copy()).rolling(window=125).apply(lambda x: x.prod()-1, raw=True).shift(20)\n",
    "Mom6m.to_pickle('Z:/redata/Sen/Mom6m_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918cc7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mom12m = (1+ adj_return.copy()).rolling(window=250).apply(lambda x: x.prod()-1, raw=True).shift(20)\n",
    "# Mom12m = Mom12m.rolling(20).mean() #平滑处理，换手率>20%\n",
    "Mom12m.to_pickle('Z:/redata/Sen/Mom12m_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee533820",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mom36m = (1+ adj_return.copy()).rolling(window=750).apply(lambda x: x.prod()-1, raw=True).shift(250)\n",
    "# Mom36m = Mom36m.rolling(20).mean() #平滑\n",
    "Mom36m.to_pickle('Z:/redata/Sen/Mom36m_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3528f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxRet = adj_return.copy().rolling(window=20).max().shift(1)\n",
    "MaxRet.to_pickle('Z:/redata/Sen/MaxRet_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937e0efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_turn = turnover_rate.rolling(window=5, closed='left').std().shift(1)\n",
    "std_turn = std_turn.rolling(10).mean()\n",
    "std_turn.to_pickle('Z:/redata/Sen/std_turn_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e00ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VolSD = volume.rolling(window = 750, min_periods=24, closed='left').std()\n",
    "VolSD.to_pickle('Z:/redata/Sen/VolSD_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d50b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "Momss=adj_return.copy().rolling(window=20).apply(lambda x: (1+x).prod()-1, raw=True).shift(250)\n",
    "Momss=Momss.rolling(20).mean()\n",
    "Momss.to_pickle('Z:/redata/Sen/Momss_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1dd43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "turnover_std = -(turnover_rate.rolling(window=20).std())\n",
    "\n",
    "turnover_std = filter_extreme_by_MAD(turnover_std)\n",
    "turnover_std.to_pickle('Z:/redata/Sen/turnover_std_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a26bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "turnrate_mean = -(turnover_rate.rolling(window=20).mean())\n",
    "turnrate_mean.to_pickle('Z:/redata/Sen/turnrate_mean_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d531d8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv = -( volume.rolling(window=20).std())\n",
    "\n",
    "mv = filter_extreme_by_MAD(mv)\n",
    "mv.to_pickle('Z:/redata/Sen/mv_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf61af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha95 = -(volume.rolling(5).std())\n",
    "alpha95.to_pickle('Z:/redata/Sen/alpha95_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16626bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret20 = -1*(adj_return.rolling(window=20).sum())\n",
    "ret20.to_pickle('Z:/redata/Sen/ret_20_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f408b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "retstd20 = -1*(adj_return.rolling(window=20).std())\n",
    "retstd20.to_pickle('Z:/redata/Sen/retstd20_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c202742",
   "metadata": {},
   "outputs": [],
   "source": [
    "maketcap = np.log(adj_close * total_amount)\n",
    "maketcap.to_pickle('Z:/redata/Sen/maketcap_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3dfb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_amount_5 = total_amount.rolling(5).std() * -1\n",
    "total_amount_5.to_pickle('Z:/redata/Sen/total_amount_5_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(X):\n",
    "    return X.rank(axis=1, method='min', pct = True)\n",
    "\n",
    "def delay(X, d):\n",
    "    return X.shift(d)\n",
    "\n",
    "def correlation(X, Y, d):\n",
    "    return X.rolling(window=d).corr(Y.rolling(window=d))\n",
    "\n",
    "def covariance(X, Y, d):\n",
    "    return X.rolling(window=d).cov(Y.rolling(window=d))\n",
    "\n",
    "def ts_max(X, d):\n",
    "    return X.rolling(window=d).max()\n",
    "\n",
    "def ts_min(X, d):\n",
    "    return X.rolling(window=d).min()\n",
    "\n",
    "alpha3 = -1 * correlation(rank(adj_open), rank(volume), 10)\n",
    "alpha3.to_pickle('Z:/redata/Sen/alpha3_factor.pkl')\n",
    "\n",
    "alpha13 = -1 * rank(covariance(rank(adj_close), rank(volume), 20))\n",
    "# alpha13 = alpha13.rolling(60).mean()\n",
    "alpha13.to_pickle('Z:/redata/Sen/alpha13_factor.pkl')\n",
    "\n",
    "alpha15 = -1 * rank(correlation(rank(adj_high), rank(volume), 3)).rolling(window=3).sum()\n",
    "# alpha15 = alpha15.rolling(60).mean()\n",
    "alpha15.to_pickle('Z:/redata/Sen/alpha15_factor.pkl')\n",
    "\n",
    "alpha16 = -1 * rank(covariance(rank(adj_high), rank(volume), 5))\n",
    "alpha16.to_pickle('Z:/redata/Sen/alpha16_factor.pkl')\n",
    "\n",
    "alpha44 = -1 * correlation(adj_high, rank(volume), 5)\n",
    "alpha44.to_pickle('Z:/redata/Sen/alpha44_factor.pkl')\n",
    "\n",
    "alpha50 = -1 * ts_max(rank(correlation(rank(volume), rank(adj_close), 5)), 5)\n",
    "alpha50.to_pickle('Z:/redata/Sen/alpha50_factor.pkl')\n",
    "\n",
    "close_normalized = (adj_close - adj_low.rolling(12).min()) / (adj_high.rolling(12).max()- adj_low.rolling(12).min())\n",
    "alpha55 = -1 * correlation(rank(close_normalized), rank(volume), 6)\n",
    "alpha55.to_pickle('Z:/redata/Sen/alpha55_factor.pkl')\n",
    "\n",
    "Var120 = -(adj_return.rolling(window = 121).var())\n",
    "Var120 = filter_extreme_by_MAD(Var120)\n",
    "Var120.to_pickle('Z:/redata/Sen/Var120_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e215ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "liquidity = -1 * (0.25 * turnover_rate.rolling(20).mean() + 0.35 * turnover_rate.rolling(60).mean() + 0.3 * turnover_rate.rolling(252).mean())\n",
    "liquidity.to_pickle('Z:/redata/Sen/liquidity_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d7a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#买卖压力撮合因子\n",
    "\n",
    "vwap = total_turnover / volume\n",
    "\n",
    "def apb(T):\n",
    "    a = vwap.rolling(window=T).mean()\n",
    "    b = (volume * vwap).rolling(window=T).sum()\n",
    "    c = volume.rolling(window=T).sum()\n",
    "    apb = np.log((a)/(b/c))\n",
    "    return apb\n",
    "\n",
    "APB_5 = apb(T=5)\n",
    "APB_5 = APB_5.rolling(20).mean()\n",
    "APB_5 = filter_extreme_by_MAD(APB_5)\n",
    "APB_5.to_pickle('Z:/redata/Sen/APB_5_factor.pkl')\n",
    "\n",
    "APB_20 = (apb(T=20)).rolling(20).mean()\n",
    "APB_20 = filter_extreme_by_MAD(APB_20)\n",
    "APB_20.to_pickle('Z:/redata/Sen/APB_20_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dfd481",
   "metadata": {},
   "outputs": [],
   "source": [
    "vwap = total_turnover / volume\n",
    "n_days = 20\n",
    "a = vwap.rolling(window=n_days,min_periods=int(n_days/2)).mean()\n",
    "b = (volume * vwap).rolling(window=n_days,min_periods=int(n_days/2)).mean()\n",
    "c = volume.rolling(window=n_days,min_periods=int(n_days/2)).mean()\n",
    "APB_1 = np.log((a/(b/c)))\n",
    "APB_1 = APB_1.rolling(20).mean()\n",
    "APB_1.to_pickle('Z:/redata/Sen/APB_1_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d512049",
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = 0.5 ** (1/42)\n",
    "daily_standard_deviation = np.sqrt(wt * ((adj_return - adj_return.rolling(252).mean())**2).rolling(252).sum())\n",
    "daily_standard_deviation.to_pickle('Z:/redata/Sen/daily_standard_deviation_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9226f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "high1 = adj_high\n",
    "low1 = adj_low\n",
    "close1 = adj_close\n",
    "\n",
    "high_r_std_Nm = (high1 / close1.shift(1)).rolling(120).std()\n",
    "low_r_std_Nm = (low1 / close1.shift(1)).rolling(120).std()\n",
    "\n",
    "hml_r_std_Nm = (high_r_std_Nm - low_r_std_Nm) * -1\n",
    "hpl_r_std_Nm = (high_r_std_Nm + low_r_std_Nm) * -1\n",
    "\n",
    "hml_r_std_Nm = filter_extreme_by_MAD(hml_r_std_Nm)\n",
    "hpl_r_std_Nm = filter_extreme_by_MAD(hpl_r_std_Nm)\n",
    "\n",
    "hml_r_std_Nm.to_pickle('Z:/redata/Sen/hml_r_std_6m_factor.pkl')\n",
    "hpl_r_std_Nm.to_pickle('Z:/redata/Sen/hpl_r_std_6m_factor.pkl')\n",
    "\n",
    "high_r_std_N = (high1 / close1.shift(1)).rolling(100).std()\n",
    "high_r_std_N = -1 * high_r_std_Nm\n",
    "high_r_std_N = filter_extreme_by_MAD(high_r_std_N)\n",
    "high_r_std_N.to_pickle('Z:/redata/Sen/high_r_std_5m_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d316d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_marketcap = -mc\n",
    "total_marketcap.to_pickle('Z:/redata/Sen/total_marketcap_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c8d402",
   "metadata": {},
   "outputs": [],
   "source": [
    "cir_mv = -circle_mv\n",
    "cir_mv.to_pickle('Z:/redata/Sen/cir_mv_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cce4197",
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_5 = adj_close.rolling(5).mean() * -1\n",
    "bias_5 = (adj_close/ma_5) - 1\n",
    "bias_5.to_pickle('Z:/redata/Sen/bias_5_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064401ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjclose = adj_close * -1\n",
    "adjclose.to_pickle('Z:/redata/Sen/adjclose_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9654a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "uppershadow = adj_high - np.maximum(adj_open, adj_close)\n",
    "lowershadow = np.minimum(adj_open, adj_close) - adj_low\n",
    "_stduppershadow = uppershadow / uppershadow.rolling(5).mean().shift(1)\n",
    "_stdlowershadow = lowershadow / lowershadow.rolling(5).mean().shift(1)\n",
    "\n",
    "william_uppershadow = adj_high - adj_close\n",
    "william_lowershadow = adj_close - adj_low\n",
    "_stdwilliam_uppershadow = william_uppershadow / william_uppershadow.rolling(5).mean().shift(1)\n",
    "_stdwilliam_lowershadow = william_lowershadow / william_lowershadow.rolling(5).mean().shift(1)\n",
    "\n",
    "uppershadow_std = _stduppershadow.rolling(20).std()\n",
    "william_lowershadow_mean = _stdwilliam_lowershadow.rolling(20).mean()\n",
    "\n",
    "uppershadow_std.to_pickle('Z:/redata/Sen/uppershadow_std_factor.pkl')\n",
    "william_lowershadow_mean.to_pickle('Z:/redata/Sen/william_lowershadow_mean_factor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6416f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 采用等权方式构造UBL因子\n",
    "UBL = (uppershadow_std + william_lowershadow_mean)*-1\n",
    "UBL = UBL.rolling(20).mean()\n",
    "UBL.to_pickle('Z:/redata/Sen/UBL_factor.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04cd925",
   "metadata": {},
   "source": [
    "# 行业市值中性化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "78ec39b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_condition(new_stk=20):\n",
    "    drop_new_stk = adj_open.rolling(window=new_stk, min_periods=new_stk).mean()\n",
    "    drop_new_stk = ~np.isnan(drop_new_stk)\n",
    "    df = st_status * halt_status * drop_new_stk\n",
    "    df = df.replace(True, 1).replace(False, np.nan)\n",
    "    return df.loc['2017':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e387fa8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1558/1558 [01:08<00:00, 22.64it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1616/1616 [01:13<00:00, 22.10it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1616/1616 [01:12<00:00, 22.41it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1616/1616 [01:14<00:00, 21.76it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1616/1616 [01:14<00:00, 21.69it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1616/1616 [01:13<00:00, 21.85it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1616/1616 [01:13<00:00, 21.92it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1616/1616 [01:11<00:00, 22.74it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1616/1616 [01:13<00:00, 21.97it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1616/1616 [01:13<00:00, 21.85it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1616/1616 [01:13<00:00, 21.93it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1558/1558 [01:08<00:00, 22.69it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1616/1616 [01:14<00:00, 21.55it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1616/1616 [01:15<00:00, 21.41it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1616/1616 [01:15<00:00, 21.37it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1616/1616 [01:15<00:00, 21.44it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1616/1616 [01:16<00:00, 21.23it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1616/1616 [01:16<00:00, 21.19it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_factor(factor_path, output_path, final_universe, risk, risk_cols):\n",
    "    df = pd.read_pickle(factor_path)\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df_clean = df * final_universe\n",
    "    df_clean = df_clean.dropna(how='all', axis=0)\n",
    "    factor_type = 'ind'\n",
    "    factor_rank = factor_neutralize(df_clean, factor_type, if_rank=True, risk=risk, risk_cols=risk_cols)\n",
    "    factor_rank.to_pickle(output_path)\n",
    "\n",
    "# Assuming final_universe and risk are defined elsewhere in your code\n",
    "final_universe = compute_condition(new_stk = 10) # Define your final_universe DataFrame\n",
    "risk = risk  # Define your risk DataFrame\n",
    "risk_cols = ['logcmv', 'retsum120']\n",
    "\n",
    "# List of factors and their corresponding paths\n",
    "factors = [\n",
    "    ('Z:/Sen/cfp_ttm_factor.pkl', 'Z:/Sen/Neutraliza/cfp_ttm_nu.pkl'),\n",
    "    ('Z:/Sen/EP_factor.pkl', 'Z:/Sen/Neutraliza/EP_nu.pkl'),\n",
    "    ('Z:/Sen/net_profit_Q_yoy_factor.pkl', 'Z:/Sen/Neutraliza/net_profit_Q_yoy_nu.pkl'),\n",
    "    ('Z:/Sen/turnover_std_factor.pkl', 'Z:/Sen/Neutraliza/turnover_std_nu.pkl'),\n",
    "    ('Z:/Sen/mv_factor.pkl', 'Z:/Sen/Neutraliza/mv_nu.pkl'),\n",
    "    ('Z:/Sen/Var120_factor.pkl', 'Z:/Sen/Neutraliza/Var120_nu.pkl'),\n",
    "    ('Z:/Sen/hml_r_std_6m_factor.pkl', 'Z:/Sen/Neutraliza/hml_r_std_6m_nu.pkl'),\n",
    "    ('Z:/Sen/liquidity_factor.pkl', 'Z:/Sen/Neutraliza/liquidity_nu.pkl'),\n",
    "    ('Z:/Sen/APB_20_factor.pkl', 'Z:/Sen/Neutraliza/APB_20_nu.pkl'),\n",
    "    ('Z:/Sen/high_r_std_5m_factor.pkl','Z:/Sen/Neutraliza/high_r_std_5m_nu.pkl'),\n",
    "    ('Z:/Sen/hpl_r_std_6m_factor.pkl','Z:/Sen/Neutraliza/hpl_r_std_6m_nu.pkl'),\n",
    "    ('Z:/Sen/BP_factor.pkl','Z:/Sen/Neutraliza/BP_nu.pkl'),\n",
    "    ('Z:/Sen/total_marketcap_factor.pkl','Z:/Sen/Neutraliza/total_marketcap_nu.pkl'),\n",
    "    ('Z:/Sen/cir_mv_factor.pkl','Z:/Sen/Neutraliza/cir_mv_nu.pkl'),\n",
    "    ('Z:/Sen/ret_20_factor.pkl','Z:/Sen/Neutraliza/ret_20_nu.pkl'),\n",
    "    ('Z:/Sen/bias_5_factor.pkl','Z:/Sen/Neutraliza/bias_5_nu.pkl'),\n",
    "    ('Z:/Sen/total_amount_5_factor.pkl','Z:/Sen/Neutraliza/total_amount_5_nu.pkl'),\n",
    "    ('Z:/Sen/adjclose_factor.pkl','Z:/Sen/Neutraliza/adjclose_nu.pkl')\n",
    "]\n",
    "\n",
    "# Process each factor\n",
    "for factor_path, output_path in factors:\n",
    "    process_factor(factor_path, output_path, final_universe, risk, risk_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0e41061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_IC(factor_rank, ret_rank):\n",
    "    IC = (factor_rank.corrwith(ret_rank, axis=1)).dropna()\n",
    "    IC_mean = IC.mean()\n",
    "    IC_std = IC.std()\n",
    "    ICIR = IC_mean / IC_std\n",
    "    ICres = pd.DataFrame([[IC_mean, ICIR]], columns=['ICmean', 'ICIR'])\n",
    "    ICIR_120_weight = IC.rolling(120).apply(lambda x: x.mean() / x.std()).dropna()\n",
    "    return IC, ICres, ICIR_120_weight\n",
    "\n",
    "# Assuming factor_rank and ret_rank are defined elsewhere in your code\n",
    "# factor_rank = ... # Define your factor_rank DataFrame\n",
    "ret_rank = o1o2.rank(axis=1, ascending=True)\n",
    "\n",
    "factors_nu = [\n",
    "('D:/redata/Neutraliza/cfp_ttm_nu.pkl', 'D:/redata/ICIR/cfp_ttm_ICIR.pkl'),\n",
    "('D:/redata/Neutraliza/EP_nu.pkl', 'D:/redata/ICIR/EP_ICIR.pkl'),\n",
    "('D:/redata/Neutraliza/net_profit_Q_yoy_nu.pkl', 'D:/redata/ICIR/net_profit_Q_yoy_ICIR.pkl'),\n",
    "('D:/redata/Neutraliza/turnover_std_nu.pkl', 'D:/redata/ICIR/turnover_std_ICIR.pkl'),\n",
    "('D:/redata/Neutraliza/mv_nu.pkl', 'D:/redata/ICIR/mv_ICIR.pkl'),\n",
    "('D:/redata/Neutraliza/Var120_nu.pkl', 'D:/redata/ICIR/Var120_ICIR.pkl'),\n",
    "('D:/redata/Neutraliza/hml_r_std_6m_nu.pkl', 'D:/redata/ICIR/hml_r_std_6m_ICIR.pkl'),\n",
    "('D:/redata/Neutraliza/liquidity_nu.pkl', 'D:/redata/ICIR/liquidity_ICIR.pkl'),\n",
    "('D:/redata/Neutraliza/APB_20_nu.pkl', 'D:/redata/ICIR/APB_20_ICIR.pkl'), \n",
    "('D:/redata/Neutraliza/high_r_std_5m_nu.pkl','D:/redata/ICIR/high_r_std_5m_ICIR.pkl'),\n",
    "('D:/redata/Neutraliza/hpl_r_std_6m_nu.pkl','D:/redata/ICIR/hpl_r_std_6m_ICIR.pkl'),\n",
    "('D:/redata/Neutraliza/BP_nu.pkl','D:/redata/ICIR/BP_ICIR.pkl'),\n",
    "('D:/redata/Neutraliza/total_marketcap_nu.pkl','D:/redata/ICIR/total_marketcap_ICIR.pkl'),\n",
    "('D:/redata/Neutraliza/cir_mv_nu.pkl','D:/redata/ICIR/cir_mv_ICIR.pkl'),\n",
    "('D:/redata/Neutraliza/ret_20_nu.pkl','D:/redata/ICIR/ret_20_ICIR.pkl'),\n",
    "('D:/redata/Neutraliza/bias_5_nu.pkl','D:/redata/ICIR/bias_5_ICIR.pkl'),\n",
    "('D:/redata/Neutraliza/total_amount_5_nu.pkl','D:/redata/ICIR/total_amount_5_ICIR.pkl'),\n",
    "('D:/redata/Neutraliza/adjclose_nu.pkl','D:/redata/ICIR/adjclose_ICIR.pkl')\n",
    "]\n",
    "\n",
    "# Process each factor\n",
    "for factor_path, output_path in factors_nu:\n",
    "    factor_rank = pd.read_pickle(factor_path)\n",
    "# Assuming ret_rank is defined and available\n",
    "    IC, ICres, ICIR_120_weight = compute_IC(factor_rank, ret_rank)\n",
    "# Optionally save ICIR_120_weight if needed\n",
    "    ICIR_120_weight.to_pickle(output_path.replace('.pkl', '_120_weight.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2f99af02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of file paths\n",
    "f_files = [\n",
    "    'D:/redata/Neutraliza/cfp_ttm_nu.pkl',\n",
    "    'D:/redata/Neutraliza/EP_nu.pkl',\n",
    "    'D:/redata/Neutraliza/net_profit_Q_yoy_nu.pkl',\n",
    "    'D:/redata/Neutraliza/turnover_std_nu.pkl',\n",
    "    'D:/redata/Neutraliza/mv_nu.pkl',\n",
    "    'D:/redata/Neutraliza/Var120_nu.pkl',\n",
    "    'D:/redata/Neutraliza/hml_r_std_6m_nu.pkl',\n",
    "    'D:/redata/Neutraliza/liquidity_nu.pkl',\n",
    "    'D:/redata/Neutraliza/APB_20_nu.pkl',\n",
    "    'D:/redata/Neutraliza/high_r_std_5m_nu.pkl',\n",
    "    'D:/redata/Neutraliza/hpl_r_std_6m_nu.pkl',\n",
    "    'D:/redata/Neutraliza/BP_nu.pkl',\n",
    "    'D:/redata/Neutraliza/total_marketcap_nu.pkl',\n",
    "    'D:/redata/Neutraliza/cir_mv_nu.pkl',\n",
    "    'D:/redata/Neutraliza/ret_20_nu.pkl',\n",
    "    'D:/redata/Neutraliza/bias_5_nu.pkl',\n",
    "    'D:/redata/Neutraliza/total_amount_5_nu.pkl',\n",
    "    'D:/redata/Neutraliza/adjclose_nu.pkl'\n",
    "]\n",
    "\n",
    "i_files = [\n",
    "    'D:/redata/ICIR/cfp_ttm_ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/EP_ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/net_profit_Q_yoy_ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/turnover_std_ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/mv_ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/Var120_ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/hml_r_std_6m_ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/liquidity_ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/APB_20_ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/high_r_std_5m_ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/hpl_r_std_6m_ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/BP_ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/total_marketcap_ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/cir_mv_ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/ret_20_ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/bias_5_ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/total_amount_5_ICIR_120_weight.pkl',\n",
    "    'D:/redata/ICIR/adjclose_ICIR_120_weight.pkl'\n",
    "]\n",
    "\n",
    "o_files = [\n",
    "    'D:/redata/CombineFactors/cfp_ttm_ICIR_combine.pkl',\n",
    "    'D:/redata/CombineFactors/EP_ICIR_combine.pkl',\n",
    "    'D:/redata/CombineFactors/net_profit_Q_yoy_ICIR_combine.pkl',\n",
    "    'D:/redata/CombineFactors/turnover_std_ICIR_combine.pkl',\n",
    "    'D:/redata/CombineFactors/mv_ICIR_combine.pkl',\n",
    "    'D:/redata/CombineFactors/Var120_ICIR_combine.pkl',\n",
    "    'D:/redata/CombineFactors/hml_r_std_6m_ICIR_combine.pkl',\n",
    "    'D:/redata/CombineFactors/liquidity_ICIR_combine.pkl',\n",
    "    'D:/redata/CombineFactors/APB_20_ICIR_combine.pkl',\n",
    "    'D:/redata/CombineFactors/high_r_std_5m_ICIR_combine.pkl',\n",
    "    'D:/redata/CombineFactors/hpl_r_std_6m_ICIR_combine.pkl',\n",
    "    'D:/redata/CombineFactors/BP_ICIR_combine.pkl',\n",
    "    'D:/redata/CombineFactors/total_marketcap_ICIR_combine.pkl',\n",
    "    'D:/redata/CombineFactors/cir_mv_ICIR_combine.pkl',\n",
    "    'D:/redata/CombineFactors/ret_20_ICIR_combine.pkl',\n",
    "    'D:/redata/CombineFactors/bias_5_ICIR_combine.pkl',\n",
    "    'D:/redata/CombineFactors/total_amount_5_ICIR_combine.pkl',\n",
    "    'D:/redata/CombineFactors/adjclose_ICIR_combine.pkl'\n",
    "]\n",
    "\n",
    "# Read f files\n",
    "f_dataframes = [pd.read_pickle(file) for file in f_files]\n",
    "\n",
    "# Read i files and convert to DataFrame\n",
    "i_dataframes = []\n",
    "for file in i_files:\n",
    "    df = pd.read_pickle(file)\n",
    "    df = pd.DataFrame(df)\n",
    "    df.columns = ['ICIR']\n",
    "    i_dataframes.append(df)\n",
    "\n",
    "# Perform multiplication and save results\n",
    "results = []\n",
    "for f_df, i_df, o_file in zip(f_dataframes, i_dataframes, o_files):\n",
    "    result = f_df.mul(i_df['ICIR'], axis=0)\n",
    "    result.to_pickle(o_file)\n",
    "    results.append(result)\n",
    "\n",
    "# Now results list contains all multiplication operation后的 DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f6887fe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f1 = pd.read_pickle( 'D:/redata/CombineFactors/cfp_ttm_ICIR_combine.pkl')\n",
    "f2 = pd.read_pickle( 'D:/redata/CombineFactors/EP_ICIR_combine.pkl')\n",
    "f3 = pd.read_pickle( 'D:/redata/CombineFactors/net_profit_Q_yoy_ICIR_combine.pkl')\n",
    "f4 = pd.read_pickle( 'D:/redata/CombineFactors/turnover_std_ICIR_combine.pkl')\n",
    "f5 = pd.read_pickle( 'D:/redata/CombineFactors/mv_ICIR_combine.pkl')\n",
    "f6 = pd.read_pickle( 'D:/redata/CombineFactors/Var120_ICIR_combine.pkl')\n",
    "f7 = pd.read_pickle( 'D:/redata/CombineFactors/hml_r_std_6m_ICIR_combine.pkl')\n",
    "f8 = pd.read_pickle( 'D:/redata/CombineFactors/liquidity_ICIR_combine.pkl')\n",
    "f9 = pd.read_pickle( 'D:/redata/CombineFactors/APB_20_ICIR_combine.pkl')\n",
    "f10 = pd.read_pickle( 'D:/redata/CombineFactors/high_r_std_5m_ICIR_combine.pkl')\n",
    "f11 = pd.read_pickle( 'D:/redata/CombineFactors/hpl_r_std_6m_ICIR_combine.pkl')\n",
    "f12 = pd.read_pickle( 'D:/redata/CombineFactors/BP_ICIR_combine.pkl')\n",
    "f13 = pd.read_pickle( 'D:/redata/CombineFactors/total_marketcap_ICIR_combine.pkl')\n",
    "f14 = pd.read_pickle( 'D:/redata/CombineFactors/cir_mv_ICIR_combine.pkl')\n",
    "f15 = pd.read_pickle( 'D:/redata/CombineFactors/ret_20_ICIR_combine.pkl')\n",
    "f16 = pd.read_pickle( 'D:/redata/CombineFactors/bias_5_ICIR_combine.pkl')\n",
    "f17 = pd.read_pickle( 'D:/redata/CombineFactors/total_amount_5_ICIR_combine.pkl')\n",
    "f18 = pd.read_pickle( 'D:/redata/CombineFactors/adjclose_ICIR_combine.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0387024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor_neutralize(df, factor_type, if_rank=True, risk=None, risk_cols=None):\n",
    "    if if_rank:\n",
    "        df = df.rank(axis=1, ascending=True)  # 原值越大rank值越大\n",
    "    if factor_type == 'hist':\n",
    "        factor_rank = df.copy()\n",
    "    elif factor_type == 'ind':\n",
    "        def neu_process(df):\n",
    "            df = df.stack().reset_index()\n",
    "            df.columns = ['date', 'ticker', 'factor']\n",
    "            df = neturalize_ind(df, risk, risk_cols)\n",
    "            resid = df.set_index(['date', 'ticker'])['resid'].unstack()\n",
    "            return resid\n",
    "        factor_rank = neu_process(df)\n",
    "        factor_rank = factor_rank.rank(axis=1, ascending=True)\n",
    "    return factor_rank\n",
    "\n",
    "def compute_condition(new_stk=20):\n",
    "    drop_new_stk = adj_open.rolling(window=new_stk, min_periods=new_stk).mean()\n",
    "    drop_new_stk = ~np.isnan(drop_new_stk)\n",
    "    df = st_status * halt_status * drop_new_stk\n",
    "    df = df.replace(True, 1).replace(False, np.nan)\n",
    "    return df.loc['2017':]\n",
    "\n",
    "def compute_IC(factor_rank, ret_rank):\n",
    "    IC = (factor_rank.corrwith(ret_rank, axis=1)).dropna()\n",
    "    IC_mean = IC.mean()\n",
    "    IC_std = IC.std()\n",
    "    ICIR = IC_mean / IC_std\n",
    "    ICres = pd.DataFrame([[IC_mean, ICIR]], columns=['ICmean', 'ICIR'])\n",
    "    ICIR_120_weight = IC.rolling(120).apply(lambda x: x.mean() / x.std()).dropna()\n",
    "    return IC, ICres, ICIR_120_weight\n",
    "\n",
    "final_universe = compute_condition(new_stk=10)\n",
    "risk = risk\n",
    "risk_cols = ['logcmv', 'retsum120']\n",
    "\n",
    "factors = [\n",
    "    ('D:/redata/factor/cfp_ttm_factor.pkl', 'D:/redata/Neutraliza/cfp_ttm_nu.pkl'),\n",
    "    ('D:/redata/factor/EP_factor.pkl', 'D:/redata/Neutraliza/EP_nu.pkl'),\n",
    "    ('D:/redata/factor/net_profit_Q_yoy_factor.pkl', 'D:/redata/Neutraliza/net_profit_Q_yoy_nu.pkl'),\n",
    "    ('D:/redata/factor/turnover_std_factor.pkl', 'Z:/redata/Neutraliza/turnover_std_nu.pkl'),\n",
    "    ('D:/redata/factor/mv_factor.pkl', 'D:/redata/Neutraliza/mv_nu.pkl'),\n",
    "    ('D:/redata/factor/Var120_factor.pkl', 'D:/redata/Neutraliza/Var120_nu.pkl'),\n",
    "    ('D:/redata/factor/hml_r_std_6m_factor.pkl', 'D:/redata/Neutraliza/hml_r_std_6m_nu.pkl'),\n",
    "    ('D:/redata/factor/liquidity_factor.pkl', 'D:/redata/Neutraliza/liquidity_nu.pkl'),\n",
    "    ('D:/redata/factor/APB_20_factor.pkl', 'D:/redata/Neutraliza/APB_20_nu.pkl'),\n",
    "    ('D:/redata/factor/high_r_std_5m_factor.pkl', 'D:/redata/Neutraliza/high_r_std_5m_nu.pkl'),\n",
    "    ('D:/redata/factor/hpl_r_std_6m_factor.pkl', 'D:/redata/Neutraliza/hpl_r_std_6m_nu.pkl'),\n",
    "    ('D:/redata/factor/BP_factor.pkl', 'D:/redata/Neutraliza/BP_nu.pkl'),\n",
    "    ('D:/redata/factor/total_marketcap_factor.pkl', 'D:/redata/Neutraliza/total_marketcap_nu.pkl'),\n",
    "    ('D:/redata/factor/cir_mv_factor.pkl', 'D:/redata/Neutraliza/cir_mv_nu.pkl'),\n",
    "    ('D:/redata/factor/ret_20_factor.pkl', 'D:/redata/Neutraliza/ret_20_nu.pkl'),\n",
    "    ('D:/redata/factor/bias_5_factor.pkl', 'D:/redata/Neutraliza/bias_5_nu.pkl'),\n",
    "    ('D:/redata/factor/total_amount_5_factor.pkl', 'D:/redata/Neutraliza/total_amount_5_nu.pkl'),\n",
    "    ('D:/redata/factor/adjclose_factor.pkl', 'D:/redata/Neutraliza/adjclose_nu.pkl')\n",
    "]\n",
    "\n",
    "for factor_path, output_path in factors:\n",
    "    df = pd.read_pickle(factor_path)\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df_clean = df * final_universe\n",
    "    df_clean = df_clean.dropna(how='all', axis=0)\n",
    "    factor_type = 'ind'\n",
    "    factor_rank = factor_neutralize(df_clean, factor_type, if_rank=True, risk=risk, risk_cols=risk_cols)\n",
    "    factor_rank.to_pickle(output_path)\n",
    "\n",
    "ret_rank = o1o2.rank(axis=1, ascending=True)\n",
    "\n",
    "factors_rank = [\n",
    "    ('D:/redata/Neutraliza/cfp_ttm_nu.pkl'),\n",
    "    ('D:/redata/Neutraliza/EP_nu.pkl'),\n",
    "    ('D:/redata/Neutraliza/net_profit_Q_yoy_nu.pkl'),\n",
    "    ('D:/redata/Neutraliza/turnover_std_nu.pkl'),\n",
    "    ('D:/redata/Neutraliza/mv_nu.pkl'),\n",
    "    ('D:/redata/Neutraliza/Var120_nu.pkl'),\n",
    "    ('D:/redata/Neutraliza/hml_r_std_6m_nu.pkl'),\n",
    "    ('D:/redata/Neutraliza/liquidity_nu.pkl'),\n",
    "    ('D:/redata/Neutraliza/APB_20_nu.pkl'),\n",
    "    ('D:/redata/Neutraliza/high_r_std_5m_nu.pkl'),\n",
    "    ('D:/redata/Neutraliza/hpl_r_std_6m_nu.pkl'),\n",
    "    ('D:/redata/Neutraliza/BP_nu.pkl'),\n",
    "    ('D:/redata/Neutraliza/total_marketcap_nu.pkl'),\n",
    "    ('D:/redata/Neutraliza/cir_mv_nu.pkl'),\n",
    "    ('D:/redata/Neutraliza/ret_20_nu.pkl'),\n",
    "    ('D:/redata/Neutraliza/bias_5_nu.pkl'),\n",
    "    ('D:/redata/Neutraliza/total_amount_5_nu.pkl'),\n",
    "    ('D:/redata/Neutraliza/adjclose_nu.pkl')\n",
    "]\n",
    "\n",
    "for factor_path in factors_rank:\n",
    "    factor_rank = pd.read_pickle(factor_path)\n",
    "    IC, ICres, ICIR_120_weight = compute_IC(factor_rank, ret_rank)\n",
    "    ICIR_120_weight.to_pickle(factor_path.replace('.pkl', '_120_weight.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb6f0c3",
   "metadata": {},
   "source": [
    "# 相关性检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e07c96ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_mean = (f1+f2+f3+f4+f8+f10+f11+f13)\n",
    "comb_mean.to_pickle('D:/redata/factor/comb_mean.pkl')\n",
    "\n",
    "#市值分层\n",
    "comb_mean_s = comb_mean * small\n",
    "comb_mean_s.to_pickle('D:/redata/factor/comb_mean_s.pkl')\n",
    "comb_mean_m = comb_mean * median\n",
    "comb_mean_m.to_pickle('D:/redata/factor/comb_mean_m.pkl')\n",
    "comb_mean_sm = comb_mean * sm\n",
    "comb_mean_sm.to_pickle('D:/redata/factor/comb_mean_sm.pkl')\n",
    "comb_mean_l = comb_mean * large\n",
    "comb_mean_l.to_pickle('D:/redata/factor/comb_mean_l.pkl')\n",
    "\n",
    "#5、10、20日\n",
    "a = position(comb_mean_sm, 5)\n",
    "a.to_pickle('D:/redata/factor/comb_mean_sm5d.pkl')\n",
    "b = position(comb_mean_sm, 10)\n",
    "b.to_pickle('D:/redata/factor/comb_mean_sm10d.pkl')\n",
    "d = position(comb_mean_sm, 20)\n",
    "d.to_pickle('D:/redata/factor/comb_mean_sm20d.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca97bd33",
   "metadata": {},
   "source": [
    "# 选股"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "83c834ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ticker</th>\n",
       "      <th>000798</th>\n",
       "      <th>002051</th>\n",
       "      <th>300684</th>\n",
       "      <th>301089</th>\n",
       "      <th>301179</th>\n",
       "      <th>603668</th>\n",
       "      <th>605133</th>\n",
       "      <th>688270</th>\n",
       "      <th>688313</th>\n",
       "      <th>688697</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-06-02</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ticker      000798  002051  300684  301089  301179  603668  605133  688270  \\\n",
       "date                                                                         \n",
       "2023-06-02       1       1       1       1       1       1       1       1   \n",
       "\n",
       "ticker      688313  688697  \n",
       "date                        \n",
       "2023-06-02       1       1  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('D:/redata/factor/comb_mean.pkl')###读入因子值\n",
    "df = df.replace(np.inf,np.nan).replace(-np.inf,np.nan)\n",
    "#剔除 新股 st halt\n",
    "df_clean = (df * final_universe)\n",
    "df_clean = df_clean.dropna(how='all',axis=0)\n",
    "factor_type = 'hist'  # 'hist' 因子原值， 'ind' 对因子做行业中性\n",
    "factor_rank = factor_neutralize(df_clean,factor_type,if_rank=True,risk=risk,risk_cols=['logcmv','retsum120']) \n",
    "\n",
    "select_num_stocks(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e640481a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
